{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Fast, accurate and scalable probabilistic data linkage using your choice of SQL backend. \u00b6 splink is a Python package for probabilistic record linkage (entity resolution). Its key features are: It is extremely fast. It is capable of linking a million records on a laptop in around a minute. It is highly accurate, with support for term frequency adjustments, and sophisticated fuzzy matching logic. It supports running linkage against multiple SQL backends, meaning it's capable of running at any scale. For smaller linkages of up to a few million records, no additional infrastructure is needed . For larger linkages, Splink currently supports Apache Spark or AWS Athena as backends. It produces a wide variety of interactive outputs, helping users to understand their model and diagnose linkage problems. The core linkage algorithm is an implementation of Fellegi-Sunter's canonical model of record linkage, with various customisations to improve accuracy. Splink includes an implementation of the Expectation Maximisation algorithm, meaning that record linkage can be performed using an unsupervised approch (i.e. labelled training data is not needed). Documentation \u00b6 The homepage for the Splink documentation can be found here . Interactive demos can be found here , or by clicking the following Binder link: Acknowledgements \u00b6 We are very grateful to ADR UK (Administrative Data Research UK) for providing funding for this work as part of the Data First project. We are also very grateful to colleagues at the UK's Office for National Statistics for their expert advice and peer review of this work.","title":"Fast, accurate and scalable probabilistic data linkage using your choice of SQL backend."},{"location":"index.html#fast-accurate-and-scalable-probabilistic-data-linkage-using-your-choice-of-sql-backend","text":"splink is a Python package for probabilistic record linkage (entity resolution). Its key features are: It is extremely fast. It is capable of linking a million records on a laptop in around a minute. It is highly accurate, with support for term frequency adjustments, and sophisticated fuzzy matching logic. It supports running linkage against multiple SQL backends, meaning it's capable of running at any scale. For smaller linkages of up to a few million records, no additional infrastructure is needed . For larger linkages, Splink currently supports Apache Spark or AWS Athena as backends. It produces a wide variety of interactive outputs, helping users to understand their model and diagnose linkage problems. The core linkage algorithm is an implementation of Fellegi-Sunter's canonical model of record linkage, with various customisations to improve accuracy. Splink includes an implementation of the Expectation Maximisation algorithm, meaning that record linkage can be performed using an unsupervised approch (i.e. labelled training data is not needed).","title":"Fast, accurate and scalable probabilistic data linkage using your choice of SQL backend."},{"location":"index.html#documentation","text":"The homepage for the Splink documentation can be found here . Interactive demos can be found here , or by clicking the following Binder link:","title":"Documentation"},{"location":"index.html#acknowledgements","text":"We are very grateful to ADR UK (Administrative Data Research UK) for providing funding for this work as part of the Data First project. We are also very grateful to colleagues at the UK's Office for National Statistics for their expert advice and peer review of this work.","title":"Acknowledgements"},{"location":"CHANGELOG.html","text":"Changelog \u00b6 All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning . 2.1.3 \u00b6 Fixed \u00b6 SQLGlot produces SQL in the wrong dialect in some instances. See issue 269 2.1.3 \u00b6 Fixed \u00b6 Versions of sqlglot before 1.18.0 could not parse the spark sql left and right functions - see here . As a result, if left or right are used in the user's custom sql case expressions, Splink will produce an error. This release bumps the version in the requirement to fix this problem. 2.1.0 \u00b6 Added \u00b6 sql_expr now added to tooltips on bayes factor chart, displaying the SQL expression for each comparison level Warnings to the user if they don't include a null level in their case expression, custom columns is different to cols used in case expression, splink now parses case_expression to auto-populate num_levels or col_name or custom_columns_used . The user may still provide this information, but is no longer required to. Note that Splink now has a depedency on sqlglot , a no-dependency SQL parser. 2.0.4 \u00b6 Added \u00b6 Add function to analyse blocking rules from splink.analyse_blocking_rule import analyse_blocking_rule in https://github.com/moj-analytical-services/splink/pull/260 Full Changelog : https://github.com/moj-analytical-services/splink/compare/v2.0.3...v2.0.4 2.0.3 \u00b6 Added \u00b6 Gamma distribution chart by @RobinL in https://github.com/moj-analytical-services/splink/pull/246 Full Changelog : https://github.com/moj-analytical-services/splink/compare/v2.0.2...v2.0.3 2.0.2 \u00b6 Added \u00b6 Add function to compute m values from labelled data by @RobinL in https://github.com/moj-analytical-services/splink/pull/248 2.0.1 \u00b6 Added \u00b6 Add function that outputs the full path to the similarity jar by @RobinL in https://github.com/moj-analytical-services/splink/pull/237 Changed \u00b6 Allow match weight to be used in the diagnostic histogram by @RobinL in https://github.com/moj-analytical-services/splink/pull/239 2.0.0 \u00b6 Changed \u00b6 Term frequency adjustments are now calculated directly from a term freqency lookup table making them more accurate Term frequency adjustments are now part of the iterative EM estimation step, improving convergence All internal calculations are changed to use bayes factors (match weights) rather than probabilities to make the maths simpler Added \u00b6 Splink now outputs match_weight , the log2(Bayes Factor) of the match score. New splink.charts.save_offline_chart function that produces charts that work in airgapped environments with no internet connection New splink.cluster.clusters_at_thresholds function that clusters are one or more match thresholds The splink.truth.roc_chart function now allows several ROCS to be plotted on a single chart, to compare the accuracy of different models Splink now includes an slower Python implementation of jaro_winkler, in case users are having trouble with the string similarity jar Removed \u00b6 Since term frequency adjustments are no longer an ex-post step, there's no longer a need for them to be calculated separately. Splink therefore no longer outputs tf_adjusted_match_prob . Instead, TF adjustments are included within match_probability 1.0.5 \u00b6 Fixed \u00b6 Bug that meant default numerical case statements were not available. See here . Thanks to geobetts Changed \u00b6 m and u probabilities are now reset to None rather than 0 in EM iteration when they cannot be estimated Now use _repr_pretty_ so that objects display nicely in Jupyter Lab rather than __repr__ , which had been interfering with the interpretatino of stack trace errors 1.0.3 - 2020-02-04 \u00b6 Bug whereby Splink lowercased case expressions, see here 1.0.2 - 2020-02-02 \u00b6 Changed \u00b6 Improve estimate comparison charts, including tooltips and better labels 1.0.1 - 2020-01-31 \u00b6 Changed \u00b6 Added mousewheel zoom to bayes factor chart Added mousewheel zoom to splink score histogram Update estimate comparison chart to use different shapes for different estimates, making it possible to distinguish overlapping symbols Fixed \u00b6 m and u history charts now display barchart correctly 1.0.0 - 2020-01-20 \u00b6 Added \u00b6 Charts now feature improved tooltips, and have a cleaner appearance. Many are now zoomable Charts now display better in Jupyter Lab, especially the html file produced by all_charts_write_html_file() m and u probabilities charts can now be produced from Settings objects The user can now combine settings objects using ModelCombiner from splink.combine_models Changed \u00b6 A number of backwards incompatible changes have been made for Splink 1.0. The main Splink API is different. Instead of Splink(...,df=df) for dedupe and Splink(...,df_l=df_l,df_r=df_r) for linking, the user provides an agument df_or_dfs , which is either a single DataFrame or a list of DataFrames. This allows linking n>2 datasets. When linking multiple dataframes, the user must now include a source_dataset column (default name source_dataset , configurable via source_dataset_column_name in the settings dict) The Params class is now called Model in the model.py module. The on-disk (json) format of the Model object has changed and is incompatible with Params The new Model class now uses the same representation for parameters as the Settings object, reducing duplicate code. Internal functions now have settings or model as function arguments, never both. Vega lite chart definitions now stored in json files in splink/files/chart_defs All case statement generation functions are now consistently named, with all names starting sql_gen_case_stmt_ Fixed case_statements.sql_gen_case_smnt_strict_equality_2 which previously behaved differently to all other case functions All case statements now have a default threshold of exact equality on their top gamma level Fixed \u00b6 Removed \u00b6","title":"Changelog"},{"location":"CHANGELOG.html#changelog","text":"All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning .","title":"Changelog"},{"location":"CHANGELOG.html#213","text":"","title":"2.1.3"},{"location":"CHANGELOG.html#fixed","text":"SQLGlot produces SQL in the wrong dialect in some instances. See issue 269","title":"Fixed"},{"location":"CHANGELOG.html#213_1","text":"","title":"2.1.3"},{"location":"CHANGELOG.html#fixed_1","text":"Versions of sqlglot before 1.18.0 could not parse the spark sql left and right functions - see here . As a result, if left or right are used in the user's custom sql case expressions, Splink will produce an error. This release bumps the version in the requirement to fix this problem.","title":"Fixed"},{"location":"CHANGELOG.html#210","text":"","title":"2.1.0"},{"location":"CHANGELOG.html#added","text":"sql_expr now added to tooltips on bayes factor chart, displaying the SQL expression for each comparison level Warnings to the user if they don't include a null level in their case expression, custom columns is different to cols used in case expression, splink now parses case_expression to auto-populate num_levels or col_name or custom_columns_used . The user may still provide this information, but is no longer required to. Note that Splink now has a depedency on sqlglot , a no-dependency SQL parser.","title":"Added"},{"location":"CHANGELOG.html#204","text":"","title":"2.0.4"},{"location":"CHANGELOG.html#added_1","text":"Add function to analyse blocking rules from splink.analyse_blocking_rule import analyse_blocking_rule in https://github.com/moj-analytical-services/splink/pull/260 Full Changelog : https://github.com/moj-analytical-services/splink/compare/v2.0.3...v2.0.4","title":"Added"},{"location":"CHANGELOG.html#203","text":"","title":"2.0.3"},{"location":"CHANGELOG.html#added_2","text":"Gamma distribution chart by @RobinL in https://github.com/moj-analytical-services/splink/pull/246 Full Changelog : https://github.com/moj-analytical-services/splink/compare/v2.0.2...v2.0.3","title":"Added"},{"location":"CHANGELOG.html#202","text":"","title":"2.0.2"},{"location":"CHANGELOG.html#added_3","text":"Add function to compute m values from labelled data by @RobinL in https://github.com/moj-analytical-services/splink/pull/248","title":"Added"},{"location":"CHANGELOG.html#201","text":"","title":"2.0.1"},{"location":"CHANGELOG.html#added_4","text":"Add function that outputs the full path to the similarity jar by @RobinL in https://github.com/moj-analytical-services/splink/pull/237","title":"Added"},{"location":"CHANGELOG.html#changed","text":"Allow match weight to be used in the diagnostic histogram by @RobinL in https://github.com/moj-analytical-services/splink/pull/239","title":"Changed"},{"location":"CHANGELOG.html#200","text":"","title":"2.0.0"},{"location":"CHANGELOG.html#changed_1","text":"Term frequency adjustments are now calculated directly from a term freqency lookup table making them more accurate Term frequency adjustments are now part of the iterative EM estimation step, improving convergence All internal calculations are changed to use bayes factors (match weights) rather than probabilities to make the maths simpler","title":"Changed"},{"location":"CHANGELOG.html#added_5","text":"Splink now outputs match_weight , the log2(Bayes Factor) of the match score. New splink.charts.save_offline_chart function that produces charts that work in airgapped environments with no internet connection New splink.cluster.clusters_at_thresholds function that clusters are one or more match thresholds The splink.truth.roc_chart function now allows several ROCS to be plotted on a single chart, to compare the accuracy of different models Splink now includes an slower Python implementation of jaro_winkler, in case users are having trouble with the string similarity jar","title":"Added"},{"location":"CHANGELOG.html#removed","text":"Since term frequency adjustments are no longer an ex-post step, there's no longer a need for them to be calculated separately. Splink therefore no longer outputs tf_adjusted_match_prob . Instead, TF adjustments are included within match_probability","title":"Removed"},{"location":"CHANGELOG.html#105","text":"","title":"1.0.5"},{"location":"CHANGELOG.html#fixed_2","text":"Bug that meant default numerical case statements were not available. See here . Thanks to geobetts","title":"Fixed"},{"location":"CHANGELOG.html#changed_2","text":"m and u probabilities are now reset to None rather than 0 in EM iteration when they cannot be estimated Now use _repr_pretty_ so that objects display nicely in Jupyter Lab rather than __repr__ , which had been interfering with the interpretatino of stack trace errors","title":"Changed"},{"location":"CHANGELOG.html#103-2020-02-04","text":"Bug whereby Splink lowercased case expressions, see here","title":"1.0.3 - 2020-02-04"},{"location":"CHANGELOG.html#102-2020-02-02","text":"","title":"1.0.2 - 2020-02-02"},{"location":"CHANGELOG.html#changed_3","text":"Improve estimate comparison charts, including tooltips and better labels","title":"Changed"},{"location":"CHANGELOG.html#101-2020-01-31","text":"","title":"1.0.1 - 2020-01-31"},{"location":"CHANGELOG.html#changed_4","text":"Added mousewheel zoom to bayes factor chart Added mousewheel zoom to splink score histogram Update estimate comparison chart to use different shapes for different estimates, making it possible to distinguish overlapping symbols","title":"Changed"},{"location":"CHANGELOG.html#fixed_3","text":"m and u history charts now display barchart correctly","title":"Fixed"},{"location":"CHANGELOG.html#100-2020-01-20","text":"","title":"1.0.0 - 2020-01-20"},{"location":"CHANGELOG.html#added_6","text":"Charts now feature improved tooltips, and have a cleaner appearance. Many are now zoomable Charts now display better in Jupyter Lab, especially the html file produced by all_charts_write_html_file() m and u probabilities charts can now be produced from Settings objects The user can now combine settings objects using ModelCombiner from splink.combine_models","title":"Added"},{"location":"CHANGELOG.html#changed_5","text":"A number of backwards incompatible changes have been made for Splink 1.0. The main Splink API is different. Instead of Splink(...,df=df) for dedupe and Splink(...,df_l=df_l,df_r=df_r) for linking, the user provides an agument df_or_dfs , which is either a single DataFrame or a list of DataFrames. This allows linking n>2 datasets. When linking multiple dataframes, the user must now include a source_dataset column (default name source_dataset , configurable via source_dataset_column_name in the settings dict) The Params class is now called Model in the model.py module. The on-disk (json) format of the Model object has changed and is incompatible with Params The new Model class now uses the same representation for parameters as the Settings object, reducing duplicate code. Internal functions now have settings or model as function arguments, never both. Vega lite chart definitions now stored in json files in splink/files/chart_defs All case statement generation functions are now consistently named, with all names starting sql_gen_case_stmt_ Fixed case_statements.sql_gen_case_smnt_strict_equality_2 which previously behaved differently to all other case functions All case statements now have a default threshold of exact equality on their top gamma level","title":"Changed"},{"location":"CHANGELOG.html#fixed_4","text":"","title":"Fixed"},{"location":"CHANGELOG.html#removed_1","text":"","title":"Removed"},{"location":"SplinkDataFrame.html","tags":["API"],"text":"Documentation for SplinkDataFrame object \u00b6 Abstraction over dataframe to handle basic operations like retrieving data and retrieving column names, which need different implementations depending on whether it's a spark dataframe, sqlite table etc. Uses methods like as_pandas_dataframe() and as_record_dict() to retrieve data Source code in splink/splink_dataframe.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 class SplinkDataFrame : \"\"\"Abstraction over dataframe to handle basic operations like retrieving data and retrieving column names, which need different implementations depending on whether it's a spark dataframe, sqlite table etc. Uses methods like `as_pandas_dataframe()` and `as_record_dict()` to retrieve data \"\"\" def __init__ ( self , templated_name , physical_name ): self . templated_name = templated_name self . physical_name = physical_name @property def columns ( self ): pass @property def columns_escaped ( self ): cols = self . columns return escape_columns ( cols ) def validate (): pass def _random_sample_sql ( percent ): raise NotImplementedError ( \"Random sample sql not implemented for this linker\" ) @property def physical_and_template_names_equal ( self ): return self . templated_name == self . physical_name def _check_drop_table_created_by_splink ( self , force_non_splink_table = False ): if not self . physical_name . startswith ( \"__splink__\" ): if not force_non_splink_table : raise ValueError ( f \"You've asked to drop table { self . physical_name } from your \" \"database which is not a table created by Splink. If you really \" \"want to drop this table, you can do so by setting \" \"force_non_splink_table=True\" ) logger . debug ( f \"Dropping table with templated name { self . templated_name } and \" f \"physical name { self . physical_name } \" ) def drop_table_from_database ( self , force_non_splink_table = False ): raise NotImplementedError ( \"Drop table from database not implemented for this linker\" ) def as_record_dict ( self , limit = None ): pass def as_pandas_dataframe ( self , limit = None ): \"\"\"Return the dataframe as a pandas dataframe. This can be computationally expensive if the dataframe is large. Args: limit (int, optional): If provided, return this number of rows (equivalent to a limit statement in SQL). Defaults to None, meaning return all rows Returns: pandas.DataFrame: pandas Dataframe \"\"\" import pandas as pd return pd . DataFrame ( self . as_record_dict ( limit = limit )) def __repr__ ( self ): return ( f \"Table name in database: ` { self . physical_name } ` \\n \" \" \\n To retrieve records, you can call the following methods on this object:\" \" \\n `.as_record_dict(limit=5)` or \" \"`.as_pandas_dataframe(limit=5)`. \\n \" \" \\n You may omit the `limit` argument to return all records.\" \" \\n\\n This table represents the following splink entity: \" f \" { self . templated_name } \" ) drop_table_from_database ( force_non_splink_table = False ) \u00b6 Source code in splink/splink_dataframe.py 55 56 57 58 def drop_table_from_database ( self , force_non_splink_table = False ): raise NotImplementedError ( \"Drop table from database not implemented for this linker\" ) as_pandas_dataframe ( limit = None ) \u00b6 Return the dataframe as a pandas dataframe. This can be computationally expensive if the dataframe is large. Parameters: Name Type Description Default limit int If provided, return this number of rows (equivalent None Returns: Type Description pandas.DataFrame: pandas Dataframe Source code in splink/splink_dataframe.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def as_pandas_dataframe ( self , limit = None ): \"\"\"Return the dataframe as a pandas dataframe. This can be computationally expensive if the dataframe is large. Args: limit (int, optional): If provided, return this number of rows (equivalent to a limit statement in SQL). Defaults to None, meaning return all rows Returns: pandas.DataFrame: pandas Dataframe \"\"\" import pandas as pd return pd . DataFrame ( self . as_record_dict ( limit = limit ))","title":"SplinkDataFrame API"},{"location":"SplinkDataFrame.html#documentation-for-splinkdataframe-object","text":"Abstraction over dataframe to handle basic operations like retrieving data and retrieving column names, which need different implementations depending on whether it's a spark dataframe, sqlite table etc. Uses methods like as_pandas_dataframe() and as_record_dict() to retrieve data Source code in splink/splink_dataframe.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 class SplinkDataFrame : \"\"\"Abstraction over dataframe to handle basic operations like retrieving data and retrieving column names, which need different implementations depending on whether it's a spark dataframe, sqlite table etc. Uses methods like `as_pandas_dataframe()` and `as_record_dict()` to retrieve data \"\"\" def __init__ ( self , templated_name , physical_name ): self . templated_name = templated_name self . physical_name = physical_name @property def columns ( self ): pass @property def columns_escaped ( self ): cols = self . columns return escape_columns ( cols ) def validate (): pass def _random_sample_sql ( percent ): raise NotImplementedError ( \"Random sample sql not implemented for this linker\" ) @property def physical_and_template_names_equal ( self ): return self . templated_name == self . physical_name def _check_drop_table_created_by_splink ( self , force_non_splink_table = False ): if not self . physical_name . startswith ( \"__splink__\" ): if not force_non_splink_table : raise ValueError ( f \"You've asked to drop table { self . physical_name } from your \" \"database which is not a table created by Splink. If you really \" \"want to drop this table, you can do so by setting \" \"force_non_splink_table=True\" ) logger . debug ( f \"Dropping table with templated name { self . templated_name } and \" f \"physical name { self . physical_name } \" ) def drop_table_from_database ( self , force_non_splink_table = False ): raise NotImplementedError ( \"Drop table from database not implemented for this linker\" ) def as_record_dict ( self , limit = None ): pass def as_pandas_dataframe ( self , limit = None ): \"\"\"Return the dataframe as a pandas dataframe. This can be computationally expensive if the dataframe is large. Args: limit (int, optional): If provided, return this number of rows (equivalent to a limit statement in SQL). Defaults to None, meaning return all rows Returns: pandas.DataFrame: pandas Dataframe \"\"\" import pandas as pd return pd . DataFrame ( self . as_record_dict ( limit = limit )) def __repr__ ( self ): return ( f \"Table name in database: ` { self . physical_name } ` \\n \" \" \\n To retrieve records, you can call the following methods on this object:\" \" \\n `.as_record_dict(limit=5)` or \" \"`.as_pandas_dataframe(limit=5)`. \\n \" \" \\n You may omit the `limit` argument to return all records.\" \" \\n\\n This table represents the following splink entity: \" f \" { self . templated_name } \" )","title":"Documentation for SplinkDataFrame object"},{"location":"SplinkDataFrame.html#splink.splink_dataframe.SplinkDataFrame.drop_table_from_database","text":"Source code in splink/splink_dataframe.py 55 56 57 58 def drop_table_from_database ( self , force_non_splink_table = False ): raise NotImplementedError ( \"Drop table from database not implemented for this linker\" )","title":"drop_table_from_database()"},{"location":"SplinkDataFrame.html#splink.splink_dataframe.SplinkDataFrame.as_pandas_dataframe","text":"Return the dataframe as a pandas dataframe. This can be computationally expensive if the dataframe is large. Parameters: Name Type Description Default limit int If provided, return this number of rows (equivalent None Returns: Type Description pandas.DataFrame: pandas Dataframe Source code in splink/splink_dataframe.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def as_pandas_dataframe ( self , limit = None ): \"\"\"Return the dataframe as a pandas dataframe. This can be computationally expensive if the dataframe is large. Args: limit (int, optional): If provided, return this number of rows (equivalent to a limit statement in SQL). Defaults to None, meaning return all rows Returns: pandas.DataFrame: pandas Dataframe \"\"\" import pandas as pd return pd . DataFrame ( self . as_record_dict ( limit = limit ))","title":"as_pandas_dataframe()"},{"location":"comparison.html","tags":["API"],"text":"Documentation for Comparison object \u00b6 Each Comparison defines how data from one or more input columns is compared to assess its similarity. For example, one Comparison may represent how similarity is assessed for a person's date of birth. Others may represent the comparison of a person's name or location. The method used to assess similarity will depend on the type of data - for instance, the method used to assess similarity of a company's turnover would be different to the method used to assess the similarity of a person's first name. A linking model thus usually contains several Comparisons. As far as possible, Comparisons should be configured to satisfy the assumption of independece conditional on the true match status, a key assumption of the Fellegi Sunter probabilistic linkage model. This would be broken, for example, if a model contained one Comparison for city, and another for postcode. Instead, in this example, a single comparison should be modelled, which may to capture similarity taking account of both the city and postcode field. Each Comparison contains two or more ComparisonLevel s which define the gradations of similarity between the input columns within the Comparison. For example, for the date of birth Comparison there may be a ComparisonLevel for an exact match, another for a one-character difference, and another for all other comparisons. To summarise: Data Linking Model \u251c\u2500-- Comparison: Date of birth \u2502 \u251c\u2500-- ComparisonLevel: Exact match \u2502 \u251c\u2500-- ComparisonLevel: One character difference \u2502 \u251c\u2500-- ComparisonLevel: All other \u251c\u2500-- Comparison: Name \u2502 \u251c\u2500-- ComparisonLevel: Exact match on first name and surname \u2502 \u251c\u2500-- ComparisonLevel: Exact match on first name \u2502 \u251c\u2500-- etc. Source code in splink/comparison.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 class Comparison : \"\"\"Each Comparison defines how data from one or more input columns is compared to assess its similarity. For example, one Comparison may represent how similarity is assessed for a person's date of birth. Others may represent the comparison of a person's name or location. The method used to assess similarity will depend on the type of data - for instance, the method used to assess similarity of a company's turnover would be different to the method used to assess the similarity of a person's first name. A linking model thus usually contains several Comparisons. As far as possible, Comparisons should be configured to satisfy the assumption of independece conditional on the true match status, a key assumption of the Fellegi Sunter probabilistic linkage model. This would be broken, for example, if a model contained one Comparison for city, and another for postcode. Instead, in this example, a single comparison should be modelled, which may to capture similarity taking account of both the city and postcode field. Each Comparison contains two or more `ComparisonLevel`s which define the gradations of similarity between the input columns within the Comparison. For example, for the date of birth Comparison there may be a ComparisonLevel for an exact match, another for a one-character difference, and another for all other comparisons. To summarise: Data Linking Model \u251c\u2500-- Comparison: Date of birth \u2502 \u251c\u2500-- ComparisonLevel: Exact match \u2502 \u251c\u2500-- ComparisonLevel: One character difference \u2502 \u251c\u2500-- ComparisonLevel: All other \u251c\u2500-- Comparison: Name \u2502 \u251c\u2500-- ComparisonLevel: Exact match on first name and surname \u2502 \u251c\u2500-- ComparisonLevel: Exact match on first name \u2502 \u251c\u2500-- etc. \"\"\" def __init__ ( self , comparison_dict , settings_obj : \"Settings\" = None ): # Protected because we don't want to modify self . _comparison_dict = comparison_dict comparison_level_list = comparison_dict [ \"comparison_levels\" ] self . comparison_levels : List [ ComparisonLevel ] = [] # If comparison_levels are already of type ComparisonLevel, register # the settings object on them # otherwise turn the dictionaries into ComparisonLevel for cl in comparison_level_list : if isinstance ( cl , ComparisonLevel ): cl . comparison = self elif settings_obj is None : cl = ComparisonLevel ( cl , self ) else : cl = ComparisonLevel ( cl , self , sql_dialect = settings_obj . _sql_dialect ) self . comparison_levels . append ( cl ) self . _settings_obj : \"Settings\" = settings_obj # Assign comparison vector values starting at highest level, count down to 0 num_levels = self . _num_levels counter = num_levels - 1 for level in self . comparison_levels : if level . _is_null_level : level . _comparison_vector_value = - 1 level . _max_level = False else : level . _comparison_vector_value = counter if counter == num_levels - 1 : level . _max_level = True else : level . _max_level = False counter -= 1 def __deepcopy__ ( self , memo ): \"\"\"When we do EM training, we need a copy of the Comparison which is independent of the original e.g. modifying the copy will not affect the original. This method implements ensures the Comparison can be deepcopied. \"\"\" cc = Comparison ( self . as_dict (), self . _settings_obj ) return cc @property def _num_levels ( self ): return len ([ cl for cl in self . comparison_levels if not cl . _is_null_level ]) @property def _comparison_levels_excluding_null ( self ): return [ cl for cl in self . comparison_levels if not cl . _is_null_level ] @property def _gamma_prefix ( self ): return self . _settings_obj . _gamma_prefix @property def _retain_intermediate_calculation_columns ( self ): return self . _settings_obj . _retain_intermediate_calculation_columns @property def _bf_column_name ( self ): return f \" { self . _settings_obj . _bf_prefix }{ self . _output_column_name } \" . replace ( \" \" , \"_\" ) @property def _has_null_level ( self ): return any ([ cl . _is_null_level for cl in self . comparison_levels ]) @property def _bf_tf_adj_column_name ( self ): bf = self . _settings_obj . _bf_prefix tf = self . _settings_obj . _tf_prefix cc_name = self . _output_column_name return f \" { bf }{ tf } adj_ { cc_name } \" . replace ( \" \" , \"_\" ) @property def _has_tf_adjustments ( self ): return any ([ cl . _has_tf_adjustments for cl in self . comparison_levels ]) @property def _case_statement ( self ): sqls = [ cl . _when_then_comparison_vector_value_sql for cl in self . comparison_levels ] sql = \" \" . join ( sqls ) sql = f \"CASE { sql } END as { self . _gamma_column_name } \" return sql @property def _input_columns_used_by_case_statement ( self ): cols = [] for cl in self . comparison_levels : cols . extend ( cl . _input_columns_used_by_sql_condition ) # dedupe_preserving_order on input column already_observed = [] deduped_cols = [] for col in cols : if col . input_name not in already_observed : deduped_cols . append ( col ) already_observed . append ( col . input_name ) return deduped_cols @property def _output_column_name ( self ): if \"output_column_name\" in self . _comparison_dict : return self . _comparison_dict [ \"output_column_name\" ] else : cols = self . _input_columns_used_by_case_statement cols = [ c . input_name for c in cols ] if len ( cols ) == 1 : return cols [ 0 ] else : return f \"custom_ { '_' . join ( cols ) } \" @property def _comparison_description ( self ): if \"comparison_description\" in self . _comparison_dict : return self . _comparison_dict [ \"comparison_description\" ] else : return self . _output_column_name @property def _gamma_column_name ( self ): return f \" { self . _gamma_prefix }{ self . _output_column_name } \" . replace ( \" \" , \"_\" ) @property def _tf_adjustment_input_col_names ( self ): cols = [ cl . _tf_adjustment_input_column_name for cl in self . comparison_levels ] cols = [ c for c in cols if c ] return cols @property def _columns_to_select_for_blocking ( self ): cols = [] for cl in self . comparison_levels : cols . extend ( cl . _columns_to_select_for_blocking ) return dedupe_preserving_order ( cols ) @property def _columns_to_select_for_comparison_vector_values ( self ): input_cols = [] for cl in self . comparison_levels : input_cols . extend ( cl . _input_columns_used_by_sql_condition ) output_cols = [] for col in input_cols : if self . _settings_obj . _retain_matching_columns : output_cols . extend ( col . names_l_r ()) output_cols . append ( self . _case_statement ) for col in input_cols : if self . _has_tf_adjustments : output_cols . extend ( col . tf_name_l_r ()) return dedupe_preserving_order ( output_cols ) @property def _columns_to_select_for_bayes_factor_parts ( self ): input_cols = [] for cl in self . comparison_levels : input_cols . extend ( cl . _input_columns_used_by_sql_condition ) output_cols = [] for col in input_cols : if self . _settings_obj . _retain_matching_columns : output_cols . extend ( col . names_l_r ()) output_cols . append ( self . _gamma_column_name ) for col in input_cols : if self . _has_tf_adjustments : if self . _settings_obj . _retain_intermediate_calculation_columns : output_cols . extend ( col . tf_name_l_r ()) # Bayes factor case when statement sqls = [ cl . _bayes_factor_sql for cl in self . comparison_levels ] sql = \" \" . join ( sqls ) sql = f \"CASE { sql } END as { self . _bf_column_name } \" output_cols . append ( sql ) # tf adjustment case when statement if self . _has_tf_adjustments : sqls = [ cl . _tf_adjustment_sql for cl in self . comparison_levels ] sql = \" \" . join ( sqls ) sql = f \"CASE { sql } END as { self . _bf_tf_adj_column_name } \" output_cols . append ( sql ) output_cols . append ( self . _gamma_column_name ) return dedupe_preserving_order ( output_cols ) @property def _columns_to_select_for_predict ( self ): input_cols = [] for cl in self . comparison_levels : input_cols . extend ( cl . _input_columns_used_by_sql_condition ) output_cols = [] for col in input_cols : if self . _settings_obj . _retain_matching_columns : output_cols . extend ( col . names_l_r ()) if ( self . _settings_obj . _training_mode or self . _settings_obj . _retain_matching_columns ): output_cols . append ( self . _gamma_column_name ) for col in input_cols : if self . _settings_obj . _retain_intermediate_calculation_columns : if self . _has_tf_adjustments : output_cols . extend ( col . tf_name_l_r ()) output_cols . extend ( self . _match_weight_columns_to_multiply ) return dedupe_preserving_order ( output_cols ) @property def _match_weight_columns_to_multiply ( self ): cols = [] cols . append ( self . _bf_column_name ) if self . _has_tf_adjustments : cols . append ( self . _bf_tf_adj_column_name ) return cols @property def _term_frequency_columns ( self ): cols = set () for cl in self . comparison_levels : cols . add ( cl . tf_adjustment_input_col_name ) return list ( cols ) def as_dict ( self ): d = { \"output_column_name\" : self . _output_column_name , \"comparison_levels\" : [ cl . as_dict () for cl in self . comparison_levels ], } if \"comparison_description\" in self . _comparison_dict : d [ \"comparison_description\" ] = self . _comparison_dict [ \"comparison_description\" ] return d def _as_completed_dict ( self ): return { \"column_name\" : self . _output_column_name , \"comparison_levels\" : [ cl . _as_completed_dict () for cl in self . comparison_levels ], \"input_columns_used_by_case_statement\" : [ c . input_name for c in self . _input_columns_used_by_case_statement ], } @property def _has_estimated_m_values ( self ): return all ( cl . _has_estimated_m_values for cl in self . comparison_levels ) @property def _has_estimated_u_values ( self ): return all ( cl . _has_estimated_u_values for cl in self . comparison_levels ) @property def _all_m_are_trained ( self ): return all ( cl . _m_is_trained for cl in self . comparison_levels ) @property def _all_u_are_trained ( self ): return all ( cl . _u_is_trained for cl in self . comparison_levels ) @property def _some_m_are_trained ( self ): return any ( cl . _m_is_trained for cl in self . _comparison_levels_excluding_null ) @property def _some_u_are_trained ( self ): return any ( cl . _u_is_trained for cl in self . _comparison_levels_excluding_null ) @property def _is_trained_message ( self ): messages = [] if self . _all_m_are_trained and self . _all_u_are_trained : return None if not self . _some_u_are_trained : messages . append ( \"no u values are trained\" ) elif self . _some_u_are_trained and not self . _all_u_are_trained : messages . append ( \"some u values are not trained\" ) if not self . _some_m_are_trained : messages . append ( \"no m values are trained\" ) elif self . _some_m_are_trained and not self . _all_m_are_trained : messages . append ( \"some m values are not trained\" ) message = \", \" . join ( messages ) message = f \" - { self . _output_column_name } ( { message } ).\" return message @property def _is_trained ( self ): return self . _all_m_are_trained and self . _all_u_are_trained @property def _as_detailed_records ( self ): records = [] for cl in self . comparison_levels : record = {} record [ \"comparison_name\" ] = self . _output_column_name record = { ** record , ** cl . _as_detailed_record } records . append ( record ) return records @property def _parameter_estimates_as_records ( self ): records = [] for cl in self . comparison_levels : new_records = cl . _parameter_estimates_as_records for r in new_records : r [ \"comparison_name\" ] = self . _output_column_name records . extend ( new_records ) return records def _get_comparison_level_by_comparison_vector_value ( self , value ) -> ComparisonLevel : for cl in self . comparison_levels : if cl . _comparison_vector_value == value : return cl raise ValueError ( f \"No comparison level with comparison vector value { value } \" ) def __repr__ ( self ): return ( f \"<Comparison { self . _comparison_description } with \" f \" { self . _num_levels } levels at { hex ( id ( self )) } >\" ) @property def _not_trained_messages ( self ): msgs = [] cname = self . _output_column_name header = f \"Comparison: ' { cname } ': \\n \" msg_template = \" {header} {m_or_u} values not fully trained\" if not self . _all_m_are_trained : msgs . append ( msg_template . format ( header = header , m_or_u = \"m\" )) if not self . _all_u_are_trained : msgs . append ( msg_template . format ( header = header , m_or_u = \"u\" )) return msgs @property def _comparison_level_description_list ( self ): cl_template = \" - ' {label} ' with SQL rule: {sql} \\n \" comp_levels = [ cl_template . format ( cvv = cl . _comparison_vector_value , label = cl . _label_for_charts , sql = cl . _sql_condition , ) for cl in self . comparison_levels ] comp_levels = \"\" . join ( comp_levels ) return comp_levels @property def _human_readable_description_succinct ( self ): input_cols = join_list_with_commas_final_and ( [ c . name ( escape = False ) for c in self . _input_columns_used_by_case_statement ] ) comp_levels = self . _comparison_level_description_list if \"comparison_description\" in self . _comparison_dict : main_desc = ( f \"of { input_cols } \\n Description: ' { self . _comparison_description } '\" ) else : main_desc = f \"of { input_cols } \" desc = f \"Comparison { main_desc } \\n Comparison levels: \\n { comp_levels } \" return desc @property def human_readable_description ( self ): input_cols = join_list_with_commas_final_and ( [ c . name ( escape = False ) for c in self . _input_columns_used_by_case_statement ] ) comp_levels = self . _comparison_level_description_list if \"comparison_description\" in self . _comparison_dict : main_desc = f \"' { self . _comparison_description } ' of { input_cols } \" else : main_desc = f \"of { input_cols } \" desc = ( f \"Comparison { main_desc } . \\n \" \"Similarity is assessed using the following \" f \"ComparisonLevels: \\n { comp_levels } \" ) return desc def match_weights_chart ( self , as_dict = False ): \"\"\"Display a chart of comparison levels of the comparison\"\"\" from .charts import comparison_match_weights_chart records = self . _as_detailed_records return comparison_match_weights_chart ( records , as_dict = as_dict ) human_readable_description () property \u00b6 Source code in splink/comparison.py 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 @property def human_readable_description ( self ): input_cols = join_list_with_commas_final_and ( [ c . name ( escape = False ) for c in self . _input_columns_used_by_case_statement ] ) comp_levels = self . _comparison_level_description_list if \"comparison_description\" in self . _comparison_dict : main_desc = f \"' { self . _comparison_description } ' of { input_cols } \" else : main_desc = f \"of { input_cols } \" desc = ( f \"Comparison { main_desc } . \\n \" \"Similarity is assessed using the following \" f \"ComparisonLevels: \\n { comp_levels } \" ) return desc match_weights_chart ( as_dict = False ) \u00b6 Display a chart of comparison levels of the comparison Source code in splink/comparison.py 486 487 488 489 490 491 def match_weights_chart ( self , as_dict = False ): \"\"\"Display a chart of comparison levels of the comparison\"\"\" from .charts import comparison_match_weights_chart records = self . _as_detailed_records return comparison_match_weights_chart ( records , as_dict = as_dict )","title":"Comparison"},{"location":"comparison.html#documentation-for-comparison-object","text":"Each Comparison defines how data from one or more input columns is compared to assess its similarity. For example, one Comparison may represent how similarity is assessed for a person's date of birth. Others may represent the comparison of a person's name or location. The method used to assess similarity will depend on the type of data - for instance, the method used to assess similarity of a company's turnover would be different to the method used to assess the similarity of a person's first name. A linking model thus usually contains several Comparisons. As far as possible, Comparisons should be configured to satisfy the assumption of independece conditional on the true match status, a key assumption of the Fellegi Sunter probabilistic linkage model. This would be broken, for example, if a model contained one Comparison for city, and another for postcode. Instead, in this example, a single comparison should be modelled, which may to capture similarity taking account of both the city and postcode field. Each Comparison contains two or more ComparisonLevel s which define the gradations of similarity between the input columns within the Comparison. For example, for the date of birth Comparison there may be a ComparisonLevel for an exact match, another for a one-character difference, and another for all other comparisons. To summarise: Data Linking Model \u251c\u2500-- Comparison: Date of birth \u2502 \u251c\u2500-- ComparisonLevel: Exact match \u2502 \u251c\u2500-- ComparisonLevel: One character difference \u2502 \u251c\u2500-- ComparisonLevel: All other \u251c\u2500-- Comparison: Name \u2502 \u251c\u2500-- ComparisonLevel: Exact match on first name and surname \u2502 \u251c\u2500-- ComparisonLevel: Exact match on first name \u2502 \u251c\u2500-- etc. Source code in splink/comparison.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 class Comparison : \"\"\"Each Comparison defines how data from one or more input columns is compared to assess its similarity. For example, one Comparison may represent how similarity is assessed for a person's date of birth. Others may represent the comparison of a person's name or location. The method used to assess similarity will depend on the type of data - for instance, the method used to assess similarity of a company's turnover would be different to the method used to assess the similarity of a person's first name. A linking model thus usually contains several Comparisons. As far as possible, Comparisons should be configured to satisfy the assumption of independece conditional on the true match status, a key assumption of the Fellegi Sunter probabilistic linkage model. This would be broken, for example, if a model contained one Comparison for city, and another for postcode. Instead, in this example, a single comparison should be modelled, which may to capture similarity taking account of both the city and postcode field. Each Comparison contains two or more `ComparisonLevel`s which define the gradations of similarity between the input columns within the Comparison. For example, for the date of birth Comparison there may be a ComparisonLevel for an exact match, another for a one-character difference, and another for all other comparisons. To summarise: Data Linking Model \u251c\u2500-- Comparison: Date of birth \u2502 \u251c\u2500-- ComparisonLevel: Exact match \u2502 \u251c\u2500-- ComparisonLevel: One character difference \u2502 \u251c\u2500-- ComparisonLevel: All other \u251c\u2500-- Comparison: Name \u2502 \u251c\u2500-- ComparisonLevel: Exact match on first name and surname \u2502 \u251c\u2500-- ComparisonLevel: Exact match on first name \u2502 \u251c\u2500-- etc. \"\"\" def __init__ ( self , comparison_dict , settings_obj : \"Settings\" = None ): # Protected because we don't want to modify self . _comparison_dict = comparison_dict comparison_level_list = comparison_dict [ \"comparison_levels\" ] self . comparison_levels : List [ ComparisonLevel ] = [] # If comparison_levels are already of type ComparisonLevel, register # the settings object on them # otherwise turn the dictionaries into ComparisonLevel for cl in comparison_level_list : if isinstance ( cl , ComparisonLevel ): cl . comparison = self elif settings_obj is None : cl = ComparisonLevel ( cl , self ) else : cl = ComparisonLevel ( cl , self , sql_dialect = settings_obj . _sql_dialect ) self . comparison_levels . append ( cl ) self . _settings_obj : \"Settings\" = settings_obj # Assign comparison vector values starting at highest level, count down to 0 num_levels = self . _num_levels counter = num_levels - 1 for level in self . comparison_levels : if level . _is_null_level : level . _comparison_vector_value = - 1 level . _max_level = False else : level . _comparison_vector_value = counter if counter == num_levels - 1 : level . _max_level = True else : level . _max_level = False counter -= 1 def __deepcopy__ ( self , memo ): \"\"\"When we do EM training, we need a copy of the Comparison which is independent of the original e.g. modifying the copy will not affect the original. This method implements ensures the Comparison can be deepcopied. \"\"\" cc = Comparison ( self . as_dict (), self . _settings_obj ) return cc @property def _num_levels ( self ): return len ([ cl for cl in self . comparison_levels if not cl . _is_null_level ]) @property def _comparison_levels_excluding_null ( self ): return [ cl for cl in self . comparison_levels if not cl . _is_null_level ] @property def _gamma_prefix ( self ): return self . _settings_obj . _gamma_prefix @property def _retain_intermediate_calculation_columns ( self ): return self . _settings_obj . _retain_intermediate_calculation_columns @property def _bf_column_name ( self ): return f \" { self . _settings_obj . _bf_prefix }{ self . _output_column_name } \" . replace ( \" \" , \"_\" ) @property def _has_null_level ( self ): return any ([ cl . _is_null_level for cl in self . comparison_levels ]) @property def _bf_tf_adj_column_name ( self ): bf = self . _settings_obj . _bf_prefix tf = self . _settings_obj . _tf_prefix cc_name = self . _output_column_name return f \" { bf }{ tf } adj_ { cc_name } \" . replace ( \" \" , \"_\" ) @property def _has_tf_adjustments ( self ): return any ([ cl . _has_tf_adjustments for cl in self . comparison_levels ]) @property def _case_statement ( self ): sqls = [ cl . _when_then_comparison_vector_value_sql for cl in self . comparison_levels ] sql = \" \" . join ( sqls ) sql = f \"CASE { sql } END as { self . _gamma_column_name } \" return sql @property def _input_columns_used_by_case_statement ( self ): cols = [] for cl in self . comparison_levels : cols . extend ( cl . _input_columns_used_by_sql_condition ) # dedupe_preserving_order on input column already_observed = [] deduped_cols = [] for col in cols : if col . input_name not in already_observed : deduped_cols . append ( col ) already_observed . append ( col . input_name ) return deduped_cols @property def _output_column_name ( self ): if \"output_column_name\" in self . _comparison_dict : return self . _comparison_dict [ \"output_column_name\" ] else : cols = self . _input_columns_used_by_case_statement cols = [ c . input_name for c in cols ] if len ( cols ) == 1 : return cols [ 0 ] else : return f \"custom_ { '_' . join ( cols ) } \" @property def _comparison_description ( self ): if \"comparison_description\" in self . _comparison_dict : return self . _comparison_dict [ \"comparison_description\" ] else : return self . _output_column_name @property def _gamma_column_name ( self ): return f \" { self . _gamma_prefix }{ self . _output_column_name } \" . replace ( \" \" , \"_\" ) @property def _tf_adjustment_input_col_names ( self ): cols = [ cl . _tf_adjustment_input_column_name for cl in self . comparison_levels ] cols = [ c for c in cols if c ] return cols @property def _columns_to_select_for_blocking ( self ): cols = [] for cl in self . comparison_levels : cols . extend ( cl . _columns_to_select_for_blocking ) return dedupe_preserving_order ( cols ) @property def _columns_to_select_for_comparison_vector_values ( self ): input_cols = [] for cl in self . comparison_levels : input_cols . extend ( cl . _input_columns_used_by_sql_condition ) output_cols = [] for col in input_cols : if self . _settings_obj . _retain_matching_columns : output_cols . extend ( col . names_l_r ()) output_cols . append ( self . _case_statement ) for col in input_cols : if self . _has_tf_adjustments : output_cols . extend ( col . tf_name_l_r ()) return dedupe_preserving_order ( output_cols ) @property def _columns_to_select_for_bayes_factor_parts ( self ): input_cols = [] for cl in self . comparison_levels : input_cols . extend ( cl . _input_columns_used_by_sql_condition ) output_cols = [] for col in input_cols : if self . _settings_obj . _retain_matching_columns : output_cols . extend ( col . names_l_r ()) output_cols . append ( self . _gamma_column_name ) for col in input_cols : if self . _has_tf_adjustments : if self . _settings_obj . _retain_intermediate_calculation_columns : output_cols . extend ( col . tf_name_l_r ()) # Bayes factor case when statement sqls = [ cl . _bayes_factor_sql for cl in self . comparison_levels ] sql = \" \" . join ( sqls ) sql = f \"CASE { sql } END as { self . _bf_column_name } \" output_cols . append ( sql ) # tf adjustment case when statement if self . _has_tf_adjustments : sqls = [ cl . _tf_adjustment_sql for cl in self . comparison_levels ] sql = \" \" . join ( sqls ) sql = f \"CASE { sql } END as { self . _bf_tf_adj_column_name } \" output_cols . append ( sql ) output_cols . append ( self . _gamma_column_name ) return dedupe_preserving_order ( output_cols ) @property def _columns_to_select_for_predict ( self ): input_cols = [] for cl in self . comparison_levels : input_cols . extend ( cl . _input_columns_used_by_sql_condition ) output_cols = [] for col in input_cols : if self . _settings_obj . _retain_matching_columns : output_cols . extend ( col . names_l_r ()) if ( self . _settings_obj . _training_mode or self . _settings_obj . _retain_matching_columns ): output_cols . append ( self . _gamma_column_name ) for col in input_cols : if self . _settings_obj . _retain_intermediate_calculation_columns : if self . _has_tf_adjustments : output_cols . extend ( col . tf_name_l_r ()) output_cols . extend ( self . _match_weight_columns_to_multiply ) return dedupe_preserving_order ( output_cols ) @property def _match_weight_columns_to_multiply ( self ): cols = [] cols . append ( self . _bf_column_name ) if self . _has_tf_adjustments : cols . append ( self . _bf_tf_adj_column_name ) return cols @property def _term_frequency_columns ( self ): cols = set () for cl in self . comparison_levels : cols . add ( cl . tf_adjustment_input_col_name ) return list ( cols ) def as_dict ( self ): d = { \"output_column_name\" : self . _output_column_name , \"comparison_levels\" : [ cl . as_dict () for cl in self . comparison_levels ], } if \"comparison_description\" in self . _comparison_dict : d [ \"comparison_description\" ] = self . _comparison_dict [ \"comparison_description\" ] return d def _as_completed_dict ( self ): return { \"column_name\" : self . _output_column_name , \"comparison_levels\" : [ cl . _as_completed_dict () for cl in self . comparison_levels ], \"input_columns_used_by_case_statement\" : [ c . input_name for c in self . _input_columns_used_by_case_statement ], } @property def _has_estimated_m_values ( self ): return all ( cl . _has_estimated_m_values for cl in self . comparison_levels ) @property def _has_estimated_u_values ( self ): return all ( cl . _has_estimated_u_values for cl in self . comparison_levels ) @property def _all_m_are_trained ( self ): return all ( cl . _m_is_trained for cl in self . comparison_levels ) @property def _all_u_are_trained ( self ): return all ( cl . _u_is_trained for cl in self . comparison_levels ) @property def _some_m_are_trained ( self ): return any ( cl . _m_is_trained for cl in self . _comparison_levels_excluding_null ) @property def _some_u_are_trained ( self ): return any ( cl . _u_is_trained for cl in self . _comparison_levels_excluding_null ) @property def _is_trained_message ( self ): messages = [] if self . _all_m_are_trained and self . _all_u_are_trained : return None if not self . _some_u_are_trained : messages . append ( \"no u values are trained\" ) elif self . _some_u_are_trained and not self . _all_u_are_trained : messages . append ( \"some u values are not trained\" ) if not self . _some_m_are_trained : messages . append ( \"no m values are trained\" ) elif self . _some_m_are_trained and not self . _all_m_are_trained : messages . append ( \"some m values are not trained\" ) message = \", \" . join ( messages ) message = f \" - { self . _output_column_name } ( { message } ).\" return message @property def _is_trained ( self ): return self . _all_m_are_trained and self . _all_u_are_trained @property def _as_detailed_records ( self ): records = [] for cl in self . comparison_levels : record = {} record [ \"comparison_name\" ] = self . _output_column_name record = { ** record , ** cl . _as_detailed_record } records . append ( record ) return records @property def _parameter_estimates_as_records ( self ): records = [] for cl in self . comparison_levels : new_records = cl . _parameter_estimates_as_records for r in new_records : r [ \"comparison_name\" ] = self . _output_column_name records . extend ( new_records ) return records def _get_comparison_level_by_comparison_vector_value ( self , value ) -> ComparisonLevel : for cl in self . comparison_levels : if cl . _comparison_vector_value == value : return cl raise ValueError ( f \"No comparison level with comparison vector value { value } \" ) def __repr__ ( self ): return ( f \"<Comparison { self . _comparison_description } with \" f \" { self . _num_levels } levels at { hex ( id ( self )) } >\" ) @property def _not_trained_messages ( self ): msgs = [] cname = self . _output_column_name header = f \"Comparison: ' { cname } ': \\n \" msg_template = \" {header} {m_or_u} values not fully trained\" if not self . _all_m_are_trained : msgs . append ( msg_template . format ( header = header , m_or_u = \"m\" )) if not self . _all_u_are_trained : msgs . append ( msg_template . format ( header = header , m_or_u = \"u\" )) return msgs @property def _comparison_level_description_list ( self ): cl_template = \" - ' {label} ' with SQL rule: {sql} \\n \" comp_levels = [ cl_template . format ( cvv = cl . _comparison_vector_value , label = cl . _label_for_charts , sql = cl . _sql_condition , ) for cl in self . comparison_levels ] comp_levels = \"\" . join ( comp_levels ) return comp_levels @property def _human_readable_description_succinct ( self ): input_cols = join_list_with_commas_final_and ( [ c . name ( escape = False ) for c in self . _input_columns_used_by_case_statement ] ) comp_levels = self . _comparison_level_description_list if \"comparison_description\" in self . _comparison_dict : main_desc = ( f \"of { input_cols } \\n Description: ' { self . _comparison_description } '\" ) else : main_desc = f \"of { input_cols } \" desc = f \"Comparison { main_desc } \\n Comparison levels: \\n { comp_levels } \" return desc @property def human_readable_description ( self ): input_cols = join_list_with_commas_final_and ( [ c . name ( escape = False ) for c in self . _input_columns_used_by_case_statement ] ) comp_levels = self . _comparison_level_description_list if \"comparison_description\" in self . _comparison_dict : main_desc = f \"' { self . _comparison_description } ' of { input_cols } \" else : main_desc = f \"of { input_cols } \" desc = ( f \"Comparison { main_desc } . \\n \" \"Similarity is assessed using the following \" f \"ComparisonLevels: \\n { comp_levels } \" ) return desc def match_weights_chart ( self , as_dict = False ): \"\"\"Display a chart of comparison levels of the comparison\"\"\" from .charts import comparison_match_weights_chart records = self . _as_detailed_records return comparison_match_weights_chart ( records , as_dict = as_dict )","title":"Documentation for Comparison object"},{"location":"comparison.html#splink.comparison.Comparison.human_readable_description","text":"Source code in splink/comparison.py 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 @property def human_readable_description ( self ): input_cols = join_list_with_commas_final_and ( [ c . name ( escape = False ) for c in self . _input_columns_used_by_case_statement ] ) comp_levels = self . _comparison_level_description_list if \"comparison_description\" in self . _comparison_dict : main_desc = f \"' { self . _comparison_description } ' of { input_cols } \" else : main_desc = f \"of { input_cols } \" desc = ( f \"Comparison { main_desc } . \\n \" \"Similarity is assessed using the following \" f \"ComparisonLevels: \\n { comp_levels } \" ) return desc","title":"human_readable_description()"},{"location":"comparison.html#splink.comparison.Comparison.match_weights_chart","text":"Display a chart of comparison levels of the comparison Source code in splink/comparison.py 486 487 488 489 490 491 def match_weights_chart ( self , as_dict = False ): \"\"\"Display a chart of comparison levels of the comparison\"\"\" from .charts import comparison_match_weights_chart records = self . _as_detailed_records return comparison_match_weights_chart ( records , as_dict = as_dict )","title":"match_weights_chart()"},{"location":"comparison_level.html","tags":["API","comparisons"],"text":"Documentation for ComparisonLevel object \u00b6 Each ComparisonLevel defines a gradation (category) of similarity within a Comparison . For example, a `Comparison` that uses the first_name and surname columns may define three `ComparisonLevel`s: An exact match on first name and surname First name and surname have a JaroWinkler score of above 0.95 All other comparisons The method used to assess similarity will depend on the type of data - for instance, the method used to assess similarity of a company's turnover would be different to the method used to assess the similarity of a person's first name. To summarise: Data Linking Model \u251c\u2500-- Comparison: Name \u2502 \u251c\u2500-- ComparisonLevel: Exact match on first_name and surname \u2502 \u251c\u2500-- ComparisonLevel: first_name and surname have JaroWinkler > 0.95 \u2502 \u251c\u2500-- ComparisonLevel: All other \u251c\u2500-- Comparison: Date of birth \u2502 \u251c\u2500-- ComparisonLevel: Exact match \u2502 \u251c\u2500-- ComparisonLevel: One character difference \u2502 \u251c\u2500-- ComparisonLevel: All other \u251c\u2500-- etc. Source code in splink/comparison_level.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 class ComparisonLevel : \"\"\"Each ComparisonLevel defines a gradation (category) of similarity within a `Comparison`. For example, a `Comparison` that uses the first_name and surname columns may define three `ComparisonLevel`s: An exact match on first name and surname First name and surname have a JaroWinkler score of above 0.95 All other comparisons The method used to assess similarity will depend on the type of data - for instance, the method used to assess similarity of a company's turnover would be different to the method used to assess the similarity of a person's first name. To summarise: ``` Data Linking Model \u251c\u2500-- Comparison: Name \u2502 \u251c\u2500-- ComparisonLevel: Exact match on first_name and surname \u2502 \u251c\u2500-- ComparisonLevel: first_name and surname have JaroWinkler > 0.95 \u2502 \u251c\u2500-- ComparisonLevel: All other \u251c\u2500-- Comparison: Date of birth \u2502 \u251c\u2500-- ComparisonLevel: Exact match \u2502 \u251c\u2500-- ComparisonLevel: One character difference \u2502 \u251c\u2500-- ComparisonLevel: All other \u251c\u2500-- etc. ``` \"\"\" def __init__ ( self , level_dict , comparison : \"Comparison\" = None , sql_dialect : str = None , ): # Protected, because we don't want to modify the original dict self . _level_dict = level_dict self . comparison : \"Comparison\" = comparison self . _sql_dialect = sql_dialect self . _sql_condition = self . _level_dict [ \"sql_condition\" ] self . _is_null_level = self . _level_dict_val_else_default ( \"is_null_level\" ) self . _tf_adjustment_weight = self . _level_dict_val_else_default ( \"tf_adjustment_weight\" ) self . _tf_minimum_u_value = self . _level_dict_val_else_default ( \"tf_minimum_u_value\" ) # Private values controlled with getter/setter self . _m_probability = self . _level_dict . get ( \"m_probability\" ) self . _u_probability = self . _level_dict . get ( \"u_probability\" ) # These will be set when the ComparisonLevel is passed into a Comparison self . _comparison_vector_value : int = None self . _max_level : bool = None # Enable the level to 'know' when it's been trained self . _trained_m_probabilities : list = [] self . _trained_u_probabilities : list = [] self . _validate () def _level_dict_val_else_default ( self , key ): val = self . _level_dict . get ( key ) if not val : val = default_value_from_schema ( key , \"comparison_level\" ) return val @property def _tf_adjustment_input_column ( self ): val = self . _level_dict_val_else_default ( \"tf_adjustment_column\" ) if val : return InputColumn ( val , tf_adjustments = True , sql_dialect = self . _sql_dialect ) else : return None @property def _tf_adjustment_input_column_name ( self ): input_column = self . _tf_adjustment_input_column if input_column : return input_column . input_name else : return None @property def _has_comparison ( self ): from .comparison import Comparison return isinstance ( self . comparison , Comparison ) @property def m_probability ( self ): if self . _is_null_level : return None if self . _m_probability == \"level not observed in training dataset\" : return 1e-6 if self . _m_probability is None and self . _has_comparison : vals = _default_m_values ( self . comparison . _num_levels ) return vals [ self . _comparison_vector_value ] return self . _m_probability @m_probability . setter def m_probability ( self , value ): if self . _is_null_level : raise AttributeError ( \"Cannot set m_probability when is_null_level is true\" ) if value == \"level not observed in training dataset\" : cc_n = self . comparison . _output_column_name cl_n = self . _label_for_charts logger . warning ( \" \\n WARNING: \\n \" f \"Level { cl_n } on comparison { cc_n } not observed in dataset, \" \"unable to train m value\" ) self . _m_probability = value @property def u_probability ( self ): if self . _is_null_level : return None if self . _u_probability == \"level not observed in training dataset\" : return 1e-6 if self . _u_probability is None : vals = _default_u_values ( self . comparison . _num_levels ) return vals [ self . _comparison_vector_value ] return self . _u_probability @u_probability . setter def u_probability ( self , value ): if self . _is_null_level : raise AttributeError ( \"Cannot set u_probability when is_null_level is true\" ) if value == \"level not observed in training dataset\" : cc_n = self . comparison . _output_column_name cl_n = self . _label_for_charts logger . warning ( \" \\n WARNING: \\n \" f \"Level { cl_n } on comparison { cc_n } not observed in dataset, \" \"unable to train u value\" ) self . _u_probability = value @property def _m_probability_description ( self ): if self . m_probability is not None : return ( \"Amongst matching record comparisons, \" f \" { self . m_probability : .2% } of records are in the \" f \" { self . _label_for_charts . lower () } comparison level\" ) @property def _u_probability_description ( self ): if self . u_probability is not None : return ( \"Amongst non-matching record comparisons, \" f \" { self . u_probability : .2% } of records are in the \" f \" { self . _label_for_charts . lower () } comparison level\" ) def _add_trained_u_probability ( self , val , desc = \"no description given\" ): self . _trained_u_probabilities . append ( { \"probability\" : val , \"description\" : desc , \"m_or_u\" : \"u\" } ) def _add_trained_m_probability ( self , val , desc = \"no description given\" ): self . _trained_m_probabilities . append ( { \"probability\" : val , \"description\" : desc , \"m_or_u\" : \"m\" } ) @property def _has_estimated_u_values ( self ): if self . _is_null_level : return True vals = [ r [ \"probability\" ] for r in self . _trained_u_probabilities ] vals = [ v for v in vals if isinstance ( v , ( int , float ))] return len ( vals ) > 0 @property def _has_estimated_m_values ( self ): if self . _is_null_level : return True vals = [ r [ \"probability\" ] for r in self . _trained_m_probabilities ] vals = [ v for v in vals if isinstance ( v , ( int , float ))] return len ( vals ) > 0 @property def _has_estimated_values ( self ): return self . _has_estimated_m_values and self . _has_estimated_u_values @property def _trained_m_median ( self ): vals = [ r [ \"probability\" ] for r in self . _trained_m_probabilities ] vals = [ v for v in vals if isinstance ( v , ( int , float ))] if len ( vals ) == 0 : return None return median ( vals ) @property def _trained_u_median ( self ): vals = [ r [ \"probability\" ] for r in self . _trained_u_probabilities ] vals = [ v for v in vals if isinstance ( v , ( int , float ))] if len ( vals ) == 0 : return None return median ( vals ) @property def _m_is_trained ( self ): if self . _is_null_level : return True if self . _m_probability == \"level not observed in data\" : return False if self . _m_probability is None : return False return True @property def _u_is_trained ( self ): if self . _is_null_level : return True if self . _u_probability == \"level not observed in data\" : return False if self . _u_probability is None : return False return True @property def _is_trained ( self ): return self . _m_is_trained and self . _u_is_trained @property def _bayes_factor ( self ): if self . _is_null_level : return 1.0 if self . m_probability is None or self . u_probability is None : return None else : return self . m_probability / self . u_probability @property def _log2_bayes_factor ( self ): if self . _is_null_level : return 0.0 else : return math . log2 ( self . _bayes_factor ) @property def _bayes_factor_description ( self ): text = ( f \"If comparison level is ` { self . _label_for_charts . lower () } ` \" \"then comparison is\" ) if self . _bayes_factor >= 1.0 : return f \" { text } { self . _bayes_factor : ,.2f } times more likely to be a match\" else : mult = 1 / self . _bayes_factor return f \" { text } { mult : ,.2f } times less likely to be a match\" @property def _label_for_charts ( self ): return self . _level_dict . get ( \"label_for_charts\" , str ( self . _comparison_vector_value ) ) @property def _is_else_level ( self ): if self . _sql_condition . strip () . upper () == \"ELSE\" : return True @property def _has_tf_adjustments ( self ): col = self . _level_dict . get ( \"tf_adjustment_column\" ) return col is not None @property def _sql_read_dialect ( self ): read_dialect = None if self . _sql_dialect is not None : read_dialect = self . _sql_dialect return read_dialect def _validate_sql ( self ): sql = self . _sql_condition if self . _is_else_level : return True try : sqlglot . parse_one ( sql , read = self . _sql_read_dialect ) except sqlglot . ParseError as e : raise ValueError ( f \"Error parsing sql_statement: \\n { sql } \" ) from e return True @property def _input_columns_used_by_sql_condition ( self ) -> List [ InputColumn ]: # returns e.g. InputColumn(first_name), InputColumn(surname) if self . _is_else_level : return [] cols = get_columns_used_from_sql ( self . _sql_condition , dialect = self . _sql_read_dialect ) # Parsed order seems to be roughly in reverse order of apearance cols = cols [:: - 1 ] cols = [ re . sub ( r \"_L$|_R$\" , \"\" , c , flags = re . IGNORECASE ) for c in cols ] cols = dedupe_preserving_order ( cols ) input_cols = [] for c in cols : # We could have tf adjustments for surname on a dmeta_surname column # If so, we want to set the tf adjustments against the surname col, # not the dmeta_surname one if c == self . _tf_adjustment_input_column_name : input_cols . append ( InputColumn ( c , tf_adjustments = True , sql_dialect = self . _sql_dialect ) ) else : input_cols . append ( InputColumn ( c , tf_adjustments = False , sql_dialect = self . _sql_dialect ) ) return input_cols @property def _columns_to_select_for_blocking ( self ): # e.g. l.first_name as first_name_l, r.first_name as first_name_r output_cols = [] cols = self . _input_columns_used_by_sql_condition for c in cols : output_cols . extend ( c . l_r_names_as_l_r ()) output_cols . extend ( c . l_r_tf_names_as_l_r ()) return dedupe_preserving_order ( output_cols ) @property def _when_then_comparison_vector_value_sql ( self ): # e.g. when first_name_l = first_name_r then 1 if not hasattr ( self , \"_comparison_vector_value\" ): raise ValueError ( \"Cannot get the 'when .. then ...' sql expression because \" \"this comparison level does not belong to a parent Comparison. \" \"The comparison_vector_value is only defined in the \" \"context of a list of ComparisonLevels within a Comparison.\" ) if self . _is_else_level : return f \" { self . _sql_condition } { self . _comparison_vector_value } \" else : return f \"WHEN { self . _sql_condition } THEN { self . _comparison_vector_value } \" @property def _is_exact_match ( self ): if self . _is_else_level : return False sqls = re . split ( r \" and \" , self . _sql_condition , flags = re . IGNORECASE ) for sql in sqls : if not _is_exact_match ( sql ): return False return True @property def _exact_match_colnames ( self ): sqls = re . split ( r \" and \" , self . _sql_condition , flags = re . IGNORECASE ) for sql in sqls : if not _is_exact_match ( sql ): raise ValueError ( \"sql_cond not an exact match so can't get exact match column name\" ) cols = [] for sql in sqls : col = _exact_match_colname ( sql ) cols . append ( col ) return cols @property def _u_probability_corresponding_to_exact_match ( self ): levels = self . comparison . comparison_levels # Find a level with a single exact match colname # which is equal to the tf adjustment input colname for level in levels : if not level . _is_exact_match : continue colnames = level . _exact_match_colnames if len ( colnames ) != 1 : continue if colnames [ 0 ] == self . _tf_adjustment_input_column_name . lower (): return level . u_probability raise ValueError ( \"Could not find an exact match level for \" f \" { self . _tf_adjustment_input_column_name } .\" \" \\n An exact match level is required to make a term frequency adjustment \" \"on a comparison level that is not an exact match.\" ) @property def _bayes_factor_sql ( self ): sql = f \"\"\" WHEN { self . comparison . _gamma_column_name } = { self . _comparison_vector_value } THEN cast( { self . _bayes_factor } as double) \"\"\" return dedent ( sql ) @property def _tf_adjustment_sql ( self ): gamma_column_name = self . comparison . _gamma_column_name gamma_colname_value_is_this_level = ( f \" { gamma_column_name } = { self . _comparison_vector_value } \" ) # A tf adjustment of 1D is a multiplier of 1.0, i.e. no adjustment if self . _comparison_vector_value == - 1 : sql = f \"WHEN { gamma_colname_value_is_this_level } then cast(1 as double)\" elif not self . _has_tf_adjustments : sql = f \"WHEN { gamma_colname_value_is_this_level } then cast(1 as double)\" elif self . _tf_adjustment_weight == 0 : sql = f \"WHEN { gamma_colname_value_is_this_level } then cast(1 as double)\" elif self . _is_else_level : sql = f \"WHEN { gamma_colname_value_is_this_level } then cast(1 as double)\" else : tf_adj_col = self . _tf_adjustment_input_column coalesce_l_r = ( f \"coalesce( { tf_adj_col . tf_name_l () } , { tf_adj_col . tf_name_r () } )\" ) coalesce_r_l = ( f \"coalesce( { tf_adj_col . tf_name_r () } , { tf_adj_col . tf_name_l () } )\" ) tf_adjustment_exists = f \" { coalesce_l_r } is not null\" u_prob_exact_match = self . _u_probability_corresponding_to_exact_match # Using coalesce protects against one of the tf adjustments being null # Which would happen if the user provided their own tf adjustment table # That didn't contain some of the values in this data # In this case rather than taking the greater of the two, we take # whichever value exists if self . _tf_minimum_u_value == 0.0 : divisor_sql = f \"\"\" (CASE WHEN { coalesce_l_r } >= { coalesce_r_l } THEN { coalesce_l_r } ELSE { coalesce_r_l } END) \"\"\" else : # This sql works correctly even when the tf_minimum_u_value is 0.0 # but is less efficient to execute, hence the above if statement divisor_sql = f \"\"\" (CASE WHEN { coalesce_l_r } >= { coalesce_r_l } AND { coalesce_l_r } > cast( { self . _tf_minimum_u_value } as double) THEN { coalesce_l_r } WHEN { coalesce_r_l } > cast( { self . _tf_minimum_u_value } as double) THEN { coalesce_r_l } ELSE cast( { self . _tf_minimum_u_value } as double) END) \"\"\" sql = f \"\"\" WHEN { gamma_colname_value_is_this_level } then (CASE WHEN { tf_adjustment_exists } THEN POW( cast( { u_prob_exact_match } as double) / { divisor_sql } , cast( { self . _tf_adjustment_weight } as double) ) ELSE cast(1 as double) END) \"\"\" return dedent ( sql ) . strip () def as_dict ( self ): \"The minimal representation of this level to use as an input to Splink\" output = {} output [ \"sql_condition\" ] = self . _sql_condition if self . _level_dict . get ( \"label_for_charts\" ): output [ \"label_for_charts\" ] = self . _label_for_charts if self . _m_probability and self . _m_is_trained : output [ \"m_probability\" ] = self . m_probability if self . _u_probability and self . _u_is_trained : output [ \"u_probability\" ] = self . u_probability if self . _has_tf_adjustments : output [ \"tf_adjustment_column\" ] = self . _tf_adjustment_input_column . input_name if self . _tf_adjustment_weight != 0 : output [ \"tf_adjustment_weight\" ] = self . _tf_adjustment_weight if self . _is_null_level : output [ \"is_null_level\" ] = True return output def _as_completed_dict ( self ): comp_dict = self . as_dict () comp_dict [ \"comparison_vector_value\" ] = self . _comparison_vector_value return comp_dict @property def _as_detailed_record ( self ): \"A detailed representation of this level to describe it in charting outputs\" output = {} output [ \"sql_condition\" ] = self . _sql_condition output [ \"label_for_charts\" ] = self . _label_for_charts output [ \"m_probability\" ] = self . m_probability output [ \"u_probability\" ] = self . u_probability output [ \"m_probability_description\" ] = self . _m_probability_description output [ \"u_probability_description\" ] = self . _u_probability_description output [ \"has_tf_adjustments\" ] = self . _has_tf_adjustments if self . _has_tf_adjustments : output [ \"tf_adjustment_column\" ] = self . _tf_adjustment_input_column . input_name else : output [ \"tf_adjustment_column\" ] = None output [ \"tf_adjustment_weight\" ] = self . _tf_adjustment_weight output [ \"is_null_level\" ] = self . _is_null_level output [ \"bayes_factor\" ] = self . _bayes_factor output [ \"log2_bayes_factor\" ] = self . _log2_bayes_factor output [ \"comparison_vector_value\" ] = self . _comparison_vector_value output [ \"max_comparison_vector_value\" ] = self . comparison . _num_levels - 1 output [ \"bayes_factor_description\" ] = self . _bayes_factor_description return output @property def _parameter_estimates_as_records ( self ): output_records = [] cl_record = self . _as_detailed_record trained_values = self . _trained_u_probabilities + self . _trained_m_probabilities for trained_value in trained_values : record = {} record [ \"m_or_u\" ] = trained_value [ \"m_or_u\" ] p = trained_value [ \"probability\" ] record [ \"estimated_probability\" ] = p record [ \"estimate_description\" ] = trained_value [ \"description\" ] if p is not None and p > 0.0 : record [ \"estimated_probability_as_log_odds\" ] = math . log2 ( p / ( 1 - p )) else : record [ \"estimated_probability_as_log_odds\" ] = None record [ \"sql_condition\" ] = cl_record [ \"sql_condition\" ] record [ \"comparison_level_label\" ] = cl_record [ \"label_for_charts\" ] record [ \"comparison_vector_value\" ] = cl_record [ \"comparison_vector_value\" ] output_records . append ( record ) return output_records def _validate ( self ): self . _validate_sql () def _abbreviated_sql ( self , cutoff = 75 ): sql = self . _sql_condition sql = ( sql [: 75 ] + \"...\" ) if len ( sql ) > 75 else sql return sql def __repr__ ( self ): return f \"< { self . _human_readable_succinct } >\" @property def _human_readable_succinct ( self ): sql = self . _abbreviated_sql ( 75 ) return f \"Comparison level ' { self . _label_for_charts } ' using SQL rule: { sql } \" @property def human_readable_description ( self ): input_cols = join_list_with_commas_final_and ( [ c . name ( escape = False ) for c in self . _input_columns_used_by_sql_condition ] ) desc = ( f \"Comparison level: { self . _label_for_charts } of { input_cols } \\n \" \"Assesses similarity between pairwise comparisons of the input columns \" f \"using the following rule \\n { self . _sql_condition } \" ) return desc m_probability () property writable \u00b6 Source code in splink/comparison_level.py 181 182 183 184 185 186 187 188 189 190 @property def m_probability ( self ): if self . _is_null_level : return None if self . _m_probability == \"level not observed in training dataset\" : return 1e-6 if self . _m_probability is None and self . _has_comparison : vals = _default_m_values ( self . comparison . _num_levels ) return vals [ self . _comparison_vector_value ] return self . _m_probability u_probability () property writable \u00b6 Source code in splink/comparison_level.py 207 208 209 210 211 212 213 214 215 216 @property def u_probability ( self ): if self . _is_null_level : return None if self . _u_probability == \"level not observed in training dataset\" : return 1e-6 if self . _u_probability is None : vals = _default_u_values ( self . comparison . _num_levels ) return vals [ self . _comparison_vector_value ] return self . _u_probability","title":"Comparison Level"},{"location":"comparison_level.html#documentation-for-comparisonlevel-object","text":"Each ComparisonLevel defines a gradation (category) of similarity within a Comparison . For example, a `Comparison` that uses the first_name and surname columns may define three `ComparisonLevel`s: An exact match on first name and surname First name and surname have a JaroWinkler score of above 0.95 All other comparisons The method used to assess similarity will depend on the type of data - for instance, the method used to assess similarity of a company's turnover would be different to the method used to assess the similarity of a person's first name. To summarise: Data Linking Model \u251c\u2500-- Comparison: Name \u2502 \u251c\u2500-- ComparisonLevel: Exact match on first_name and surname \u2502 \u251c\u2500-- ComparisonLevel: first_name and surname have JaroWinkler > 0.95 \u2502 \u251c\u2500-- ComparisonLevel: All other \u251c\u2500-- Comparison: Date of birth \u2502 \u251c\u2500-- ComparisonLevel: Exact match \u2502 \u251c\u2500-- ComparisonLevel: One character difference \u2502 \u251c\u2500-- ComparisonLevel: All other \u251c\u2500-- etc. Source code in splink/comparison_level.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 class ComparisonLevel : \"\"\"Each ComparisonLevel defines a gradation (category) of similarity within a `Comparison`. For example, a `Comparison` that uses the first_name and surname columns may define three `ComparisonLevel`s: An exact match on first name and surname First name and surname have a JaroWinkler score of above 0.95 All other comparisons The method used to assess similarity will depend on the type of data - for instance, the method used to assess similarity of a company's turnover would be different to the method used to assess the similarity of a person's first name. To summarise: ``` Data Linking Model \u251c\u2500-- Comparison: Name \u2502 \u251c\u2500-- ComparisonLevel: Exact match on first_name and surname \u2502 \u251c\u2500-- ComparisonLevel: first_name and surname have JaroWinkler > 0.95 \u2502 \u251c\u2500-- ComparisonLevel: All other \u251c\u2500-- Comparison: Date of birth \u2502 \u251c\u2500-- ComparisonLevel: Exact match \u2502 \u251c\u2500-- ComparisonLevel: One character difference \u2502 \u251c\u2500-- ComparisonLevel: All other \u251c\u2500-- etc. ``` \"\"\" def __init__ ( self , level_dict , comparison : \"Comparison\" = None , sql_dialect : str = None , ): # Protected, because we don't want to modify the original dict self . _level_dict = level_dict self . comparison : \"Comparison\" = comparison self . _sql_dialect = sql_dialect self . _sql_condition = self . _level_dict [ \"sql_condition\" ] self . _is_null_level = self . _level_dict_val_else_default ( \"is_null_level\" ) self . _tf_adjustment_weight = self . _level_dict_val_else_default ( \"tf_adjustment_weight\" ) self . _tf_minimum_u_value = self . _level_dict_val_else_default ( \"tf_minimum_u_value\" ) # Private values controlled with getter/setter self . _m_probability = self . _level_dict . get ( \"m_probability\" ) self . _u_probability = self . _level_dict . get ( \"u_probability\" ) # These will be set when the ComparisonLevel is passed into a Comparison self . _comparison_vector_value : int = None self . _max_level : bool = None # Enable the level to 'know' when it's been trained self . _trained_m_probabilities : list = [] self . _trained_u_probabilities : list = [] self . _validate () def _level_dict_val_else_default ( self , key ): val = self . _level_dict . get ( key ) if not val : val = default_value_from_schema ( key , \"comparison_level\" ) return val @property def _tf_adjustment_input_column ( self ): val = self . _level_dict_val_else_default ( \"tf_adjustment_column\" ) if val : return InputColumn ( val , tf_adjustments = True , sql_dialect = self . _sql_dialect ) else : return None @property def _tf_adjustment_input_column_name ( self ): input_column = self . _tf_adjustment_input_column if input_column : return input_column . input_name else : return None @property def _has_comparison ( self ): from .comparison import Comparison return isinstance ( self . comparison , Comparison ) @property def m_probability ( self ): if self . _is_null_level : return None if self . _m_probability == \"level not observed in training dataset\" : return 1e-6 if self . _m_probability is None and self . _has_comparison : vals = _default_m_values ( self . comparison . _num_levels ) return vals [ self . _comparison_vector_value ] return self . _m_probability @m_probability . setter def m_probability ( self , value ): if self . _is_null_level : raise AttributeError ( \"Cannot set m_probability when is_null_level is true\" ) if value == \"level not observed in training dataset\" : cc_n = self . comparison . _output_column_name cl_n = self . _label_for_charts logger . warning ( \" \\n WARNING: \\n \" f \"Level { cl_n } on comparison { cc_n } not observed in dataset, \" \"unable to train m value\" ) self . _m_probability = value @property def u_probability ( self ): if self . _is_null_level : return None if self . _u_probability == \"level not observed in training dataset\" : return 1e-6 if self . _u_probability is None : vals = _default_u_values ( self . comparison . _num_levels ) return vals [ self . _comparison_vector_value ] return self . _u_probability @u_probability . setter def u_probability ( self , value ): if self . _is_null_level : raise AttributeError ( \"Cannot set u_probability when is_null_level is true\" ) if value == \"level not observed in training dataset\" : cc_n = self . comparison . _output_column_name cl_n = self . _label_for_charts logger . warning ( \" \\n WARNING: \\n \" f \"Level { cl_n } on comparison { cc_n } not observed in dataset, \" \"unable to train u value\" ) self . _u_probability = value @property def _m_probability_description ( self ): if self . m_probability is not None : return ( \"Amongst matching record comparisons, \" f \" { self . m_probability : .2% } of records are in the \" f \" { self . _label_for_charts . lower () } comparison level\" ) @property def _u_probability_description ( self ): if self . u_probability is not None : return ( \"Amongst non-matching record comparisons, \" f \" { self . u_probability : .2% } of records are in the \" f \" { self . _label_for_charts . lower () } comparison level\" ) def _add_trained_u_probability ( self , val , desc = \"no description given\" ): self . _trained_u_probabilities . append ( { \"probability\" : val , \"description\" : desc , \"m_or_u\" : \"u\" } ) def _add_trained_m_probability ( self , val , desc = \"no description given\" ): self . _trained_m_probabilities . append ( { \"probability\" : val , \"description\" : desc , \"m_or_u\" : \"m\" } ) @property def _has_estimated_u_values ( self ): if self . _is_null_level : return True vals = [ r [ \"probability\" ] for r in self . _trained_u_probabilities ] vals = [ v for v in vals if isinstance ( v , ( int , float ))] return len ( vals ) > 0 @property def _has_estimated_m_values ( self ): if self . _is_null_level : return True vals = [ r [ \"probability\" ] for r in self . _trained_m_probabilities ] vals = [ v for v in vals if isinstance ( v , ( int , float ))] return len ( vals ) > 0 @property def _has_estimated_values ( self ): return self . _has_estimated_m_values and self . _has_estimated_u_values @property def _trained_m_median ( self ): vals = [ r [ \"probability\" ] for r in self . _trained_m_probabilities ] vals = [ v for v in vals if isinstance ( v , ( int , float ))] if len ( vals ) == 0 : return None return median ( vals ) @property def _trained_u_median ( self ): vals = [ r [ \"probability\" ] for r in self . _trained_u_probabilities ] vals = [ v for v in vals if isinstance ( v , ( int , float ))] if len ( vals ) == 0 : return None return median ( vals ) @property def _m_is_trained ( self ): if self . _is_null_level : return True if self . _m_probability == \"level not observed in data\" : return False if self . _m_probability is None : return False return True @property def _u_is_trained ( self ): if self . _is_null_level : return True if self . _u_probability == \"level not observed in data\" : return False if self . _u_probability is None : return False return True @property def _is_trained ( self ): return self . _m_is_trained and self . _u_is_trained @property def _bayes_factor ( self ): if self . _is_null_level : return 1.0 if self . m_probability is None or self . u_probability is None : return None else : return self . m_probability / self . u_probability @property def _log2_bayes_factor ( self ): if self . _is_null_level : return 0.0 else : return math . log2 ( self . _bayes_factor ) @property def _bayes_factor_description ( self ): text = ( f \"If comparison level is ` { self . _label_for_charts . lower () } ` \" \"then comparison is\" ) if self . _bayes_factor >= 1.0 : return f \" { text } { self . _bayes_factor : ,.2f } times more likely to be a match\" else : mult = 1 / self . _bayes_factor return f \" { text } { mult : ,.2f } times less likely to be a match\" @property def _label_for_charts ( self ): return self . _level_dict . get ( \"label_for_charts\" , str ( self . _comparison_vector_value ) ) @property def _is_else_level ( self ): if self . _sql_condition . strip () . upper () == \"ELSE\" : return True @property def _has_tf_adjustments ( self ): col = self . _level_dict . get ( \"tf_adjustment_column\" ) return col is not None @property def _sql_read_dialect ( self ): read_dialect = None if self . _sql_dialect is not None : read_dialect = self . _sql_dialect return read_dialect def _validate_sql ( self ): sql = self . _sql_condition if self . _is_else_level : return True try : sqlglot . parse_one ( sql , read = self . _sql_read_dialect ) except sqlglot . ParseError as e : raise ValueError ( f \"Error parsing sql_statement: \\n { sql } \" ) from e return True @property def _input_columns_used_by_sql_condition ( self ) -> List [ InputColumn ]: # returns e.g. InputColumn(first_name), InputColumn(surname) if self . _is_else_level : return [] cols = get_columns_used_from_sql ( self . _sql_condition , dialect = self . _sql_read_dialect ) # Parsed order seems to be roughly in reverse order of apearance cols = cols [:: - 1 ] cols = [ re . sub ( r \"_L$|_R$\" , \"\" , c , flags = re . IGNORECASE ) for c in cols ] cols = dedupe_preserving_order ( cols ) input_cols = [] for c in cols : # We could have tf adjustments for surname on a dmeta_surname column # If so, we want to set the tf adjustments against the surname col, # not the dmeta_surname one if c == self . _tf_adjustment_input_column_name : input_cols . append ( InputColumn ( c , tf_adjustments = True , sql_dialect = self . _sql_dialect ) ) else : input_cols . append ( InputColumn ( c , tf_adjustments = False , sql_dialect = self . _sql_dialect ) ) return input_cols @property def _columns_to_select_for_blocking ( self ): # e.g. l.first_name as first_name_l, r.first_name as first_name_r output_cols = [] cols = self . _input_columns_used_by_sql_condition for c in cols : output_cols . extend ( c . l_r_names_as_l_r ()) output_cols . extend ( c . l_r_tf_names_as_l_r ()) return dedupe_preserving_order ( output_cols ) @property def _when_then_comparison_vector_value_sql ( self ): # e.g. when first_name_l = first_name_r then 1 if not hasattr ( self , \"_comparison_vector_value\" ): raise ValueError ( \"Cannot get the 'when .. then ...' sql expression because \" \"this comparison level does not belong to a parent Comparison. \" \"The comparison_vector_value is only defined in the \" \"context of a list of ComparisonLevels within a Comparison.\" ) if self . _is_else_level : return f \" { self . _sql_condition } { self . _comparison_vector_value } \" else : return f \"WHEN { self . _sql_condition } THEN { self . _comparison_vector_value } \" @property def _is_exact_match ( self ): if self . _is_else_level : return False sqls = re . split ( r \" and \" , self . _sql_condition , flags = re . IGNORECASE ) for sql in sqls : if not _is_exact_match ( sql ): return False return True @property def _exact_match_colnames ( self ): sqls = re . split ( r \" and \" , self . _sql_condition , flags = re . IGNORECASE ) for sql in sqls : if not _is_exact_match ( sql ): raise ValueError ( \"sql_cond not an exact match so can't get exact match column name\" ) cols = [] for sql in sqls : col = _exact_match_colname ( sql ) cols . append ( col ) return cols @property def _u_probability_corresponding_to_exact_match ( self ): levels = self . comparison . comparison_levels # Find a level with a single exact match colname # which is equal to the tf adjustment input colname for level in levels : if not level . _is_exact_match : continue colnames = level . _exact_match_colnames if len ( colnames ) != 1 : continue if colnames [ 0 ] == self . _tf_adjustment_input_column_name . lower (): return level . u_probability raise ValueError ( \"Could not find an exact match level for \" f \" { self . _tf_adjustment_input_column_name } .\" \" \\n An exact match level is required to make a term frequency adjustment \" \"on a comparison level that is not an exact match.\" ) @property def _bayes_factor_sql ( self ): sql = f \"\"\" WHEN { self . comparison . _gamma_column_name } = { self . _comparison_vector_value } THEN cast( { self . _bayes_factor } as double) \"\"\" return dedent ( sql ) @property def _tf_adjustment_sql ( self ): gamma_column_name = self . comparison . _gamma_column_name gamma_colname_value_is_this_level = ( f \" { gamma_column_name } = { self . _comparison_vector_value } \" ) # A tf adjustment of 1D is a multiplier of 1.0, i.e. no adjustment if self . _comparison_vector_value == - 1 : sql = f \"WHEN { gamma_colname_value_is_this_level } then cast(1 as double)\" elif not self . _has_tf_adjustments : sql = f \"WHEN { gamma_colname_value_is_this_level } then cast(1 as double)\" elif self . _tf_adjustment_weight == 0 : sql = f \"WHEN { gamma_colname_value_is_this_level } then cast(1 as double)\" elif self . _is_else_level : sql = f \"WHEN { gamma_colname_value_is_this_level } then cast(1 as double)\" else : tf_adj_col = self . _tf_adjustment_input_column coalesce_l_r = ( f \"coalesce( { tf_adj_col . tf_name_l () } , { tf_adj_col . tf_name_r () } )\" ) coalesce_r_l = ( f \"coalesce( { tf_adj_col . tf_name_r () } , { tf_adj_col . tf_name_l () } )\" ) tf_adjustment_exists = f \" { coalesce_l_r } is not null\" u_prob_exact_match = self . _u_probability_corresponding_to_exact_match # Using coalesce protects against one of the tf adjustments being null # Which would happen if the user provided their own tf adjustment table # That didn't contain some of the values in this data # In this case rather than taking the greater of the two, we take # whichever value exists if self . _tf_minimum_u_value == 0.0 : divisor_sql = f \"\"\" (CASE WHEN { coalesce_l_r } >= { coalesce_r_l } THEN { coalesce_l_r } ELSE { coalesce_r_l } END) \"\"\" else : # This sql works correctly even when the tf_minimum_u_value is 0.0 # but is less efficient to execute, hence the above if statement divisor_sql = f \"\"\" (CASE WHEN { coalesce_l_r } >= { coalesce_r_l } AND { coalesce_l_r } > cast( { self . _tf_minimum_u_value } as double) THEN { coalesce_l_r } WHEN { coalesce_r_l } > cast( { self . _tf_minimum_u_value } as double) THEN { coalesce_r_l } ELSE cast( { self . _tf_minimum_u_value } as double) END) \"\"\" sql = f \"\"\" WHEN { gamma_colname_value_is_this_level } then (CASE WHEN { tf_adjustment_exists } THEN POW( cast( { u_prob_exact_match } as double) / { divisor_sql } , cast( { self . _tf_adjustment_weight } as double) ) ELSE cast(1 as double) END) \"\"\" return dedent ( sql ) . strip () def as_dict ( self ): \"The minimal representation of this level to use as an input to Splink\" output = {} output [ \"sql_condition\" ] = self . _sql_condition if self . _level_dict . get ( \"label_for_charts\" ): output [ \"label_for_charts\" ] = self . _label_for_charts if self . _m_probability and self . _m_is_trained : output [ \"m_probability\" ] = self . m_probability if self . _u_probability and self . _u_is_trained : output [ \"u_probability\" ] = self . u_probability if self . _has_tf_adjustments : output [ \"tf_adjustment_column\" ] = self . _tf_adjustment_input_column . input_name if self . _tf_adjustment_weight != 0 : output [ \"tf_adjustment_weight\" ] = self . _tf_adjustment_weight if self . _is_null_level : output [ \"is_null_level\" ] = True return output def _as_completed_dict ( self ): comp_dict = self . as_dict () comp_dict [ \"comparison_vector_value\" ] = self . _comparison_vector_value return comp_dict @property def _as_detailed_record ( self ): \"A detailed representation of this level to describe it in charting outputs\" output = {} output [ \"sql_condition\" ] = self . _sql_condition output [ \"label_for_charts\" ] = self . _label_for_charts output [ \"m_probability\" ] = self . m_probability output [ \"u_probability\" ] = self . u_probability output [ \"m_probability_description\" ] = self . _m_probability_description output [ \"u_probability_description\" ] = self . _u_probability_description output [ \"has_tf_adjustments\" ] = self . _has_tf_adjustments if self . _has_tf_adjustments : output [ \"tf_adjustment_column\" ] = self . _tf_adjustment_input_column . input_name else : output [ \"tf_adjustment_column\" ] = None output [ \"tf_adjustment_weight\" ] = self . _tf_adjustment_weight output [ \"is_null_level\" ] = self . _is_null_level output [ \"bayes_factor\" ] = self . _bayes_factor output [ \"log2_bayes_factor\" ] = self . _log2_bayes_factor output [ \"comparison_vector_value\" ] = self . _comparison_vector_value output [ \"max_comparison_vector_value\" ] = self . comparison . _num_levels - 1 output [ \"bayes_factor_description\" ] = self . _bayes_factor_description return output @property def _parameter_estimates_as_records ( self ): output_records = [] cl_record = self . _as_detailed_record trained_values = self . _trained_u_probabilities + self . _trained_m_probabilities for trained_value in trained_values : record = {} record [ \"m_or_u\" ] = trained_value [ \"m_or_u\" ] p = trained_value [ \"probability\" ] record [ \"estimated_probability\" ] = p record [ \"estimate_description\" ] = trained_value [ \"description\" ] if p is not None and p > 0.0 : record [ \"estimated_probability_as_log_odds\" ] = math . log2 ( p / ( 1 - p )) else : record [ \"estimated_probability_as_log_odds\" ] = None record [ \"sql_condition\" ] = cl_record [ \"sql_condition\" ] record [ \"comparison_level_label\" ] = cl_record [ \"label_for_charts\" ] record [ \"comparison_vector_value\" ] = cl_record [ \"comparison_vector_value\" ] output_records . append ( record ) return output_records def _validate ( self ): self . _validate_sql () def _abbreviated_sql ( self , cutoff = 75 ): sql = self . _sql_condition sql = ( sql [: 75 ] + \"...\" ) if len ( sql ) > 75 else sql return sql def __repr__ ( self ): return f \"< { self . _human_readable_succinct } >\" @property def _human_readable_succinct ( self ): sql = self . _abbreviated_sql ( 75 ) return f \"Comparison level ' { self . _label_for_charts } ' using SQL rule: { sql } \" @property def human_readable_description ( self ): input_cols = join_list_with_commas_final_and ( [ c . name ( escape = False ) for c in self . _input_columns_used_by_sql_condition ] ) desc = ( f \"Comparison level: { self . _label_for_charts } of { input_cols } \\n \" \"Assesses similarity between pairwise comparisons of the input columns \" f \"using the following rule \\n { self . _sql_condition } \" ) return desc","title":"Documentation for ComparisonLevel object"},{"location":"comparison_level.html#splink.comparison_level.ComparisonLevel.m_probability","text":"Source code in splink/comparison_level.py 181 182 183 184 185 186 187 188 189 190 @property def m_probability ( self ): if self . _is_null_level : return None if self . _m_probability == \"level not observed in training dataset\" : return 1e-6 if self . _m_probability is None and self . _has_comparison : vals = _default_m_values ( self . comparison . _num_levels ) return vals [ self . _comparison_vector_value ] return self . _m_probability","title":"m_probability()"},{"location":"comparison_level.html#splink.comparison_level.ComparisonLevel.u_probability","text":"Source code in splink/comparison_level.py 207 208 209 210 211 212 213 214 215 216 @property def u_probability ( self ): if self . _is_null_level : return None if self . _u_probability == \"level not observed in training dataset\" : return 1e-6 if self . _u_probability is None : vals = _default_u_values ( self . comparison . _num_levels ) return vals [ self . _comparison_vector_value ] return self . _u_probability","title":"u_probability()"},{"location":"comparison_level_library.html","tags":["API","comparisons"],"text":"Documentation for comparison_level_library \u00b6 distance_function_level ( col_name , distance_function_name , distance_threshold , higher_is_more_similar = True , m_probability = None ) \u00b6 Represents a comparison using a user-provided distance function, where the similarity Parameters: Name Type Description Default col_name str Input column name required distance_function_name str The name of the distance function required distance_threshold Union [ int , float ] The threshold to use to assess similarity required higher_is_more_similar bool If True, a higher value of the distance function indicates a higher similarity (e.g. jaro_winkler). If false, a higher value indicates a lower similarity (e.g. levenshtein). True m_probability float Starting value for m probability. Defaults to None. None Returns: Name Type Description ComparisonLevel ComparisonLevel Source code in splink/comparison_level_library.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def distance_function_level ( col_name : str , distance_function_name : str , distance_threshold : Union [ int , float ], higher_is_more_similar : bool = True , m_probability = None , ) -> ComparisonLevel : \"\"\"Represents a comparison using a user-provided distance function, where the similarity Args: col_name (str): Input column name distance_function_name (str): The name of the distance function distance_threshold (Union[int, float]): The threshold to use to assess similarity higher_is_more_similar (bool): If True, a higher value of the distance function indicates a higher similarity (e.g. jaro_winkler). If false, a higher value indicates a lower similarity (e.g. levenshtein). m_probability (float, optional): Starting value for m probability. Defaults to None. Returns: ComparisonLevel: \"\"\" col = InputColumn ( col_name , sql_dialect = _mutable_params [ \"dialect\" ]) if higher_is_more_similar : operator = \">=\" else : operator = \"<=\" sql_cond = ( f \" { distance_function_name } ( { col . name_l () } , { col . name_r () } ) \" f \" { operator } { distance_threshold } \" ) level_dict = { \"sql_condition\" : sql_cond , \"label_for_charts\" : f \" { distance_function_name } { operator } { distance_threshold } \" , } if m_probability : level_dict [ \"m_probability\" ] = m_probability return ComparisonLevel ( level_dict , sql_dialect = _mutable_params [ \"dialect\" ]) null_level ( col_name ) \u00b6 Represents comparisons where one or both sides of the comparison contains null values so the similarity cannot be evaluated. Assumed to have a partial match weight of zero (null effect on overall match weight) Parameters: Name Type Description Default col_name str Input column name required Returns: Name Type Description ComparisonLevel ComparisonLevel Comparison level Source code in splink/comparison_level_library.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def null_level ( col_name ) -> ComparisonLevel : \"\"\"Represents comparisons where one or both sides of the comparison contains null values so the similarity cannot be evaluated. Assumed to have a partial match weight of zero (null effect on overall match weight) Args: col_name (str): Input column name Returns: ComparisonLevel: Comparison level \"\"\" col = InputColumn ( col_name , sql_dialect = _mutable_params [ \"dialect\" ]) level_dict = { \"sql_condition\" : f \" { col . name_l () } IS NULL OR { col . name_r () } IS NULL\" , \"label_for_charts\" : \"Null\" , \"is_null_level\" : True , } return ComparisonLevel ( level_dict , sql_dialect = _mutable_params [ \"dialect\" ]) exact_match_level ( col_name , m_probability = None , term_frequency_adjustments = False ) \u00b6 Source code in splink/comparison_level_library.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def exact_match_level ( col_name , m_probability = None , term_frequency_adjustments = False ) -> ComparisonLevel : col = InputColumn ( col_name , sql_dialect = _mutable_params [ \"dialect\" ]) level_dict = { \"sql_condition\" : f \" { col . name_l () } = { col . name_r () } \" , \"label_for_charts\" : \"Exact match\" , } if m_probability : level_dict [ \"m_probability\" ] = m_probability if term_frequency_adjustments : level_dict [ \"tf_adjustment_column\" ] = col_name return ComparisonLevel ( level_dict , sql_dialect = _mutable_params [ \"dialect\" ]) levenshtein_level ( col_name , distance_threshold , m_probability = None ) \u00b6 Represents a comparison using a levenshtein distance function, Parameters: Name Type Description Default col_name str Input column name required distance_threshold Union [ int , float ] The threshold to use to assess similarity required m_probability float Starting value for m probability. Defaults to None. None Returns: Name Type Description ComparisonLevel ComparisonLevel Source code in splink/comparison_level_library.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 def levenshtein_level ( col_name : str , distance_threshold : Union [ int , float ], m_probability = None , ) -> ComparisonLevel : \"\"\"Represents a comparison using a levenshtein distance function, Args: col_name (str): Input column name distance_threshold (Union[int, float]): The threshold to use to assess similarity m_probability (float, optional): Starting value for m probability. Defaults to None. Returns: ComparisonLevel: \"\"\" lev_name = _mutable_params [ \"levenshtein\" ] return distance_function_level ( col_name , lev_name , distance_threshold , False , m_probability = None , ) jaccard_level ( col_name , distance_threshold , higher_is_more_similar = True , m_probability = None ) \u00b6 Represents a comparison using a jaccard distance function, Parameters: Name Type Description Default col_name str Input column name required distance_threshold Union [ int , float ] The threshold to use to assess similarity required m_probability float Starting value for m probability. Defaults to None. None Returns: Name Type Description ComparisonLevel ComparisonLevel Source code in splink/comparison_level_library.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 def jaccard_level ( col_name : str , distance_threshold : Union [ int , float ], higher_is_more_similar : bool = True , m_probability = None , ) -> ComparisonLevel : \"\"\"Represents a comparison using a jaccard distance function, Args: col_name (str): Input column name distance_threshold (Union[int, float]): The threshold to use to assess similarity m_probability (float, optional): Starting value for m probability. Defaults to None. Returns: ComparisonLevel: \"\"\" return distance_function_level ( col_name , \"jaccard\" , distance_threshold , True , m_probability = None , ) else_level ( m_probability = None ) \u00b6 Source code in splink/comparison_level_library.py 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 def else_level ( m_probability = None , ) -> ComparisonLevel : if isinstance ( m_probability , str ): raise ValueError ( \"You provided a string for the value of m probability when it should be \" \"numeric. Perhaps you passed a column name. Note that you do not need to \" \"pass a column name into the else level.\" ) level_dict = { \"sql_condition\" : \"ELSE\" , \"label_for_charts\" : \"All other comparisons\" , } if m_probability : level_dict [ \"m_probability\" ] = m_probability return ComparisonLevel ( level_dict ) columns_reversed_level ( col_name_1 , col_name_2 , m_probability = None , tf_adjustment_column = None ) \u00b6 Represents a comparison where the columns are reversed. For example, if surname is in the forename field and vice versa Parameters: Name Type Description Default col_name_1 str First column, e.g. forename required col_name_2 str Second column, e.g. surname required m_probability float Starting value for m probability. Defaults to None. None tf_adjustment_column str Column to use for term frequency adjustments if an exact match is observed. Defaults to None. None Returns: Name Type Description ComparisonLevel ComparisonLevel Source code in splink/comparison_level_library.py 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def columns_reversed_level ( col_name_1 : str , col_name_2 : str , m_probability = None , tf_adjustment_column = None ) -> ComparisonLevel : \"\"\"Represents a comparison where the columns are reversed. For example, if surname is in the forename field and vice versa Args: col_name_1 (str): First column, e.g. forename col_name_2 (str): Second column, e.g. surname m_probability (float, optional): Starting value for m probability. Defaults to None. tf_adjustment_column (str, optional): Column to use for term frequency adjustments if an exact match is observed. Defaults to None. Returns: ComparisonLevel: \"\"\" col_1 = InputColumn ( col_name_1 , sql_dialect = _mutable_params [ \"dialect\" ]) col_2 = InputColumn ( col_name_2 , sql_dialect = _mutable_params [ \"dialect\" ]) s = f \" { col_1 . name_l () } = { col_2 . name_r () } and { col_1 . name_r () } = { col_2 . name_l () } \" level_dict = { \"sql_condition\" : s , \"label_for_charts\" : \"Exact match on reversed cols\" , } if m_probability : level_dict [ \"m_probability\" ] = m_probability if tf_adjustment_column : level_dict [ \"tf_adjustment_column\" ] = tf_adjustment_column return ComparisonLevel ( level_dict , sql_dialect = _mutable_params [ \"dialect\" ])","title":"Comparison Level Library"},{"location":"comparison_level_library.html#documentation-for-comparison_level_library","text":"","title":"Documentation for comparison_level_library"},{"location":"comparison_level_library.html#splink.comparison_level_library.distance_function_level","text":"Represents a comparison using a user-provided distance function, where the similarity Parameters: Name Type Description Default col_name str Input column name required distance_function_name str The name of the distance function required distance_threshold Union [ int , float ] The threshold to use to assess similarity required higher_is_more_similar bool If True, a higher value of the distance function indicates a higher similarity (e.g. jaro_winkler). If false, a higher value indicates a lower similarity (e.g. levenshtein). True m_probability float Starting value for m probability. Defaults to None. None Returns: Name Type Description ComparisonLevel ComparisonLevel Source code in splink/comparison_level_library.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def distance_function_level ( col_name : str , distance_function_name : str , distance_threshold : Union [ int , float ], higher_is_more_similar : bool = True , m_probability = None , ) -> ComparisonLevel : \"\"\"Represents a comparison using a user-provided distance function, where the similarity Args: col_name (str): Input column name distance_function_name (str): The name of the distance function distance_threshold (Union[int, float]): The threshold to use to assess similarity higher_is_more_similar (bool): If True, a higher value of the distance function indicates a higher similarity (e.g. jaro_winkler). If false, a higher value indicates a lower similarity (e.g. levenshtein). m_probability (float, optional): Starting value for m probability. Defaults to None. Returns: ComparisonLevel: \"\"\" col = InputColumn ( col_name , sql_dialect = _mutable_params [ \"dialect\" ]) if higher_is_more_similar : operator = \">=\" else : operator = \"<=\" sql_cond = ( f \" { distance_function_name } ( { col . name_l () } , { col . name_r () } ) \" f \" { operator } { distance_threshold } \" ) level_dict = { \"sql_condition\" : sql_cond , \"label_for_charts\" : f \" { distance_function_name } { operator } { distance_threshold } \" , } if m_probability : level_dict [ \"m_probability\" ] = m_probability return ComparisonLevel ( level_dict , sql_dialect = _mutable_params [ \"dialect\" ])","title":"distance_function_level()"},{"location":"comparison_level_library.html#splink.comparison_level_library.null_level","text":"Represents comparisons where one or both sides of the comparison contains null values so the similarity cannot be evaluated. Assumed to have a partial match weight of zero (null effect on overall match weight) Parameters: Name Type Description Default col_name str Input column name required Returns: Name Type Description ComparisonLevel ComparisonLevel Comparison level Source code in splink/comparison_level_library.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def null_level ( col_name ) -> ComparisonLevel : \"\"\"Represents comparisons where one or both sides of the comparison contains null values so the similarity cannot be evaluated. Assumed to have a partial match weight of zero (null effect on overall match weight) Args: col_name (str): Input column name Returns: ComparisonLevel: Comparison level \"\"\" col = InputColumn ( col_name , sql_dialect = _mutable_params [ \"dialect\" ]) level_dict = { \"sql_condition\" : f \" { col . name_l () } IS NULL OR { col . name_r () } IS NULL\" , \"label_for_charts\" : \"Null\" , \"is_null_level\" : True , } return ComparisonLevel ( level_dict , sql_dialect = _mutable_params [ \"dialect\" ])","title":"null_level()"},{"location":"comparison_level_library.html#splink.comparison_level_library.exact_match_level","text":"Source code in splink/comparison_level_library.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def exact_match_level ( col_name , m_probability = None , term_frequency_adjustments = False ) -> ComparisonLevel : col = InputColumn ( col_name , sql_dialect = _mutable_params [ \"dialect\" ]) level_dict = { \"sql_condition\" : f \" { col . name_l () } = { col . name_r () } \" , \"label_for_charts\" : \"Exact match\" , } if m_probability : level_dict [ \"m_probability\" ] = m_probability if term_frequency_adjustments : level_dict [ \"tf_adjustment_column\" ] = col_name return ComparisonLevel ( level_dict , sql_dialect = _mutable_params [ \"dialect\" ])","title":"exact_match_level()"},{"location":"comparison_level_library.html#splink.comparison_level_library.levenshtein_level","text":"Represents a comparison using a levenshtein distance function, Parameters: Name Type Description Default col_name str Input column name required distance_threshold Union [ int , float ] The threshold to use to assess similarity required m_probability float Starting value for m probability. Defaults to None. None Returns: Name Type Description ComparisonLevel ComparisonLevel Source code in splink/comparison_level_library.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 def levenshtein_level ( col_name : str , distance_threshold : Union [ int , float ], m_probability = None , ) -> ComparisonLevel : \"\"\"Represents a comparison using a levenshtein distance function, Args: col_name (str): Input column name distance_threshold (Union[int, float]): The threshold to use to assess similarity m_probability (float, optional): Starting value for m probability. Defaults to None. Returns: ComparisonLevel: \"\"\" lev_name = _mutable_params [ \"levenshtein\" ] return distance_function_level ( col_name , lev_name , distance_threshold , False , m_probability = None , )","title":"levenshtein_level()"},{"location":"comparison_level_library.html#splink.comparison_level_library.jaccard_level","text":"Represents a comparison using a jaccard distance function, Parameters: Name Type Description Default col_name str Input column name required distance_threshold Union [ int , float ] The threshold to use to assess similarity required m_probability float Starting value for m probability. Defaults to None. None Returns: Name Type Description ComparisonLevel ComparisonLevel Source code in splink/comparison_level_library.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 def jaccard_level ( col_name : str , distance_threshold : Union [ int , float ], higher_is_more_similar : bool = True , m_probability = None , ) -> ComparisonLevel : \"\"\"Represents a comparison using a jaccard distance function, Args: col_name (str): Input column name distance_threshold (Union[int, float]): The threshold to use to assess similarity m_probability (float, optional): Starting value for m probability. Defaults to None. Returns: ComparisonLevel: \"\"\" return distance_function_level ( col_name , \"jaccard\" , distance_threshold , True , m_probability = None , )","title":"jaccard_level()"},{"location":"comparison_level_library.html#splink.comparison_level_library.else_level","text":"Source code in splink/comparison_level_library.py 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 def else_level ( m_probability = None , ) -> ComparisonLevel : if isinstance ( m_probability , str ): raise ValueError ( \"You provided a string for the value of m probability when it should be \" \"numeric. Perhaps you passed a column name. Note that you do not need to \" \"pass a column name into the else level.\" ) level_dict = { \"sql_condition\" : \"ELSE\" , \"label_for_charts\" : \"All other comparisons\" , } if m_probability : level_dict [ \"m_probability\" ] = m_probability return ComparisonLevel ( level_dict )","title":"else_level()"},{"location":"comparison_level_library.html#splink.comparison_level_library.columns_reversed_level","text":"Represents a comparison where the columns are reversed. For example, if surname is in the forename field and vice versa Parameters: Name Type Description Default col_name_1 str First column, e.g. forename required col_name_2 str Second column, e.g. surname required m_probability float Starting value for m probability. Defaults to None. None tf_adjustment_column str Column to use for term frequency adjustments if an exact match is observed. Defaults to None. None Returns: Name Type Description ComparisonLevel ComparisonLevel Source code in splink/comparison_level_library.py 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def columns_reversed_level ( col_name_1 : str , col_name_2 : str , m_probability = None , tf_adjustment_column = None ) -> ComparisonLevel : \"\"\"Represents a comparison where the columns are reversed. For example, if surname is in the forename field and vice versa Args: col_name_1 (str): First column, e.g. forename col_name_2 (str): Second column, e.g. surname m_probability (float, optional): Starting value for m probability. Defaults to None. tf_adjustment_column (str, optional): Column to use for term frequency adjustments if an exact match is observed. Defaults to None. Returns: ComparisonLevel: \"\"\" col_1 = InputColumn ( col_name_1 , sql_dialect = _mutable_params [ \"dialect\" ]) col_2 = InputColumn ( col_name_2 , sql_dialect = _mutable_params [ \"dialect\" ]) s = f \" { col_1 . name_l () } = { col_2 . name_r () } and { col_1 . name_r () } = { col_2 . name_l () } \" level_dict = { \"sql_condition\" : s , \"label_for_charts\" : \"Exact match on reversed cols\" , } if m_probability : level_dict [ \"m_probability\" ] = m_probability if tf_adjustment_column : level_dict [ \"tf_adjustment_column\" ] = tf_adjustment_column return ComparisonLevel ( level_dict , sql_dialect = _mutable_params [ \"dialect\" ])","title":"columns_reversed_level()"},{"location":"comparison_library.html","tags":["API","comparisons"],"text":"Documentation for comparison_library \u00b6 exact_match ( col_name , term_frequency_adjustments = False , m_probability_exact_match = None , m_probability_else = None ) \u00b6 A comparison of the data in col_name with two levels: - Exact match - Anything else Parameters: Name Type Description Default col_name str The name of the column to compare required term_frequency_adjustments bool If True, term frequency adjustments will be made on the exact match level. Defaults to False. False m_probability_exact_match _type_ If provided, overrides the default m probability for the exact match level. Defaults to None. None m_probability_else _type_ If provided, overrides the default m probability for the 'anything else' level. Defaults to None. None Returns: Name Type Description Comparison Comparison A comparison that can be inclued in the Splink settings dictionary Source code in splink/comparison_library.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def exact_match ( col_name , term_frequency_adjustments = False , m_probability_exact_match = None , m_probability_else = None , ) -> Comparison : \"\"\"A comparison of the data in `col_name` with two levels: - Exact match - Anything else Args: col_name (str): The name of the column to compare term_frequency_adjustments (bool, optional): If True, term frequency adjustments will be made on the exact match level. Defaults to False. m_probability_exact_match (_type_, optional): If provided, overrides the default m probability for the exact match level. Defaults to None. m_probability_else (_type_, optional): If provided, overrides the default m probability for the 'anything else' level. Defaults to None. Returns: Comparison: A comparison that can be inclued in the Splink settings dictionary \"\"\" comparison_dict = { \"comparison_description\" : \"Exact match vs. anything else\" , \"comparison_levels\" : [ cl . null_level ( col_name ), cl . exact_match_level ( col_name , term_frequency_adjustments = term_frequency_adjustments , m_probability = m_probability_exact_match , ), cl . else_level ( m_probability = m_probability_else ), ], } return Comparison ( comparison_dict ) distance_function_at_thresholds ( col_name , distance_function_name , distance_threshold_or_thresholds , higher_is_more_similar = True , include_exact_match_level = True , term_frequency_adjustments = False , m_probability_exact_match = None , m_probability_or_probabilities_lev = None , m_probability_else = None ) \u00b6 A comparison of the data in col_name with a user-provided distance function used to assess middle similarity levels. The user-provided distance function must exist in the SQL backend. An example of the output with default arguments and setting distance_function_name to jaccard and distance_threshold_or_thresholds = [0.9,0.7] would be - Exact match - Jaccard distance <= 0.9 - Jaccard distance <= 0.7 - Anything else Parameters: Name Type Description Default col_name str The name of the column to compare required distance_function_name str The name of the distance function required distance_threshold_or_thresholds Union [ int , list ] The threshold(s) to use for the middle similarity level(s). Defaults to [1, 2]. required higher_is_more_similar bool If True, a higher value of the distance function indicates a higher similarity (e.g. jaro_winkler). If false, a higher value indicates a lower similarity (e.g. levenshtein). True include_exact_match_level bool If True, include an exact match level. Defaults to True. True term_frequency_adjustments bool If True, apply term frequency adjustments to the exact match level. Defaults to False. False m_probability_exact_match _type_ If provided, overrides the default m probability for the exact match level. Defaults to None. None m_probability_or_probabilities_lev Union [ float , list ] description . If provided, overrides the default m probabilities for the thresholds specified. Defaults to None. None m_probability_else _type_ If provided, overrides the default m probability for the 'anything else' level. Defaults to None. None Returns: Name Type Description Comparison Comparison Source code in splink/comparison_library.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 def distance_function_at_thresholds ( col_name : str , distance_function_name : str , distance_threshold_or_thresholds : Union [ int , list ], higher_is_more_similar : bool = True , include_exact_match_level = True , term_frequency_adjustments = False , m_probability_exact_match = None , m_probability_or_probabilities_lev : Union [ float , list ] = None , m_probability_else = None , ) -> Comparison : \"\"\"A comparison of the data in `col_name` with a user-provided distance function used to assess middle similarity levels. The user-provided distance function must exist in the SQL backend. An example of the output with default arguments and setting `distance_function_name` to `jaccard` and `distance_threshold_or_thresholds = [0.9,0.7]` would be - Exact match - Jaccard distance <= 0.9 - Jaccard distance <= 0.7 - Anything else Args: col_name (str): The name of the column to compare distance_function_name (str): The name of the distance function distance_threshold_or_thresholds (Union[int, list], optional): The threshold(s) to use for the middle similarity level(s). Defaults to [1, 2]. higher_is_more_similar (bool): If True, a higher value of the distance function indicates a higher similarity (e.g. jaro_winkler). If false, a higher value indicates a lower similarity (e.g. levenshtein). include_exact_match_level (bool, optional): If True, include an exact match level. Defaults to True. term_frequency_adjustments (bool, optional): If True, apply term frequency adjustments to the exact match level. Defaults to False. m_probability_exact_match (_type_, optional): If provided, overrides the default m probability for the exact match level. Defaults to None. m_probability_or_probabilities_lev (Union[float, list], optional): _description_. If provided, overrides the default m probabilities for the thresholds specified. Defaults to None. m_probability_else (_type_, optional): If provided, overrides the default m probability for the 'anything else' level. Defaults to None. Returns: Comparison: \"\"\" distance_thresholds = ensure_is_iterable ( distance_threshold_or_thresholds ) if m_probability_or_probabilities_lev is None : m_probability_or_probabilities_lev = [ None ] * len ( distance_thresholds ) m_probabilities = ensure_is_iterable ( m_probability_or_probabilities_lev ) comparison_levels = [] comparison_levels . append ( cl . null_level ( col_name )) if include_exact_match_level : level = cl . exact_match_level ( col_name , term_frequency_adjustments = term_frequency_adjustments , m_probability = m_probability_exact_match , ) comparison_levels . append ( level ) for thres , m_prob in zip ( distance_thresholds , m_probabilities ): level = cl . distance_function_level ( col_name , distance_function_name = distance_function_name , higher_is_more_similar = higher_is_more_similar , distance_threshold = thres , m_probability = m_prob , ) comparison_levels . append ( level ) comparison_levels . append ( cl . else_level ( m_probability = m_probability_else ), ) comparison_desc = \"\" if include_exact_match_level : comparison_desc += \"Exact match vs. \" thres_desc = \", \" . join ([ str ( d ) for d in distance_thresholds ]) plural = \"\" if len ( distance_thresholds ) == 1 else \"s\" comparison_desc += ( f \" { distance_function_name } at threshold { plural } { thres_desc } vs. \" ) comparison_desc += \"anything else\" comparison_dict = { \"comparison_description\" : comparison_desc , \"comparison_levels\" : comparison_levels , } return Comparison ( comparison_dict ) levenshtein_at_thresholds ( col_name , distance_threshold_or_thresholds = [ 1 , 2 ], include_exact_match_level = True , term_frequency_adjustments = False , m_probability_exact_match = None , m_probability_or_probabilities_lev = None , m_probability_else = None ) \u00b6 A comparison of the data in col_name with the levenshtein distance used to assess middle similarity levels. An example of the output with default arguments and setting distance_threshold_or_thresholds = [1,2] would be - Exact match - levenshtein distance <= 1 - levenshtein distance <= 2 - Anything else Parameters: Name Type Description Default col_name str The name of the column to compare required distance_threshold_or_thresholds Union [ int , list ] The threshold(s) to use for the middle similarity level(s). Defaults to [1, 2]. [1, 2] include_exact_match_level bool If True, include an exact match level. Defaults to True. True term_frequency_adjustments bool If True, apply term frequency adjustments to the exact match level. Defaults to False. False m_probability_exact_match _type_ If provided, overrides the default m probability for the exact match level. Defaults to None. None m_probability_or_probabilities_lev Union [ float , list ] description . If provided, overrides the default m probabilities for the thresholds specified. Defaults to None. None m_probability_else _type_ If provided, overrides the default m probability for the 'anything else' level. Defaults to None. None Returns: Name Type Description Comparison Comparison Source code in splink/comparison_library.py 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 def levenshtein_at_thresholds ( col_name : str , distance_threshold_or_thresholds : Union [ int , list ] = [ 1 , 2 ], include_exact_match_level = True , term_frequency_adjustments = False , m_probability_exact_match = None , m_probability_or_probabilities_lev : Union [ float , list ] = None , m_probability_else = None , ) -> Comparison : \"\"\"A comparison of the data in `col_name` with the levenshtein distance used to assess middle similarity levels. An example of the output with default arguments and setting `distance_threshold_or_thresholds = [1,2]` would be - Exact match - levenshtein distance <= 1 - levenshtein distance <= 2 - Anything else Args: col_name (str): The name of the column to compare distance_threshold_or_thresholds (Union[int, list], optional): The threshold(s) to use for the middle similarity level(s). Defaults to [1, 2]. include_exact_match_level (bool, optional): If True, include an exact match level. Defaults to True. term_frequency_adjustments (bool, optional): If True, apply term frequency adjustments to the exact match level. Defaults to False. m_probability_exact_match (_type_, optional): If provided, overrides the default m probability for the exact match level. Defaults to None. m_probability_or_probabilities_lev (Union[float, list], optional): _description_. If provided, overrides the default m probabilities for the thresholds specified. Defaults to None. m_probability_else (_type_, optional): If provided, overrides the default m probability for the 'anything else' level. Defaults to None. Returns: Comparison: \"\"\" return distance_function_at_thresholds ( col_name , cl . _mutable_params [ \"levenshtein\" ], distance_threshold_or_thresholds , False , include_exact_match_level , term_frequency_adjustments , m_probability_exact_match , m_probability_or_probabilities_lev , m_probability_else , ) jaccard_at_thresholds ( col_name , distance_threshold_or_thresholds = [ 0.9 , 0.7 ], include_exact_match_level = True , term_frequency_adjustments = False , m_probability_exact_match = None , m_probability_or_probabilities_lev = None , m_probability_else = None ) \u00b6 A comparison of the data in col_name with the jaccard distance used to assess middle similarity levels. An example of the output with default arguments and setting distance_threshold_or_thresholds = [1,2] would be - Exact match - Jaccard distance <= 0.9 - Jaccard distance <= 0.7 - Anything else Parameters: Name Type Description Default col_name str The name of the column to compare required distance_threshold_or_thresholds Union [ int , list ] The threshold(s) to use for the middle similarity level(s). Defaults to [0.9, 0.7]. [0.9, 0.7] include_exact_match_level bool If True, include an exact match level. Defaults to True. True term_frequency_adjustments bool If True, apply term frequency adjustments to the exact match level. Defaults to False. False m_probability_exact_match _type_ If provided, overrides the default m probability for the exact match level. Defaults to None. None m_probability_or_probabilities_lev Union [ float , list ] description . If provided, overrides the default m probabilities for the thresholds specified. Defaults to None. None m_probability_else _type_ If provided, overrides the default m probability for the 'anything else' level. Defaults to None. None Returns: Name Type Description Comparison Comparison Source code in splink/comparison_library.py 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 def jaccard_at_thresholds ( col_name : str , distance_threshold_or_thresholds : Union [ int , list ] = [ 0.9 , 0.7 ], include_exact_match_level = True , term_frequency_adjustments = False , m_probability_exact_match = None , m_probability_or_probabilities_lev : Union [ float , list ] = None , m_probability_else = None , ) -> Comparison : \"\"\"A comparison of the data in `col_name` with the jaccard distance used to assess middle similarity levels. An example of the output with default arguments and setting `distance_threshold_or_thresholds = [1,2]` would be - Exact match - Jaccard distance <= 0.9 - Jaccard distance <= 0.7 - Anything else Args: col_name (str): The name of the column to compare distance_threshold_or_thresholds (Union[int, list], optional): The threshold(s) to use for the middle similarity level(s). Defaults to [0.9, 0.7]. include_exact_match_level (bool, optional): If True, include an exact match level. Defaults to True. term_frequency_adjustments (bool, optional): If True, apply term frequency adjustments to the exact match level. Defaults to False. m_probability_exact_match (_type_, optional): If provided, overrides the default m probability for the exact match level. Defaults to None. m_probability_or_probabilities_lev (Union[float, list], optional): _description_. If provided, overrides the default m probabilities for the thresholds specified. Defaults to None. m_probability_else (_type_, optional): If provided, overrides the default m probability for the 'anything else' level. Defaults to None. Returns: Comparison: \"\"\" return distance_function_at_thresholds ( col_name , \"jaccard\" , distance_threshold_or_thresholds , True , include_exact_match_level , term_frequency_adjustments , m_probability_exact_match , m_probability_or_probabilities_lev , m_probability_else , )","title":"Comparison Library"},{"location":"comparison_library.html#documentation-for-comparison_library","text":"","title":"Documentation for comparison_library"},{"location":"comparison_library.html#splink.comparison_library.exact_match","text":"A comparison of the data in col_name with two levels: - Exact match - Anything else Parameters: Name Type Description Default col_name str The name of the column to compare required term_frequency_adjustments bool If True, term frequency adjustments will be made on the exact match level. Defaults to False. False m_probability_exact_match _type_ If provided, overrides the default m probability for the exact match level. Defaults to None. None m_probability_else _type_ If provided, overrides the default m probability for the 'anything else' level. Defaults to None. None Returns: Name Type Description Comparison Comparison A comparison that can be inclued in the Splink settings dictionary Source code in splink/comparison_library.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def exact_match ( col_name , term_frequency_adjustments = False , m_probability_exact_match = None , m_probability_else = None , ) -> Comparison : \"\"\"A comparison of the data in `col_name` with two levels: - Exact match - Anything else Args: col_name (str): The name of the column to compare term_frequency_adjustments (bool, optional): If True, term frequency adjustments will be made on the exact match level. Defaults to False. m_probability_exact_match (_type_, optional): If provided, overrides the default m probability for the exact match level. Defaults to None. m_probability_else (_type_, optional): If provided, overrides the default m probability for the 'anything else' level. Defaults to None. Returns: Comparison: A comparison that can be inclued in the Splink settings dictionary \"\"\" comparison_dict = { \"comparison_description\" : \"Exact match vs. anything else\" , \"comparison_levels\" : [ cl . null_level ( col_name ), cl . exact_match_level ( col_name , term_frequency_adjustments = term_frequency_adjustments , m_probability = m_probability_exact_match , ), cl . else_level ( m_probability = m_probability_else ), ], } return Comparison ( comparison_dict )","title":"exact_match()"},{"location":"comparison_library.html#splink.comparison_library.distance_function_at_thresholds","text":"A comparison of the data in col_name with a user-provided distance function used to assess middle similarity levels. The user-provided distance function must exist in the SQL backend. An example of the output with default arguments and setting distance_function_name to jaccard and distance_threshold_or_thresholds = [0.9,0.7] would be - Exact match - Jaccard distance <= 0.9 - Jaccard distance <= 0.7 - Anything else Parameters: Name Type Description Default col_name str The name of the column to compare required distance_function_name str The name of the distance function required distance_threshold_or_thresholds Union [ int , list ] The threshold(s) to use for the middle similarity level(s). Defaults to [1, 2]. required higher_is_more_similar bool If True, a higher value of the distance function indicates a higher similarity (e.g. jaro_winkler). If false, a higher value indicates a lower similarity (e.g. levenshtein). True include_exact_match_level bool If True, include an exact match level. Defaults to True. True term_frequency_adjustments bool If True, apply term frequency adjustments to the exact match level. Defaults to False. False m_probability_exact_match _type_ If provided, overrides the default m probability for the exact match level. Defaults to None. None m_probability_or_probabilities_lev Union [ float , list ] description . If provided, overrides the default m probabilities for the thresholds specified. Defaults to None. None m_probability_else _type_ If provided, overrides the default m probability for the 'anything else' level. Defaults to None. None Returns: Name Type Description Comparison Comparison Source code in splink/comparison_library.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 def distance_function_at_thresholds ( col_name : str , distance_function_name : str , distance_threshold_or_thresholds : Union [ int , list ], higher_is_more_similar : bool = True , include_exact_match_level = True , term_frequency_adjustments = False , m_probability_exact_match = None , m_probability_or_probabilities_lev : Union [ float , list ] = None , m_probability_else = None , ) -> Comparison : \"\"\"A comparison of the data in `col_name` with a user-provided distance function used to assess middle similarity levels. The user-provided distance function must exist in the SQL backend. An example of the output with default arguments and setting `distance_function_name` to `jaccard` and `distance_threshold_or_thresholds = [0.9,0.7]` would be - Exact match - Jaccard distance <= 0.9 - Jaccard distance <= 0.7 - Anything else Args: col_name (str): The name of the column to compare distance_function_name (str): The name of the distance function distance_threshold_or_thresholds (Union[int, list], optional): The threshold(s) to use for the middle similarity level(s). Defaults to [1, 2]. higher_is_more_similar (bool): If True, a higher value of the distance function indicates a higher similarity (e.g. jaro_winkler). If false, a higher value indicates a lower similarity (e.g. levenshtein). include_exact_match_level (bool, optional): If True, include an exact match level. Defaults to True. term_frequency_adjustments (bool, optional): If True, apply term frequency adjustments to the exact match level. Defaults to False. m_probability_exact_match (_type_, optional): If provided, overrides the default m probability for the exact match level. Defaults to None. m_probability_or_probabilities_lev (Union[float, list], optional): _description_. If provided, overrides the default m probabilities for the thresholds specified. Defaults to None. m_probability_else (_type_, optional): If provided, overrides the default m probability for the 'anything else' level. Defaults to None. Returns: Comparison: \"\"\" distance_thresholds = ensure_is_iterable ( distance_threshold_or_thresholds ) if m_probability_or_probabilities_lev is None : m_probability_or_probabilities_lev = [ None ] * len ( distance_thresholds ) m_probabilities = ensure_is_iterable ( m_probability_or_probabilities_lev ) comparison_levels = [] comparison_levels . append ( cl . null_level ( col_name )) if include_exact_match_level : level = cl . exact_match_level ( col_name , term_frequency_adjustments = term_frequency_adjustments , m_probability = m_probability_exact_match , ) comparison_levels . append ( level ) for thres , m_prob in zip ( distance_thresholds , m_probabilities ): level = cl . distance_function_level ( col_name , distance_function_name = distance_function_name , higher_is_more_similar = higher_is_more_similar , distance_threshold = thres , m_probability = m_prob , ) comparison_levels . append ( level ) comparison_levels . append ( cl . else_level ( m_probability = m_probability_else ), ) comparison_desc = \"\" if include_exact_match_level : comparison_desc += \"Exact match vs. \" thres_desc = \", \" . join ([ str ( d ) for d in distance_thresholds ]) plural = \"\" if len ( distance_thresholds ) == 1 else \"s\" comparison_desc += ( f \" { distance_function_name } at threshold { plural } { thres_desc } vs. \" ) comparison_desc += \"anything else\" comparison_dict = { \"comparison_description\" : comparison_desc , \"comparison_levels\" : comparison_levels , } return Comparison ( comparison_dict )","title":"distance_function_at_thresholds()"},{"location":"comparison_library.html#splink.comparison_library.levenshtein_at_thresholds","text":"A comparison of the data in col_name with the levenshtein distance used to assess middle similarity levels. An example of the output with default arguments and setting distance_threshold_or_thresholds = [1,2] would be - Exact match - levenshtein distance <= 1 - levenshtein distance <= 2 - Anything else Parameters: Name Type Description Default col_name str The name of the column to compare required distance_threshold_or_thresholds Union [ int , list ] The threshold(s) to use for the middle similarity level(s). Defaults to [1, 2]. [1, 2] include_exact_match_level bool If True, include an exact match level. Defaults to True. True term_frequency_adjustments bool If True, apply term frequency adjustments to the exact match level. Defaults to False. False m_probability_exact_match _type_ If provided, overrides the default m probability for the exact match level. Defaults to None. None m_probability_or_probabilities_lev Union [ float , list ] description . If provided, overrides the default m probabilities for the thresholds specified. Defaults to None. None m_probability_else _type_ If provided, overrides the default m probability for the 'anything else' level. Defaults to None. None Returns: Name Type Description Comparison Comparison Source code in splink/comparison_library.py 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 def levenshtein_at_thresholds ( col_name : str , distance_threshold_or_thresholds : Union [ int , list ] = [ 1 , 2 ], include_exact_match_level = True , term_frequency_adjustments = False , m_probability_exact_match = None , m_probability_or_probabilities_lev : Union [ float , list ] = None , m_probability_else = None , ) -> Comparison : \"\"\"A comparison of the data in `col_name` with the levenshtein distance used to assess middle similarity levels. An example of the output with default arguments and setting `distance_threshold_or_thresholds = [1,2]` would be - Exact match - levenshtein distance <= 1 - levenshtein distance <= 2 - Anything else Args: col_name (str): The name of the column to compare distance_threshold_or_thresholds (Union[int, list], optional): The threshold(s) to use for the middle similarity level(s). Defaults to [1, 2]. include_exact_match_level (bool, optional): If True, include an exact match level. Defaults to True. term_frequency_adjustments (bool, optional): If True, apply term frequency adjustments to the exact match level. Defaults to False. m_probability_exact_match (_type_, optional): If provided, overrides the default m probability for the exact match level. Defaults to None. m_probability_or_probabilities_lev (Union[float, list], optional): _description_. If provided, overrides the default m probabilities for the thresholds specified. Defaults to None. m_probability_else (_type_, optional): If provided, overrides the default m probability for the 'anything else' level. Defaults to None. Returns: Comparison: \"\"\" return distance_function_at_thresholds ( col_name , cl . _mutable_params [ \"levenshtein\" ], distance_threshold_or_thresholds , False , include_exact_match_level , term_frequency_adjustments , m_probability_exact_match , m_probability_or_probabilities_lev , m_probability_else , )","title":"levenshtein_at_thresholds()"},{"location":"comparison_library.html#splink.comparison_library.jaccard_at_thresholds","text":"A comparison of the data in col_name with the jaccard distance used to assess middle similarity levels. An example of the output with default arguments and setting distance_threshold_or_thresholds = [1,2] would be - Exact match - Jaccard distance <= 0.9 - Jaccard distance <= 0.7 - Anything else Parameters: Name Type Description Default col_name str The name of the column to compare required distance_threshold_or_thresholds Union [ int , list ] The threshold(s) to use for the middle similarity level(s). Defaults to [0.9, 0.7]. [0.9, 0.7] include_exact_match_level bool If True, include an exact match level. Defaults to True. True term_frequency_adjustments bool If True, apply term frequency adjustments to the exact match level. Defaults to False. False m_probability_exact_match _type_ If provided, overrides the default m probability for the exact match level. Defaults to None. None m_probability_or_probabilities_lev Union [ float , list ] description . If provided, overrides the default m probabilities for the thresholds specified. Defaults to None. None m_probability_else _type_ If provided, overrides the default m probability for the 'anything else' level. Defaults to None. None Returns: Name Type Description Comparison Comparison Source code in splink/comparison_library.py 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 def jaccard_at_thresholds ( col_name : str , distance_threshold_or_thresholds : Union [ int , list ] = [ 0.9 , 0.7 ], include_exact_match_level = True , term_frequency_adjustments = False , m_probability_exact_match = None , m_probability_or_probabilities_lev : Union [ float , list ] = None , m_probability_else = None , ) -> Comparison : \"\"\"A comparison of the data in `col_name` with the jaccard distance used to assess middle similarity levels. An example of the output with default arguments and setting `distance_threshold_or_thresholds = [1,2]` would be - Exact match - Jaccard distance <= 0.9 - Jaccard distance <= 0.7 - Anything else Args: col_name (str): The name of the column to compare distance_threshold_or_thresholds (Union[int, list], optional): The threshold(s) to use for the middle similarity level(s). Defaults to [0.9, 0.7]. include_exact_match_level (bool, optional): If True, include an exact match level. Defaults to True. term_frequency_adjustments (bool, optional): If True, apply term frequency adjustments to the exact match level. Defaults to False. m_probability_exact_match (_type_, optional): If provided, overrides the default m probability for the exact match level. Defaults to None. m_probability_or_probabilities_lev (Union[float, list], optional): _description_. If provided, overrides the default m probabilities for the thresholds specified. Defaults to None. m_probability_else (_type_, optional): If provided, overrides the default m probability for the 'anything else' level. Defaults to None. Returns: Comparison: \"\"\" return distance_function_at_thresholds ( col_name , \"jaccard\" , distance_threshold_or_thresholds , True , include_exact_match_level , term_frequency_adjustments , m_probability_exact_match , m_probability_or_probabilities_lev , m_probability_else , )","title":"jaccard_at_thresholds()"},{"location":"em_training_session.html","tags":["API","Expectation Maximisation"],"text":"Documentation for EMTrainingSession object \u00b6 Manages training models using the Expectation Maximisation algorithm, and holds statistics on the evolution of parameter estimates. Plots diagnostic charts Source code in splink/em_training_session.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 class EMTrainingSession : \"\"\"Manages training models using the Expectation Maximisation algorithm, and holds statistics on the evolution of parameter estimates. Plots diagnostic charts \"\"\" def __init__ ( self , linker : \"Linker\" , blocking_rule_for_training : str , fix_u_probabilities : bool = False , fix_m_probabilities : bool = False , fix_probability_two_random_records_match : bool = False , comparisons_to_deactivate : List [ Comparison ] = None , comparison_levels_to_reverse_blocking_rule : List [ ComparisonLevel ] = None , ): logger . info ( \" \\n ----- Starting EM training session ----- \\n \" ) self . _original_settings_obj = linker . _settings_obj self . _original_linker = linker self . _training_linker = deepcopy ( linker ) self . _settings_obj = self . _training_linker . _settings_obj self . _settings_obj . _retain_matching_columns = False self . _settings_obj . _retain_intermediate_calculation_columns = False self . _settings_obj . _training_mode = True self . _settings_obj . _blocking_rule_for_training = blocking_rule_for_training self . _blocking_rule_for_training = blocking_rule_for_training if comparison_levels_to_reverse_blocking_rule : self . _comparison_levels_to_reverse_blocking_rule = ( comparison_levels_to_reverse_blocking_rule ) else : self . _comparison_levels_to_reverse_blocking_rule = self . _original_settings_obj . _get_comparison_levels_corresponding_to_training_blocking_rule ( # noqa blocking_rule_for_training ) self . _settings_obj . _probability_two_random_records_match = ( self . _blocking_adjusted_probability_two_random_records_match ) self . _training_fix_u_probabilities = fix_u_probabilities self . _training_fix_m_probabilities = fix_m_probabilities self . _training_fix_probability_two_random_records_match = ( fix_probability_two_random_records_match ) # Remove comparison columns which are either 'used up' by the blocking rules # or alternatively, if the user has manually provided a list to remove, # use this instead if not comparisons_to_deactivate : comparisons_to_deactivate = [] br_cols = get_columns_used_from_sql ( blocking_rule_for_training , self . _settings_obj . _sql_dialect ) for cc in self . _settings_obj . comparisons : cc_cols = cc . _input_columns_used_by_case_statement cc_cols = [ c . input_name for c in cc_cols ] if set ( br_cols ) . intersection ( cc_cols ): comparisons_to_deactivate . append ( cc ) cc_names_to_deactivate = [ cc . _output_column_name for cc in comparisons_to_deactivate ] self . _comparisons_that_cannot_be_estimated : List [ Comparison ] = comparisons_to_deactivate filtered_ccs = [ cc for cc in self . _settings_obj . comparisons if cc . _output_column_name not in cc_names_to_deactivate ] self . _settings_obj . comparisons = filtered_ccs self . _comparisons_that_can_be_estimated = filtered_ccs self . _settings_obj_history = [] # Add iteration 0 i.e. the starting parameters self . _add_iteration () def _training_log_message ( self ): not_estimated = [ cc . _output_column_name for cc in self . _comparisons_that_cannot_be_estimated ] not_estimated = \"\" . join ([ f \" \\n - { cc } \" for cc in not_estimated ]) estimated = [ cc . _output_column_name for cc in self . _comparisons_that_can_be_estimated ] estimated = \"\" . join ([ f \" \\n - { cc } \" for cc in estimated ]) if self . _training_fix_m_probabilities and self . _training_fix_u_probabilities : raise ValueError ( \"Can't train model if you fix both m and u probabilites\" ) elif self . _training_fix_u_probabilities : mu = \"m probabilities\" elif self . _training_fix_m_probabilities : mu = \"u probabilities\" else : mu = \"m and u probabilities\" logger . info ( f \"Estimating the { mu } of the model by blocking on: \\n \" f \" { self . _blocking_rule_for_training } \\n\\n \" \"Parameter estimates will be made for the following comparison(s):\" f \" { estimated } \\n \" \" \\n Parameter estimates cannot be made for the following comparison(s)\" f \" since they are used in the blocking rules: { not_estimated } \" ) def _comparison_vectors ( self ): self . _training_log_message () sql = block_using_rules_sql ( self . _training_linker ) self . _training_linker . _enqueue_sql ( sql , \"__splink__df_blocked\" ) is_spark = self . _original_linker . _settings_obj . _sql_dialect == \"spark\" break_after_blocking = getattr ( self . _original_linker , \"break_lineage_after_blocking\" , False ) if is_spark and break_after_blocking : df_blocked = self . _training_linker . _execute_sql_pipeline ([]) sql = compute_comparison_vector_values_sql ( self . _settings_obj ) self . _training_linker . _enqueue_sql ( sql , \"__splink__df_comparison_vectors\" ) return self . _training_linker . _execute_sql_pipeline ([ df_blocked ]) sql = compute_comparison_vector_values_sql ( self . _settings_obj ) self . _training_linker . _enqueue_sql ( sql , \"__splink__df_comparison_vectors\" ) return self . _training_linker . _execute_sql_pipeline ([]) def _train ( self ): cvv = self . _comparison_vectors () # Compute the new params, populating the paramters in the copied settings object # At this stage, we do not overwrite any of the parameters # in the original (main) setting object expectation_maximisation ( self , cvv ) training_desc = f \"EM, blocked on: { self . _blocking_rule_for_training } \" # Add m and u values to original settings for cc in self . _settings_obj . comparisons : orig_cc = self . _original_settings_obj . _get_comparison_by_output_column_name ( cc . _output_column_name ) for cl in cc . _comparison_levels_excluding_null : orig_cl = orig_cc . _get_comparison_level_by_comparison_vector_value ( cl . _comparison_vector_value ) if not self . _training_fix_m_probabilities : not_observed = \"level not observed in training dataset\" if cl . _m_probability == not_observed : orig_cl . _add_trained_m_probability ( not_observed , training_desc ) logger . info ( f \"m probability not trained for { cc . _output_column_name } - \" f \" { cl . _label_for_charts } (comparison vector value: \" f \" { cl . _comparison_vector_value } ). This usually means the \" \"comparison level was never observed in the training data.\" ) else : orig_cl . _add_trained_m_probability ( cl . m_probability , training_desc ) if not self . _training_fix_u_probabilities : not_observed = \"level not observed in training dataset\" if cl . _u_probability == not_observed : orig_cl . _add_trained_u_probability ( not_observed , training_desc ) logger . info ( f \"u probability not trained for { cc . _output_column_name } - \" f \" { cl . _label_for_charts } (comparison vector value: \" f \" { cl . _comparison_vector_value } ). This usually means the \" \"comparison level was never observed in the training data.\" ) else : orig_cl . _add_trained_u_probability ( cl . u_probability , training_desc ) self . _original_linker . _em_training_sessions . append ( self ) def _add_iteration ( self ): self . _settings_obj_history . append ( deepcopy ( self . _settings_obj )) @property def _blocking_adjusted_probability_two_random_records_match ( self ): orig_prop_m = self . _original_settings_obj . _probability_two_random_records_match adj_bayes_factor = prob_to_bayes_factor ( orig_prop_m ) logger . log ( 15 , f \"Original prob two random records match: { orig_prop_m : .3f } \" ) comp_levels = self . _comparison_levels_to_reverse_blocking_rule if not comp_levels : comp_levels = self . _original_settings_obj . _get_comparison_levels_corresponding_to_training_blocking_rule ( # noqa self . _blocking_rule_for_training ) for cl in comp_levels : adj_bayes_factor = cl . _bayes_factor * adj_bayes_factor logger . log ( 15 , f \"Increasing prob two random records match using \" f \" { cl . comparison . _output_column_name } - { cl . _label_for_charts } \" f \" using bayes factor { cl . _bayes_factor : ,.3f } \" , ) adjusted_prop_m = bayes_factor_to_prob ( adj_bayes_factor ) logger . log ( 15 , f \" \\n Prob two random records match adjusted for blocking on \" f \" { self . _blocking_rule_for_training } : \" f \" { adjusted_prop_m : .3f } \" , ) return adjusted_prop_m @property def _iteration_history_records ( self ): output_records = [] for iteration , settings_obj in enumerate ( self . _settings_obj_history ): records = settings_obj . _parameters_as_detailed_records for r in records : r [ \"iteration\" ] = iteration r [ \"probability_two_random_records_match\" ] = self . _settings_obj . _probability_two_random_records_match output_records . extend ( records ) return output_records @property def _lambda_history_records ( self ): output_records = [] for i , s in enumerate ( self . _settings_obj_history ): lam = s . _probability_two_random_records_match r = { \"probability_two_random_records_match\" : lam , \"probability_two_random_records_match_reciprocal\" : 1 / lam , \"iteration\" : i , } output_records . append ( r ) return output_records def probability_two_random_records_match_iteration_chart ( self ): records = self . _lambda_history_records return probability_two_random_records_match_iteration_chart ( records ) def match_weights_interactive_history_chart ( self ): records = self . _iteration_history_records return match_weights_interactive_history_chart ( records , blocking_rule = self . _blocking_rule_for_training ) def m_u_values_interactive_history_chart ( self ): records = self . _iteration_history_records return m_u_parameters_interactive_history_chart ( records ) def _max_change_message ( self , max_change_dict ): message = \"Largest change in params was\" if max_change_dict [ \"max_change_type\" ] == \"probability_two_random_records_match\" : message = ( f \" { message } { max_change_dict [ 'max_change_value' ] : ,.3g } in \" \"probability_two_random_records_match\" ) else : cl = max_change_dict [ \"current_comparison_level\" ] m_u = max_change_dict [ \"max_change_type\" ] cc_name = cl . comparison . _output_column_name cl_label = cl . _label_for_charts level_text = f \" { cc_name } , level ` { cl_label } `\" message = ( f \" { message } { max_change_dict [ 'max_change_value' ] : ,.3g } in \" f \"the { m_u } of { level_text } \" ) return message def _max_change_in_parameters_comparison_levels ( self ): previous_iteration = self . _settings_obj_history [ - 2 ] this_iteration = self . _settings_obj_history [ - 1 ] max_change = - 0.1 max_change_levels = { \"previous_iteration\" : None , \"this_iteration\" : None , \"max_change_type\" : None , \"max_change_value\" : None , } comparisons = zip ( previous_iteration . comparisons , this_iteration . comparisons ) for comparison in comparisons : prev_cc = comparison [ 0 ] this_cc = comparison [ 1 ] z_cls = zip ( prev_cc . comparison_levels , this_cc . comparison_levels ) for z_cl in z_cls : if z_cl [ 0 ] . _is_null_level : continue prev_cl = z_cl [ 0 ] this_cl = z_cl [ 1 ] change_m = this_cl . m_probability - prev_cl . m_probability change_u = this_cl . u_probability - prev_cl . u_probability change = max ( abs ( change_m ), abs ( change_u )) change_type = ( \"m_probability\" if abs ( change_m ) > abs ( change_u ) else \"u_probability\" ) change_value = change_m if abs ( change_m ) > abs ( change_u ) else change_u if change > max_change : max_change = change max_change_levels [ \"prev_comparison_level\" ] = prev_cl max_change_levels [ \"current_comparison_level\" ] = this_cl max_change_levels [ \"max_change_type\" ] = change_type max_change_levels [ \"max_change_value\" ] = change_value max_change_levels [ \"max_abs_change_value\" ] = abs ( change_value ) change_probability_two_random_records_match = ( this_iteration . _probability_two_random_records_match - previous_iteration . _probability_two_random_records_match ) if abs ( change_probability_two_random_records_match ) > max_change : max_change = abs ( change_probability_two_random_records_match ) max_change_levels [ \"prev_comparison_level\" ] = None max_change_levels [ \"current_comparison_level\" ] = None max_change_levels [ \"max_change_type\" ] = \"probability_two_random_records_match\" max_change_levels [ \"max_change_value\" ] = change_probability_two_random_records_match max_change_levels [ \"max_abs_change_value\" ] = abs ( change_probability_two_random_records_match ) max_change_levels [ \"message\" ] = self . _max_change_message ( max_change_levels ) return max_change_levels def __repr__ ( self ): deactivated_cols = \", \" . join ( [ cc . _output_column_name for cc in self . _comparisons_that_cannot_be_estimated ] ) return ( f \"<EMTrainingSession, blocking on { self . _blocking_rule_for_training } , \" f \"deactivating comparisons { deactivated_cols } >\" ) match_weights_interactive_history_chart () \u00b6 Source code in splink/em_training_session.py 284 285 286 287 288 def match_weights_interactive_history_chart ( self ): records = self . _iteration_history_records return match_weights_interactive_history_chart ( records , blocking_rule = self . _blocking_rule_for_training ) m_u_values_interactive_history_chart () \u00b6 Source code in splink/em_training_session.py 290 291 292 def m_u_values_interactive_history_chart ( self ): records = self . _iteration_history_records return m_u_parameters_interactive_history_chart ( records )","title":"EM Training Session API"},{"location":"em_training_session.html#documentation-for-emtrainingsession-object","text":"Manages training models using the Expectation Maximisation algorithm, and holds statistics on the evolution of parameter estimates. Plots diagnostic charts Source code in splink/em_training_session.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 class EMTrainingSession : \"\"\"Manages training models using the Expectation Maximisation algorithm, and holds statistics on the evolution of parameter estimates. Plots diagnostic charts \"\"\" def __init__ ( self , linker : \"Linker\" , blocking_rule_for_training : str , fix_u_probabilities : bool = False , fix_m_probabilities : bool = False , fix_probability_two_random_records_match : bool = False , comparisons_to_deactivate : List [ Comparison ] = None , comparison_levels_to_reverse_blocking_rule : List [ ComparisonLevel ] = None , ): logger . info ( \" \\n ----- Starting EM training session ----- \\n \" ) self . _original_settings_obj = linker . _settings_obj self . _original_linker = linker self . _training_linker = deepcopy ( linker ) self . _settings_obj = self . _training_linker . _settings_obj self . _settings_obj . _retain_matching_columns = False self . _settings_obj . _retain_intermediate_calculation_columns = False self . _settings_obj . _training_mode = True self . _settings_obj . _blocking_rule_for_training = blocking_rule_for_training self . _blocking_rule_for_training = blocking_rule_for_training if comparison_levels_to_reverse_blocking_rule : self . _comparison_levels_to_reverse_blocking_rule = ( comparison_levels_to_reverse_blocking_rule ) else : self . _comparison_levels_to_reverse_blocking_rule = self . _original_settings_obj . _get_comparison_levels_corresponding_to_training_blocking_rule ( # noqa blocking_rule_for_training ) self . _settings_obj . _probability_two_random_records_match = ( self . _blocking_adjusted_probability_two_random_records_match ) self . _training_fix_u_probabilities = fix_u_probabilities self . _training_fix_m_probabilities = fix_m_probabilities self . _training_fix_probability_two_random_records_match = ( fix_probability_two_random_records_match ) # Remove comparison columns which are either 'used up' by the blocking rules # or alternatively, if the user has manually provided a list to remove, # use this instead if not comparisons_to_deactivate : comparisons_to_deactivate = [] br_cols = get_columns_used_from_sql ( blocking_rule_for_training , self . _settings_obj . _sql_dialect ) for cc in self . _settings_obj . comparisons : cc_cols = cc . _input_columns_used_by_case_statement cc_cols = [ c . input_name for c in cc_cols ] if set ( br_cols ) . intersection ( cc_cols ): comparisons_to_deactivate . append ( cc ) cc_names_to_deactivate = [ cc . _output_column_name for cc in comparisons_to_deactivate ] self . _comparisons_that_cannot_be_estimated : List [ Comparison ] = comparisons_to_deactivate filtered_ccs = [ cc for cc in self . _settings_obj . comparisons if cc . _output_column_name not in cc_names_to_deactivate ] self . _settings_obj . comparisons = filtered_ccs self . _comparisons_that_can_be_estimated = filtered_ccs self . _settings_obj_history = [] # Add iteration 0 i.e. the starting parameters self . _add_iteration () def _training_log_message ( self ): not_estimated = [ cc . _output_column_name for cc in self . _comparisons_that_cannot_be_estimated ] not_estimated = \"\" . join ([ f \" \\n - { cc } \" for cc in not_estimated ]) estimated = [ cc . _output_column_name for cc in self . _comparisons_that_can_be_estimated ] estimated = \"\" . join ([ f \" \\n - { cc } \" for cc in estimated ]) if self . _training_fix_m_probabilities and self . _training_fix_u_probabilities : raise ValueError ( \"Can't train model if you fix both m and u probabilites\" ) elif self . _training_fix_u_probabilities : mu = \"m probabilities\" elif self . _training_fix_m_probabilities : mu = \"u probabilities\" else : mu = \"m and u probabilities\" logger . info ( f \"Estimating the { mu } of the model by blocking on: \\n \" f \" { self . _blocking_rule_for_training } \\n\\n \" \"Parameter estimates will be made for the following comparison(s):\" f \" { estimated } \\n \" \" \\n Parameter estimates cannot be made for the following comparison(s)\" f \" since they are used in the blocking rules: { not_estimated } \" ) def _comparison_vectors ( self ): self . _training_log_message () sql = block_using_rules_sql ( self . _training_linker ) self . _training_linker . _enqueue_sql ( sql , \"__splink__df_blocked\" ) is_spark = self . _original_linker . _settings_obj . _sql_dialect == \"spark\" break_after_blocking = getattr ( self . _original_linker , \"break_lineage_after_blocking\" , False ) if is_spark and break_after_blocking : df_blocked = self . _training_linker . _execute_sql_pipeline ([]) sql = compute_comparison_vector_values_sql ( self . _settings_obj ) self . _training_linker . _enqueue_sql ( sql , \"__splink__df_comparison_vectors\" ) return self . _training_linker . _execute_sql_pipeline ([ df_blocked ]) sql = compute_comparison_vector_values_sql ( self . _settings_obj ) self . _training_linker . _enqueue_sql ( sql , \"__splink__df_comparison_vectors\" ) return self . _training_linker . _execute_sql_pipeline ([]) def _train ( self ): cvv = self . _comparison_vectors () # Compute the new params, populating the paramters in the copied settings object # At this stage, we do not overwrite any of the parameters # in the original (main) setting object expectation_maximisation ( self , cvv ) training_desc = f \"EM, blocked on: { self . _blocking_rule_for_training } \" # Add m and u values to original settings for cc in self . _settings_obj . comparisons : orig_cc = self . _original_settings_obj . _get_comparison_by_output_column_name ( cc . _output_column_name ) for cl in cc . _comparison_levels_excluding_null : orig_cl = orig_cc . _get_comparison_level_by_comparison_vector_value ( cl . _comparison_vector_value ) if not self . _training_fix_m_probabilities : not_observed = \"level not observed in training dataset\" if cl . _m_probability == not_observed : orig_cl . _add_trained_m_probability ( not_observed , training_desc ) logger . info ( f \"m probability not trained for { cc . _output_column_name } - \" f \" { cl . _label_for_charts } (comparison vector value: \" f \" { cl . _comparison_vector_value } ). This usually means the \" \"comparison level was never observed in the training data.\" ) else : orig_cl . _add_trained_m_probability ( cl . m_probability , training_desc ) if not self . _training_fix_u_probabilities : not_observed = \"level not observed in training dataset\" if cl . _u_probability == not_observed : orig_cl . _add_trained_u_probability ( not_observed , training_desc ) logger . info ( f \"u probability not trained for { cc . _output_column_name } - \" f \" { cl . _label_for_charts } (comparison vector value: \" f \" { cl . _comparison_vector_value } ). This usually means the \" \"comparison level was never observed in the training data.\" ) else : orig_cl . _add_trained_u_probability ( cl . u_probability , training_desc ) self . _original_linker . _em_training_sessions . append ( self ) def _add_iteration ( self ): self . _settings_obj_history . append ( deepcopy ( self . _settings_obj )) @property def _blocking_adjusted_probability_two_random_records_match ( self ): orig_prop_m = self . _original_settings_obj . _probability_two_random_records_match adj_bayes_factor = prob_to_bayes_factor ( orig_prop_m ) logger . log ( 15 , f \"Original prob two random records match: { orig_prop_m : .3f } \" ) comp_levels = self . _comparison_levels_to_reverse_blocking_rule if not comp_levels : comp_levels = self . _original_settings_obj . _get_comparison_levels_corresponding_to_training_blocking_rule ( # noqa self . _blocking_rule_for_training ) for cl in comp_levels : adj_bayes_factor = cl . _bayes_factor * adj_bayes_factor logger . log ( 15 , f \"Increasing prob two random records match using \" f \" { cl . comparison . _output_column_name } - { cl . _label_for_charts } \" f \" using bayes factor { cl . _bayes_factor : ,.3f } \" , ) adjusted_prop_m = bayes_factor_to_prob ( adj_bayes_factor ) logger . log ( 15 , f \" \\n Prob two random records match adjusted for blocking on \" f \" { self . _blocking_rule_for_training } : \" f \" { adjusted_prop_m : .3f } \" , ) return adjusted_prop_m @property def _iteration_history_records ( self ): output_records = [] for iteration , settings_obj in enumerate ( self . _settings_obj_history ): records = settings_obj . _parameters_as_detailed_records for r in records : r [ \"iteration\" ] = iteration r [ \"probability_two_random_records_match\" ] = self . _settings_obj . _probability_two_random_records_match output_records . extend ( records ) return output_records @property def _lambda_history_records ( self ): output_records = [] for i , s in enumerate ( self . _settings_obj_history ): lam = s . _probability_two_random_records_match r = { \"probability_two_random_records_match\" : lam , \"probability_two_random_records_match_reciprocal\" : 1 / lam , \"iteration\" : i , } output_records . append ( r ) return output_records def probability_two_random_records_match_iteration_chart ( self ): records = self . _lambda_history_records return probability_two_random_records_match_iteration_chart ( records ) def match_weights_interactive_history_chart ( self ): records = self . _iteration_history_records return match_weights_interactive_history_chart ( records , blocking_rule = self . _blocking_rule_for_training ) def m_u_values_interactive_history_chart ( self ): records = self . _iteration_history_records return m_u_parameters_interactive_history_chart ( records ) def _max_change_message ( self , max_change_dict ): message = \"Largest change in params was\" if max_change_dict [ \"max_change_type\" ] == \"probability_two_random_records_match\" : message = ( f \" { message } { max_change_dict [ 'max_change_value' ] : ,.3g } in \" \"probability_two_random_records_match\" ) else : cl = max_change_dict [ \"current_comparison_level\" ] m_u = max_change_dict [ \"max_change_type\" ] cc_name = cl . comparison . _output_column_name cl_label = cl . _label_for_charts level_text = f \" { cc_name } , level ` { cl_label } `\" message = ( f \" { message } { max_change_dict [ 'max_change_value' ] : ,.3g } in \" f \"the { m_u } of { level_text } \" ) return message def _max_change_in_parameters_comparison_levels ( self ): previous_iteration = self . _settings_obj_history [ - 2 ] this_iteration = self . _settings_obj_history [ - 1 ] max_change = - 0.1 max_change_levels = { \"previous_iteration\" : None , \"this_iteration\" : None , \"max_change_type\" : None , \"max_change_value\" : None , } comparisons = zip ( previous_iteration . comparisons , this_iteration . comparisons ) for comparison in comparisons : prev_cc = comparison [ 0 ] this_cc = comparison [ 1 ] z_cls = zip ( prev_cc . comparison_levels , this_cc . comparison_levels ) for z_cl in z_cls : if z_cl [ 0 ] . _is_null_level : continue prev_cl = z_cl [ 0 ] this_cl = z_cl [ 1 ] change_m = this_cl . m_probability - prev_cl . m_probability change_u = this_cl . u_probability - prev_cl . u_probability change = max ( abs ( change_m ), abs ( change_u )) change_type = ( \"m_probability\" if abs ( change_m ) > abs ( change_u ) else \"u_probability\" ) change_value = change_m if abs ( change_m ) > abs ( change_u ) else change_u if change > max_change : max_change = change max_change_levels [ \"prev_comparison_level\" ] = prev_cl max_change_levels [ \"current_comparison_level\" ] = this_cl max_change_levels [ \"max_change_type\" ] = change_type max_change_levels [ \"max_change_value\" ] = change_value max_change_levels [ \"max_abs_change_value\" ] = abs ( change_value ) change_probability_two_random_records_match = ( this_iteration . _probability_two_random_records_match - previous_iteration . _probability_two_random_records_match ) if abs ( change_probability_two_random_records_match ) > max_change : max_change = abs ( change_probability_two_random_records_match ) max_change_levels [ \"prev_comparison_level\" ] = None max_change_levels [ \"current_comparison_level\" ] = None max_change_levels [ \"max_change_type\" ] = \"probability_two_random_records_match\" max_change_levels [ \"max_change_value\" ] = change_probability_two_random_records_match max_change_levels [ \"max_abs_change_value\" ] = abs ( change_probability_two_random_records_match ) max_change_levels [ \"message\" ] = self . _max_change_message ( max_change_levels ) return max_change_levels def __repr__ ( self ): deactivated_cols = \", \" . join ( [ cc . _output_column_name for cc in self . _comparisons_that_cannot_be_estimated ] ) return ( f \"<EMTrainingSession, blocking on { self . _blocking_rule_for_training } , \" f \"deactivating comparisons { deactivated_cols } >\" )","title":"Documentation for EMTrainingSession object"},{"location":"em_training_session.html#splink.em_training_session.EMTrainingSession.match_weights_interactive_history_chart","text":"Source code in splink/em_training_session.py 284 285 286 287 288 def match_weights_interactive_history_chart ( self ): records = self . _iteration_history_records return match_weights_interactive_history_chart ( records , blocking_rule = self . _blocking_rule_for_training )","title":"match_weights_interactive_history_chart()"},{"location":"em_training_session.html#splink.em_training_session.EMTrainingSession.m_u_values_interactive_history_chart","text":"Source code in splink/em_training_session.py 290 291 292 def m_u_values_interactive_history_chart ( self ): records = self . _iteration_history_records return m_u_parameters_interactive_history_chart ( records )","title":"m_u_values_interactive_history_chart()"},{"location":"linker.html","tags":["API"],"text":"Documentation for Linker object \u00b6 Manages the data linkage process and holds the data linkage model. Source code in splink/linker.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 1601 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 1617 1618 1619 1620 1621 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631 1632 1633 1634 1635 1636 1637 1638 1639 1640 1641 1642 1643 1644 1645 1646 1647 1648 1649 1650 1651 1652 1653 1654 1655 1656 1657 1658 1659 1660 1661 1662 1663 1664 1665 1666 1667 1668 1669 1670 1671 1672 1673 1674 1675 1676 1677 1678 1679 1680 1681 1682 1683 1684 1685 1686 1687 1688 1689 1690 1691 1692 1693 1694 1695 1696 1697 1698 1699 1700 1701 1702 1703 1704 1705 1706 1707 1708 1709 1710 1711 1712 1713 1714 1715 1716 1717 1718 1719 1720 1721 1722 1723 1724 1725 1726 1727 1728 1729 1730 1731 1732 1733 1734 class Linker : \"\"\"Manages the data linkage process and holds the data linkage model.\"\"\" def __init__ ( self , input_table_or_tables : Union [ str , list ], settings_dict : dict = None , set_up_basic_logging : bool = True , input_table_aliases : Union [ str , list ] = None , ): \"\"\"The Linker object manages the data linkage process and holds the data linkage model. Most of Splink's functionality can be accessed by calling methods (functions) on the linker, such as `linker.predict()`, `linker.profile_columns()` etc. The Linker class is intended for subclassing for specific backends, e.g. a DuckDBLinker. Args: input_table_or_tables (Union[str, list]): Input data into the linkage model. Either a single string (the name of a table in a database) for deduplication jobs, or a list of strings (the name of tables in a database) for link_only or link_and_dedupe settings_dict (dict, optional): A Splink settings dictionary. If not provided when the object is created, can later be added using `linker.initialise_settings()` Defaults to None. set_up_basic_logging (bool, optional): If true, sets ups up basic logging so that Splink sends messages at INFO level to stdout. Defaults to True. input_table_aliases (Union[str, list], optional): Labels assigned to input tables in Splink outputs. If the names of the tables in the input database are long or unspecific, this argument can be used to attach more easily readable/interpretable names. Defaults to None. \"\"\" self . _pipeline = SQLPipeline () self . _settings_dict = settings_dict if settings_dict is None : self . _settings_obj_ = None else : self . _settings_obj_ = Settings ( settings_dict ) self . _input_tables_dict = self . _get_input_tables_dict ( input_table_or_tables , input_table_aliases ) self . _validate_input_dfs () self . _em_training_sessions = [] self . _names_of_tables_created_by_splink : list = [] self . _find_new_matches_mode = False self . _train_u_using_random_sample_mode = False self . _compare_two_records_mode = False self . _self_link_mode = False self . _output_schema = \"\" self . debug_mode = False if set_up_basic_logging : logging . basicConfig ( format = \" %(message)s \" , ) splink_logger = logging . getLogger ( \"splink\" ) splink_logger . setLevel ( logging . INFO ) @property def _settings_obj ( self ) -> Settings : if self . _settings_obj_ is None : raise ValueError ( \"You did not provide a settings dictionary when you \" \"created the linker. To continue, you need to provide a settings \" \"dictionary using the `initialise_settings()` method on your linker \" \"object. i.e. linker.initialise_settings(settings_dict)\" ) return self . _settings_obj_ @property def _input_tablename_l ( self ): if self . _find_new_matches_mode : return \"__splink__df_concat_with_tf\" if self . _self_link_mode : return \"__splink__df_concat_with_tf\" if self . _compare_two_records_mode : return \"__splink__compare_two_records_left_with_tf\" if self . _train_u_using_random_sample_mode : return \"__splink__df_concat_with_tf_sample\" if self . _two_dataset_link_only : return \"__splink_df_concat_with_tf_left\" return \"__splink__df_concat_with_tf\" @property def _input_tablename_r ( self ): if self . _find_new_matches_mode : return \"__splink__df_new_records_with_tf\" if self . _self_link_mode : return \"__splink__df_concat_with_tf\" if self . _compare_two_records_mode : return \"__splink__compare_two_records_right_with_tf\" if self . _train_u_using_random_sample_mode : return \"__splink__df_concat_with_tf_sample\" if self . _two_dataset_link_only : return \"__splink_df_concat_with_tf_right\" return \"__splink__df_concat_with_tf\" @property def _two_dataset_link_only ( self ): # Two dataset link only join is a special case where an inner join of the # two datasets is much more efficient than self-joining the vertically # concatenation of all input datasets if self . _find_new_matches_mode : return True if self . _compare_two_records_mode : return True if ( len ( self . _input_tables_dict ) == 2 and self . _settings_obj . _link_type == \"link_only\" ): return True else : return False def _prepend_schema_to_table_name ( self , table_name ): if self . _output_schema : return f \" { self . _output_schema } . { table_name } \" else : return table_name def _initialise_df_concat ( self , materialise = True ): if self . _table_exists_in_database ( \"__splink__df_concat\" ): return sql = vertically_concatenate_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_concat\" ) self . _execute_sql_pipeline ( materialise_as_hash = False ) def _initialise_df_concat_with_tf ( self , materialise = True ): if self . _table_exists_in_database ( \"__splink__df_concat_with_tf\" ): return sql = vertically_concatenate_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_concat\" ) sqls = compute_all_term_frequencies_sqls ( self ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) if self . _two_dataset_link_only : # If we do not materialise __splink_df_concat_with_tf # we'd have to run all the code up to this point twice self . _execute_sql_pipeline ( materialise_as_hash = False ) source_dataset_col = self . _settings_obj . _source_dataset_column_name # Need df_l to be the one with the lowest id to preeserve the property # that the left dataset is the one with the lowest concatenated id keys = self . _input_tables_dict . keys () keys = list ( sorted ( keys )) df_l = self . _input_tables_dict [ keys [ 0 ]] df_r = self . _input_tables_dict [ keys [ 1 ]] sql = f \"\"\" select * from __splink__df_concat_with_tf where { source_dataset_col } = ' { df_l . templated_name } ' \"\"\" self . _enqueue_sql ( sql , \"__splink_df_concat_with_tf_left\" ) self . _execute_sql_pipeline ( materialise_as_hash = False ) sql = f \"\"\" select * from __splink__df_concat_with_tf where { source_dataset_col } = ' { df_r . templated_name } ' \"\"\" self . _enqueue_sql ( sql , \"__splink_df_concat_with_tf_right\" ) self . _execute_sql_pipeline ( materialise_as_hash = False ) else : if materialise : self . _execute_sql_pipeline ( materialise_as_hash = False ) def _table_to_splink_dataframe ( self , templated_name , physical_name ) -> SplinkDataFrame : \"\"\"Create a SplinkDataframe from a table in the underlying database called `physical_name`. Associate a `templated_name` with this table, which signifies the purpose or 'meaning' of this table to splink. (e.g. `__splink__df_blocked`) Args: templated_name (str): The purpose of the table to Splink physical_name (str): The name of the table in the underlying databse \"\"\" raise NotImplementedError ( \"_table_to_splink_dataframe not implemented on this linker\" ) def _enqueue_sql ( self , sql , output_table_name ): \"\"\"Add sql to the current pipeline, but do not execute the pipeline.\"\"\" self . _pipeline . enqueue_sql ( sql , output_table_name ) def _execute_sql_pipeline ( self , input_dataframes : List [ SplinkDataFrame ] = [], materialise_as_hash = True , use_cache = True , transpile = True , ) -> SplinkDataFrame : \"\"\"Execute the SQL queued in the current pipeline as a single statement e.g. `with a as (), b as , c as (), select ... from c`, then execute the pipeline, returning the resultant table as a SplinkDataFrame Args: input_dataframes (List[SplinkDataFrame], optional): A 'starting point' of SplinkDataFrames if needed. Defaults to []. materialise_as_hash (bool, optional): If true, the output tablename will end in a unique identifer. Defaults to True. use_cache (bool, optional): If true, look at whether the SQL pipeline has been executed before, and if so, use the existing result. Defaults to True. transpile (bool, optional): Transpile the SQL using SQLGlot. Defaults to True. Returns: SplinkDataFrame: An abstraction representing the table created by the sql pipeline \"\"\" if not self . debug_mode : sql_gen = self . _pipeline . _generate_pipeline ( input_dataframes ) output_tablename_templated = self . _pipeline . queue [ - 1 ] . output_table_name dataframe = self . _sql_to_splink_dataframe ( sql_gen , output_tablename_templated , materialise_as_hash , use_cache , transpile , ) return dataframe else : # In debug mode, we do not pipeline the sql and print the # results of each part of the pipeline for task in self . _pipeline . _generate_pipeline_parts ( input_dataframes ): output_tablename = task . output_table_name sql = task . sql print ( \"------\" ) print ( f \"--------Creating table: { output_tablename } --------\" ) dataframe = self . _sql_to_splink_dataframe ( sql , output_tablename , materialise_as_hash = False , use_cache = False , transpile = transpile , ) return dataframe def _execute_sql ( self , sql , templated_name , physical_name , transpile = True ): raise NotImplementedError ( f \"execute_sql not implemented for { type ( self ) } \" ) def _enqueue_and_execute_sql_pipeline ( self , sql , output_table_name , materialise_as_hash = True , use_cache = True , transpile = True , ) -> SplinkDataFrame : \"\"\"Wrapper method to enqueue and execute a sql pipeline in a single call.\"\"\" self . _enqueue_sql ( sql , output_table_name ) return self . _execute_sql_pipeline ([], materialise_as_hash , use_cache , transpile ) def _sql_to_splink_dataframe ( self , sql , output_tablename_templated , materialise_as_hash = True , use_cache = True , transpile = True , ) -> SplinkDataFrame : \"\"\"Execute sql (or if identical sql has been run before, return cached results), reset pipeline, and return a SplinkDataFrame representing the results of the sql\"\"\" self . _pipeline . reset () hash = hashlib . sha256 ( sql . encode ()) . hexdigest ()[: 7 ] # Ensure hash is valid sql table name table_name_hash = f \" { output_tablename_templated } _ { hash } \" if use_cache : if self . _table_exists_in_database ( output_tablename_templated ): logger . debug ( f \"Using existing table { output_tablename_templated } \" ) return self . _table_to_splink_dataframe ( output_tablename_templated , output_tablename_templated ) if self . _table_exists_in_database ( table_name_hash ): logger . debug ( f \"Using cache for { output_tablename_templated } \" f \" with physical name { table_name_hash } \" ) return self . _table_to_splink_dataframe ( output_tablename_templated , table_name_hash ) if self . debug_mode : print ( sql ) if materialise_as_hash : splink_dataframe = self . _execute_sql ( sql , output_tablename_templated , table_name_hash , transpile = transpile ) else : splink_dataframe = self . _execute_sql ( sql , output_tablename_templated , output_tablename_templated , transpile = transpile , ) self . _names_of_tables_created_by_splink . append ( splink_dataframe . physical_name ) if self . debug_mode : df_pd = splink_dataframe . as_pandas_dataframe () try : from IPython.display import display display ( df_pd ) except ModuleNotFoundError : print ( df_pd ) return splink_dataframe def __deepcopy__ ( self , memo ): \"\"\"When we do EM training, we need a copy of the linker which is independent of the main linker e.g. setting parameters on the copy will not affect the main linker. This method implements ensures linker can be deepcopied. \"\"\" new_linker = copy ( self ) new_linker . _em_training_sessions = [] new_settings = deepcopy ( self . _settings_obj ) new_linker . _settings_obj_ = new_settings return new_linker def _ensure_aliases_populated_and_is_list ( self , input_table_or_tables , input_table_aliases ): if input_table_aliases is None : input_table_aliases = input_table_or_tables input_table_aliases = ensure_is_list ( input_table_aliases ) return input_table_aliases def _get_input_tables_dict ( self , input_table_or_tables , input_table_aliases ): input_table_or_tables = ensure_is_list ( input_table_or_tables ) input_table_aliases = self . _ensure_aliases_populated_and_is_list ( input_table_or_tables , input_table_aliases ) d = {} for table_name , table_alias in zip ( input_table_or_tables , input_table_aliases ): d [ table_alias ] = self . _table_to_splink_dataframe ( table_alias , table_name ) return d def _get_input_tf_dict ( self , df_dict ): d = {} for df_name , df_value in df_dict . items (): renamed = colname_to_tf_tablename ( df_name ) d [ renamed ] = self . _table_to_splink_dataframe ( renamed , df_value ) return d def _predict_warning ( self ): if not self . _settings_obj . _is_fully_trained : msg = ( \" \\n -- WARNING -- \\n \" \"You have called predict(), but there are some parameter \" \"estimates which have neither been estimated or specified in your \" \"settings dictionary. To produce predictions the following\" \" untrained trained parameters will use default values.\" ) messages = self . _settings_obj . _not_trained_messages () warn_message = \" \\n \" . join ([ msg ] + messages ) logger . warning ( warn_message ) def _table_exists_in_database ( self , table_name ): raise NotImplementedError ( f \"table_exists_in_database not implemented for { type ( self ) } \" ) def _validate_input_dfs ( self ): for df in self . _input_tables_dict . values (): df . validate () if self . _settings_obj_ is not None : if self . _settings_obj . _link_type == \"dedupe_only\" : if len ( self . _input_tables_dict ) > 1 : raise ValueError ( 'If link_type = \"dedupe only\" then input tables must contain' \"only a single input table\" , ) def _populate_probability_two_random_records_match_from_trained_values ( self ): recip_prop_matches_estimates = [] logger . log ( 15 , ( \"---- Using training sessions to compute \" \"probability two random records match ----\" ), ) for em_training_session in self . _em_training_sessions : training_lambda = ( em_training_session . _settings_obj . _probability_two_random_records_match ) training_lambda_bf = prob_to_bayes_factor ( training_lambda ) reverse_levels = ( em_training_session . _comparison_levels_to_reverse_blocking_rule ) logger . log ( 15 , \" \\n \" f \"Probability two random records match from trained model blocking on \" f \" { em_training_session . _blocking_rule_for_training } : \" f \" { training_lambda : ,.3f } \" , ) for reverse_level in reverse_levels : # Get comparison level on current settings obj cc = self . _settings_obj . _get_comparison_by_output_column_name ( reverse_level . comparison . _output_column_name ) cl = cc . _get_comparison_level_by_comparison_vector_value ( reverse_level . _comparison_vector_value ) if cl . _has_estimated_values : bf = cl . _trained_m_median / cl . _trained_u_median else : bf = cl . _bayes_factor logger . log ( 15 , f \"Reversing comparison level { cc . _output_column_name } \" f \" using bayes factor { bf : ,.3f } \" , ) training_lambda_bf = training_lambda_bf / bf as_prob = bayes_factor_to_prob ( training_lambda_bf ) logger . log ( 15 , ( \"This estimate of probability two random records match now: \" f \" { as_prob : ,.3f } \" f \"with reciprocal { ( 1 / as_prob ) : ,.3f } \" ), ) logger . log ( 15 , \" \\n ---------\" ) p = bayes_factor_to_prob ( training_lambda_bf ) recip_prop_matches_estimates . append ( 1 / p ) prop_matches_estimate = 1 / median ( recip_prop_matches_estimates ) self . _settings_obj . _probability_two_random_records_match = prop_matches_estimate logger . log ( 15 , \" \\n Median of prop of matches estimates: \" f \" { self . _settings_obj . _probability_two_random_records_match : ,.3f } \" \"reciprocal \" f \" { 1 / self . _settings_obj . _probability_two_random_records_match : ,.3f } \" , ) def _records_to_table ( records , as_table_name ): # Create table in database containing records # Probably quite difficult to implement correctly # Due to data type issues. raise NotImplementedError def _populate_m_u_from_trained_values ( self ): ccs = self . _settings_obj . comparisons for cc in ccs : for cl in cc . _comparison_levels_excluding_null : if cl . _has_estimated_u_values : cl . u_probability = cl . _trained_u_median if cl . _has_estimated_m_values : cl . m_probability = cl . _trained_m_median def _delete_tables_created_by_splink_from_db ( self , retain_term_frequency = True , retain_df_concat_with_tf = True ): tables_remaining = [] for name in self . _names_of_tables_created_by_splink : # Only delete tables explicitly marked as having been created by splink if \"__splink__\" not in name : tables_remaining . append ( name ) continue if name == \"__splink__df_concat_with_tf\" : if retain_df_concat_with_tf : tables_remaining . append ( name ) else : self . _delete_table_from_database ( name ) elif name . startswith ( \"__splink__df_tf_\" ): if retain_term_frequency : tables_remaining . append ( name ) else : self . _delete_table_from_database ( name ) else : self . _delete_table_from_database ( name ) self . _names_of_tables_created_by_splink = tables_remaining def _raise_error_if_necessary_waterfall_columns_not_computed ( self ): ricc = self . _settings_obj . _retain_intermediate_calculation_columns rmc = self . _settings_obj . _retain_matching_columns if not ( ricc and rmc ): raise ValueError ( \"retain_intermediate_calculation_columns and \" \"retain_matching_columns must both be set to True in your settings\" \" dictionary to use this function, because otherwise the necessary \" \"columns will not be available in the input records.\" f \" Their current values are { ricc } and { rmc } , respectively. \" \"Please re-run your linkage with them both set to True.\" ) def initialise_settings ( self , settings_dict : dict ): \"\"\"Initialise settings for the linker. To be used if settings were not passed to the linker on creation. Examples: >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> linker.profile_columns(\"first_name\", \"surname\") >>> linker.initialise_settings(settings_dict) Args: settings_dict (dict): A Splink settings dictionary \"\"\" self . _settings_dict = settings_dict self . _settings_obj_ = Settings ( settings_dict ) self . _validate_input_dfs () def compute_tf_table ( self , column_name : str ) -> SplinkDataFrame : \"\"\"Compute a term frequency table for a given column and persist to the database This method is useful if you want to pre-compute term frequency tables e.g. so that real time linkage executes faster, or so that you can estimate various models without having to recompute term frequency tables each time Examples: >>> # Example 1: Real time linkage >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> linker.load_settings_from_json(\"saved_settings.json\") >>> linker.compute_tf_table(\"surname\") >>> linker.compare_two_records(record_left, record_right) >>> # Example 2: Pre-computed term frequency tables in Spark >>> linker = SparkLinker(df) >>> df_first_name_tf = linker.compute_tf_table(\"first_name\") >>> df_first_name_tf.write.parquet(\"folder/first_name_tf\") >>> >>> # On subsequent data linking job, read this table rather than recompute >>> df_first_name_tf = spark.read.parquet(\"folder/first_name_tf\") >>> df_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\") Args: column_name (str): The column name in the input table Returns: SplinkDataFrame: The resultant table as a splink data frame \"\"\" sql = vertically_concatenate_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_concat\" ) input_col = InputColumn ( column_name , tf_adjustments = True ) sql = term_frequencies_for_single_column_sql ( input_col ) self . _enqueue_sql ( sql , colname_to_tf_tablename ( input_col )) return self . _execute_sql_pipeline ( materialise_as_hash = False ) def deterministic_link ( self ) -> SplinkDataFrame : \"\"\"Uses the blocking rules specified by `blocking_rules_to_generate_predictions` in the settings dictionary to generate pairwise record comparisons. This should be a list of blocking rules which are strict enough to generate only true links. Deterministic linkage, however, is likely to result in missed links (false negatives). Examples: >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> >>> settings = { >>> \"link_type\": \"dedupe_only\", >>> \"blocking_rules_to_generate_predictions\": [ >>> \"l.first_name = r.first_name\", >>> \"l.surname = r.surname\", >>> ], >>> \"comparisons\": [] >>> } >>> >>> from splink.duckdb.duckdb_linker import DuckDBLinker >>> >>> linker = DuckDBLinker(df, settings, connection=\":memory:\") >>> df = linker.deterministic_link() Returns: SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons. This represents a table materialised in the database. Methods on the SplinkDataFrame allow you to access the underlying data. \"\"\" self . _initialise_df_concat_with_tf () sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) return self . _execute_sql_pipeline () def estimate_u_using_random_sampling ( self , target_rows : int ): \"\"\"Estimate the u parameters of the linkage model using random sampling. The u parameters represent the proportion of record comparisons that fall into each comparison level amongst truly non-matching records. This procedure takes a sample of the data and generates the cartesian product of pairwise record comparisons amongst the sampled records. The validity of the u values rests on the assumption that the resultant pairwise comparisons are non-matches (or at least, they are very unlikely to be matches). For large datasets, this is typically true. Args: target_rows (int): The target number of pairwise record comparisons from which to derive the u values. Larger will give more accurate estimates but lead to longer runtimes. In our experience at least 1e9 (one billion) gives best results but can take a long time to compute. 1e7 (ten million) is often adequate whilst testing different model specifications, before the final model is estimated. Examples: >>> linker.estimate_u_using_random_sampling(1e8) Returns: Updates the estimated u parameters within the linker object and returns nothing. \"\"\" self . _initialise_df_concat_with_tf ( materialise = True ) estimate_u_values ( self , target_rows ) self . _populate_m_u_from_trained_values () self . _settings_obj . _columns_without_estimated_parameters_message () def estimate_m_from_label_column ( self , label_colname : str ): \"\"\"Estimate the m parameters of the linkage model from a label (ground truth) column in the input dataframe(s). The m parameters represent the proportion of record comparisons that fall into each comparison level amongst truly matching records. The ground truth column is used to generate pairwise record comparisons which are then assumed to be matches. For example, if the entity being matched is persons, and your input dataset(s) contain social security number, this could be used to estimate the m values for the model. Note that this column does not need to be fully populated. A common case is where a unique identifier such as social security number is only partially populated. Args: label_colname (str): The name of the column containing the ground truth label in the input data. Examples: >>> linker.estimate_m_from_label_column(\"social_security_number\") Returns: Updates the estimated m parameters within the linker object and returns nothing. \"\"\" self . _initialise_df_concat_with_tf ( materialise = True ) estimate_m_values_from_label_column ( self , self . _input_tables_dict , label_colname ) self . _populate_m_u_from_trained_values () self . _settings_obj . _columns_without_estimated_parameters_message () def estimate_parameters_using_expectation_maximisation ( self , blocking_rule : str , comparisons_to_deactivate : List [ Union [ str , Comparison ]] = None , comparison_levels_to_reverse_blocking_rule : List [ ComparisonLevel ] = None , fix_probability_two_random_records_match : bool = False , fix_m_probabilities = False , fix_u_probabilities = True , ) -> EMTrainingSession : \"\"\"Estimate the parameters of the linkage model using expectation maximisation. By default, the m probabilities are estimated, but not the u probabilities, because good estiamtes for the u probabilities can be obtained from `linker.estimate_u_using_random_sampling()`. You can change this by setting `fix_u_probabilities` to False. The blocking rule provided is used to generate pairwise record comparisons. Usually, this should be a blocking rule that results in a dataframe where matches are between about 1% and 99% of the comparisons. By default, m parameters are estimated for all comparisons except those which are included in the blocking rule. For example, if the blocking rule is `l.first_name = r.first_name`, then parameter esimates will be made for all comparison except those which use `first_name` in their sql_condition By default, the probability two random records match is estimated for the blocked data, and then the m and u parameters for the columns specified in the blocking rules are used to estiamte the global probability two random records match. To control which comparisons should have their parameter estimated, and the process of 'reversing out' the global probability two random records match, the user may specify `comparisons_to_deactivate` and `comparison_levels_to_reverse_blocking_rule`. This is useful, for example if you block on the dmetaphone of a column but match on the original column. Examples: >>> # Default behaviour >>> br_training = \"l.first_name = r.first_name and l.dob = r.dob\" >>> linker.estimate_parameters_using_expectation_maximisation(br_training) >>> # Specify which comparisons to deactivate >>> br_training = \"l.dmeta_first_name = r.dmeta_first_name\" >>> settings_obj = linker._settings_obj >>> comp = settings_obj._get_comparison_by_output_column_name(\"first_name\") >>> dmeta_level = comp._get_comparison_level_by_comparison_vector_value(1) >>> linker.estimate_parameters_using_expectation_maximisation( >>> br_training, >>> comparisons_to_deactivate=[\"first_name\"], >>> comparison_levels_to_reverse_blocking_rule=[dmeta_level], >>> ) Args: blocking_rule (str): The blocking rule used to generate pairwise record comparisons. comparisons_to_deactivate (list, optional): By default, splink will analyse the blocking rule provided and estimate the m parameters for all comaprisons except those included in the blocking rule. If comparisons_to_deactivate are provided, spink will instead estimate m parameters for all comparison except those specified in the comparisons_to_deactivate list. This list can either contain the output_column_name of the Comparison as a string, or Comparison objects. Defaults to None. comparison_levels_to_reverse_blocking_rule (list, optional): By default, splink will analyse the blocking rule provided and adjust the global probability two random records match to account for the matches specified in the blocking rule. If provided, this argument will overrule this default behaviour. The user must provide a list of ComparisonLevel objects. Defaults to None. fix_probability_two_random_records_match (bool, optional): If True, do not update the probability two random records match after each iteration. Defaults to False. fix_m_probabilities (bool, optional): If True, do not update the m probabilities after each iteration. Defaults to False. fix_u_probabilities (bool, optional): If True, do not update the u probabilities after each iteration. Defaults to True. Examples: >>> blocking_rule = \"l.first_name = r.first_name and l.dob = r.dob\" >>> linker.estimate_parameters_using_expectation_maximisation(blocking_rule) Returns: EMTrainingSession: An object containing information about the training session such as how parameters changed during the iteration history \"\"\" self . _initialise_df_concat_with_tf ( materialise = True ) if comparisons_to_deactivate : # If user provided a string, convert to Comparison object comparisons_to_deactivate = [ self . _settings_obj . _get_comparison_by_output_column_name ( n ) if isinstance ( n , str ) else n for n in comparisons_to_deactivate ] if comparison_levels_to_reverse_blocking_rule is None : logger . warning ( \" \\n WARNING: \\n \" \"You have provided comparisons_to_deactivate but not \" \"comparison_levels_to_reverse_blocking_rule. \\n \" \"If comparisons_to_deactivate is provided, then \" \"you usually need to provide corresponding \" \"comparison_levels_to_reverse_blocking_rule. \" \"because each comparison to deactivate if effectively treated \" \"as an exact match.\" ) em_training_session = EMTrainingSession ( self , blocking_rule , fix_u_probabilities = fix_u_probabilities , fix_m_probabilities = fix_m_probabilities , fix_probability_two_random_records_match = fix_probability_two_random_records_match , # noqa 501 comparisons_to_deactivate = comparisons_to_deactivate , comparison_levels_to_reverse_blocking_rule = comparison_levels_to_reverse_blocking_rule , # noqa 501 ) em_training_session . _train () self . _populate_m_u_from_trained_values () self . _populate_probability_two_random_records_match_from_trained_values () self . _settings_obj . _columns_without_estimated_parameters_message () return em_training_session def predict ( self , threshold_match_probability : float = None , threshold_match_weight : float = None , ) -> SplinkDataFrame : \"\"\"Create a dataframe of scored pairwise comparisons using the parameters of the linkage model. Uses the blocking rules specified in the `blocking_rules_to_generate_predictions` of the settings dictionary to generate the pairwise comparisons. Args: threshold_match_probability (float, optional): If specified, filter the results to include only pairwise comparisons with a match_probability above this threshold. Defaults to None. threshold_match_weight (float, optional): If specified, filter the results to include only pairwise comparisons with a match_weight above this threshold. Defaults to None. Examples: >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> linker.load_settings_from_json(\"saved_settings.json\") >>> df = linker.predict(threshold_match_probability=0.95) >>> df.as_pandas_dataframe(limit=5) Returns: SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons. This represents a table materialised in the database. Methods on the SplinkDataFrame allow you to access the underlying data. \"\"\" # If the user only calls predict, it runs as a single pipeline with no # materialisation of anything self . _initialise_df_concat_with_tf ( materialise = False ) sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) sql = compute_comparison_vector_values_sql ( self . _settings_obj ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vectors\" ) sqls = predict_from_comparison_vectors_sqls ( self . _settings_obj , threshold_match_probability , threshold_match_weight ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) predictions = self . _execute_sql_pipeline () self . _predict_warning () return predictions def find_matches_to_new_records ( self , records : List [ dict ], blocking_rules = None , match_weight_threshold =- 4 ) -> SplinkDataFrame : \"\"\"Given one or more records, find records in the input dataset(s) which match and return in order of the splink prediction score. This effectively provides a way of searching the input datasets for given record(s) Args: records (List[dict]): Input search record(s). blocking_rules (str, optional): Blocking rules to select which records to find and score. If None, do not use a blocking rule - meaning the input records will be compared to all records provided to the linker when it was instantiated. Defaults to None. match_weight_threshold (int, optional): Return matches with a match weight above this threshold. Defaults to -4. Examples: >>> linker = DuckDBLinker(df) >>> linker.load_settings_from_json(\"saved_settings.json\") >>> # Pre-compute tf tables for any tables with >>> # term frequency adjustments >>> linker.compute_tf_table(\"first_name\") >>> record = {'unique_id': 1, >>> 'first_name': \"John\", >>> 'surname': \"Smith\", >>> 'dob': \"1971-05-24\", >>> 'city': \"London\", >>> 'email': \"john@smith.net\" >>> } >>> df = linker.find_matches_to_new_records([record], blocking_rules=[]) Returns: SplinkDataFrame: The pairwise comparisons. \"\"\" original_blocking_rules = ( self . _settings_obj . _blocking_rules_to_generate_predictions ) original_link_type = self . _settings_obj . _link_type self . _records_to_table ( records , \"__splink__df_new_records\" ) if blocking_rules is not None : self . _settings_obj . _blocking_rules_to_generate_predictions = blocking_rules self . _settings_obj . _link_type = \"link_only_find_matches_to_new_records\" self . _find_new_matches_mode = True sql = _join_tf_to_input_df_sql ( self ) sql = sql . replace ( \"__splink__df_concat\" , \"__splink__df_new_records\" ) self . _enqueue_sql ( sql , \"__splink__df_new_records_with_tf\" ) sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) sql = compute_comparison_vector_values_sql ( self . _settings_obj ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vectors\" ) sqls = predict_from_comparison_vectors_sqls ( self . _settings_obj ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) sql = f \"\"\" select * from __splink__df_predict where match_weight > { match_weight_threshold } \"\"\" self . _enqueue_sql ( sql , \"__splink_find_matches_predictions\" ) predictions = self . _execute_sql_pipeline ( use_cache = False ) self . _settings_obj . _blocking_rules_to_generate_predictions = ( original_blocking_rules ) self . _settings_obj . _link_type = original_link_type self . _find_new_matches_mode = False return predictions def compare_two_records ( self , record_1 : dict , record_2 : dict ): \"\"\"Use the linkage model to compare and score a pairwise record comparison based on the two input records provided Args: record_1 (dict): dictionary representing the first record. Columns names and data types must be the same as the columns in the settings object record_2 (dict): dictionary representing the second record. Columns names and data types must be the same as the columns in the settings object Examples: >>> linker = DuckDBLinker(df) >>> linker.load_settings_from_json(\"saved_settings.json\") >>> linker.compare_two_records(record_left, record_right) Returns: SplinkDataFrame: Pairwise comparison with scored prediction \"\"\" original_blocking_rules = ( self . _settings_obj . _blocking_rules_to_generate_predictions ) original_link_type = self . _settings_obj . _link_type self . _compare_two_records_mode = True self . _settings_obj . _blocking_rules_to_generate_predictions = [] self . _records_to_table ([ record_1 ], \"__splink__compare_two_records_left\" ) self . _records_to_table ([ record_2 ], \"__splink__compare_two_records_right\" ) sql_join_tf = _join_tf_to_input_df_sql ( self ) sql_join_tf = sql_join_tf . replace ( \"__splink__df_concat\" , \"__splink__compare_two_records_left\" ) self . _enqueue_sql ( sql_join_tf , \"__splink__compare_two_records_left_with_tf\" ) sql_join_tf = sql_join_tf . replace ( \"__splink__compare_two_records_left\" , \"__splink__compare_two_records_right\" ) self . _enqueue_sql ( sql_join_tf , \"__splink__compare_two_records_right_with_tf\" ) sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) sql = compute_comparison_vector_values_sql ( self . _settings_obj ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vectors\" ) sqls = predict_from_comparison_vectors_sqls ( self . _settings_obj ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) predictions = self . _execute_sql_pipeline ( use_cache = False ) self . _settings_obj . _blocking_rules_to_generate_predictions = ( original_blocking_rules ) self . _settings_obj . _link_type = original_link_type self . _compare_two_records_mode = False return predictions def _self_link ( self ) -> SplinkDataFrame : \"\"\"Use the linkage model to compare and score all records in our input df with themselves. Returns: SplinkDataFrame: Scored pairwise comparisons of the input records to themselves. \"\"\" original_blocking_rules = ( self . _settings_obj . _blocking_rules_to_generate_predictions ) original_link_type = self . _settings_obj . _link_type # Changes our sql to allow for a self link. # This is used in `_sql_gen_where_condition` in blocking.py # to remove any 'where' clauses when blocking (normally when blocking # we want to *remove* self links!) self . _self_link_mode = True # Block on uid i.e. create pairwise record comparisons where the uid matches uid_cols = self . _settings_obj . _unique_id_input_columns uid_l = _composite_unique_id_from_edges_sql ( uid_cols , None , \"l\" ) uid_r = _composite_unique_id_from_edges_sql ( uid_cols , None , \"r\" ) self . _settings_obj . _blocking_rules_to_generate_predictions = [ f \" { uid_l } = { uid_r } \" ] self . _initialise_df_concat_with_tf () sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) sql = compute_comparison_vector_values_sql ( self . _settings_obj ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vectors\" ) sqls = predict_from_comparison_vectors_sqls ( self . _settings_obj ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) predictions = self . _execute_sql_pipeline ( use_cache = False ) self . _settings_obj . _blocking_rules_to_generate_predictions = ( original_blocking_rules ) self . _settings_obj . _link_type = original_link_type self . _self_link_mode = False return predictions def cluster_pairwise_predictions_at_threshold ( self , df_predict : SplinkDataFrame , threshold_match_probability : float ) -> SplinkDataFrame : \"\"\"Clusters the pairwise match predictions that result from `linker.predict()` into groups of connected record using the connected components graph clustering algorithm Records with an estimated `match_probability` above `threshold_match_probability` are considered to be a match (i.e. they represent the same entity). Args: df_predict (SplinkDataFrame): The results of `linker.predict()` threshold_match_probability (float): Filter the pairwise match predictions to include only pairwise comparisons with a match_probability above this threshold. This dataframe is then fed into the clustering algorithm. Returns: SplinkDataFrame: A SplinkDataFrame containing a list of all IDs, clustered into groups based on the desired match threshold. \"\"\" self . _initialise_df_concat_with_tf ( df_predict ) edges_table = _cc_create_unique_id_cols ( self , df_predict , threshold_match_probability , ) cc = solve_connected_components ( self , edges_table ) return cc def delete_tables_created_by_splink_from_db ( self , retain_term_frequency = True , retain_df_concat_with_tf = True ): tables_remaining = [] current_tables = self . _names_of_tables_created_by_splink for splink_df in current_tables : name = splink_df . templated_name # Only delete tables explicitly marked as having been created by splink if \"__splink__\" not in name : tables_remaining . append ( splink_df ) continue if name == \"__splink__df_concat_with_tf\" : if retain_df_concat_with_tf : tables_remaining . append ( splink_df ) else : self . _delete_table_from_database ( name ) elif name . startswith ( \"__splink__df_tf_\" ): if retain_term_frequency : tables_remaining . append ( splink_df ) else : self . _delete_table_from_database ( name ) else : self . _delete_table_from_database ( name ) self . _names_of_tables_created_by_splink = tables_remaining def profile_columns ( self , column_expressions : Union [ str , List [ str ]], top_n = 10 , bottom_n = 10 ): return profile_columns ( self , column_expressions , top_n = top_n , bottom_n = bottom_n ) def train_m_from_pairwise_labels ( self , table_name ): self . _initialise_df_concat_with_tf ( materialise = True ) estimate_m_from_pairwise_labels ( self , table_name ) def roc_chart_from_labels ( self , labels_tablename , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ): \"\"\"Generate a ROC chart from labelled (ground truth) data. The table of labels should be in the following format, and should be registered with your database: |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score| |----------------|-----------|----------------|-----------|--------------------| |df_1 |1 |df_2 |2 |0.99 | |df_1 |1 |df_2 |3 |0.2 | Note that `source_dataset` and `unique_id` should correspond to the values specified in the settings dict, and the `input_table_aliases` passed to the `linker` object. For `dedupe_only` links, the `source_dataset` columns can be ommitted. Args: labels_tablename (str): Name of table containing labels in the database threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> # DuckDBLinker >>> labels = pd.read_csv(\"my_labels.csv\") >>> linker._con.register(\"labels\", labels) >>> linker.roc_chart_from_labels(\"labels\") >>> >>> # SparkLinker >>> labels = spark.read.csv(\"my_labels.csv\", header=True) >>> labels.createDataFrame(\"labels\") >>> linker.roc_chart_from_labels(\"labels\") Returns: SplinkDataFrame \"\"\" df_truth_space = roc_table ( self , labels_tablename , threshold_actual = threshold_actual , match_weight_round_to_nearest = match_weight_round_to_nearest , ) recs = df_truth_space . as_record_dict () return roc_chart ( recs ) def precision_recall_chart_from_labels ( self , labels_tablename ): \"\"\"Generate a precision-recall chart from labelled (ground truth) data. The table of labels should be in the following format, and should be registered as a table with your database: |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score| |----------------|-----------|----------------|-----------|--------------------| |df_1 |1 |df_2 |2 |0.99 | |df_1 |1 |df_2 |3 |0.2 | Note that `source_dataset` and `unique_id` should correspond to the values specified in the settings dict, and the `input_table_aliases` passed to the `linker` object. For `dedupe_only` links, the `source_dataset` columns can be ommitted. Args: labels_tablename (str): Name of table containing labels in the database threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> # DuckDBLinker >>> labels = pd.read_csv(\"my_labels.csv\") >>> linker._con.register(\"labels\", labels) >>> linker.precision_recall_chart_from_labels(\"labels\") >>> >>> # SparkLinker >>> labels = spark.read.csv(\"my_labels.csv\", header=True) >>> labels.createDataFrame(\"labels\") >>> linker.precision_recall_chart_from_labels(\"labels\") Returns: SplinkDataFrame \"\"\" df_truth_space = roc_table ( self , labels_tablename ) recs = df_truth_space . as_record_dict () return precision_recall_chart ( recs ) def roc_table_from_labels ( self , labels_tablename , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ) -> SplinkDataFrame : \"\"\"Generate truth statistics (false positive etc.) for each threshold value of match_probability, suitable for plotting a ROC chart. The table of labels should be in the following format, and should be registered with your database: |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score| |----------------|-----------|----------------|-----------|--------------------| |df_1 |1 |df_2 |2 |0.99 | |df_1 |1 |df_2 |3 |0.2 | Note that `source_dataset` and `unique_id` should correspond to the values specified in the settings dict, and the `input_table_aliases` passed to the `linker` object. For `dedupe_only` links, the `source_dataset` columns can be ommitted. Args: labels_tablename (str): Name of table containing labels in the database threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> # DuckDBLinker >>> labels = pd.read_csv(\"my_labels.csv\") >>> linker._con.register(\"labels\", labels) >>> linker.roc_table_from_labels(\"labels\") >>> >>> # SparkLinker >>> labels = spark.read.csv(\"my_labels.csv\", header=True) >>> labels.createDataFrame(\"labels\") >>> linker.roc_table_from_labels(\"labels\") Returns: SplinkDataFrame \"\"\" return roc_table ( self , labels_tablename , threshold_actual = threshold_actual , match_weight_round_to_nearest = match_weight_round_to_nearest , ) def match_weight_histogram ( self , df_predict : SplinkDataFrame , target_bins : int = 30 , width = 600 , height = 250 ): \"\"\"Generate a histogram that shows the distribution of match weights in `df_predict` Args: df_predict (SplinkDataFrame): Output of `linker.predict()` target_bins (int, optional): Target number of bins in histogram. Defaults to 30. width (int, optional): Width of output. Defaults to 600. height (int, optional): Height of output chart. Defaults to 250. \"\"\" df = histogram_data ( self , df_predict , target_bins ) recs = df . as_record_dict () return match_weight_histogram ( recs , width = width , height = height ) def waterfall_chart ( self , records : List [ dict ], filter_nulls = True ): \"\"\"Visualise how the final match weight is computed for the provided pairwise record comparisons. Records must be provided as a list of dictionaries. This would usually be obtained from `df.as_record_dict(limit=n)` where `df` is a SplinkDataFrame. Examples: >>> df = linker.predict(threshold_match_weight=2) >>> records = df.as_record_dict(limit=10) >>> linker.waterfall_chart(records) Args: records (List[dict]): Usually be obtained from `df.as_record_dict(limit=n)` where `df` is a SplinkDataFrame. filter_nulls (bool, optional): Whether the visualiation shows null comparisons, which have no effect on final match weight. Defaults to True. \"\"\" self . _raise_error_if_necessary_waterfall_columns_not_computed () return waterfall_chart ( records , self . _settings_obj , filter_nulls ) def unlinkables_chart ( self , x_col = \"match_weight\" , source_dataset = None , as_dict = False , ): \"\"\"Generate an interactive chart displaying the proportion of records that are \"unlinkable\" for a given splink score threshold and model parameters. Unlinkable records are those that, even when compared with themselves, do not contain enough information to confirm a match. Args: x_col (str, optional): Column to use for the x-axis. Defaults to \"match_weight\". source_dataset (str, optional): Name of the source dataset to use for the title of the output chart. as_dict (bool, optional): If True, return a dict version of the chart. Examples: >>> # For the simplest code pipeline, load a pre-trained model >>> # and run this against the test data. >>> df = pd.read_csv(\"./tests/datasets/fake_1000_from_splink_demos.csv\") >>> linker = DuckDBLinker(df) >>> linker.load_settings_from_json(\"saved_settings.json\") >>> linker.unlinkables_chart() >>> >>> # For more complex code pipelines, you can run an entire pipeline >>> # that estimates your m and u values, before `unlinkables_chart(). \"\"\" # Link our initial df on itself and calculate the % of unlinkable entries records = unlinkables_data ( self , x_col ) return unlinkables_chart ( records , x_col , source_dataset ) def comparison_viewer_dashboard ( self , df_predict : SplinkDataFrame , out_path : str , overwrite = False , num_example_rows = 2 , ): \"\"\"Generate an interactive html visualization of the linker's predictions and save to `out_path`. For more information see [this video](https://www.youtube.com/watch?v=DNvCMqjipis) Args: df_predict (SplinkDataFrame): The outputs of `linker.predict()` out_path (str): The path (including filename) to save the html file to. overwrite (bool, optional): Overwrite the html file if it already exists? Defaults to False. num_example_rows (int, optional): Number of example rows per comparison vector. Defaults to 2. Examples: >>> df_predictions = linker.predict() >>> linker.comparison_viewer_dashboard(df_predictions, \"scv.html\", True, 2) >>> >>> # Optionally, in Jupyter, you can display the results inline >>> # Otherwise you can just load the html file in your browser >>> from IPython.display import IFrame >>> IFrame(src=\"./scv.html\", width=\"100%\", height=1200) \"\"\" self . _raise_error_if_necessary_waterfall_columns_not_computed () sql = comparison_vector_distribution_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vector_distribution\" ) sqls = comparison_viewer_table_sqls ( self , num_example_rows ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) df = self . _execute_sql_pipeline ([ df_predict ]) render_splink_comparison_viewer_html ( df . as_record_dict (), self . _settings_obj . _as_completed_dict (), out_path , overwrite , ) def parameter_estimate_comparisons_chart ( self , include_m = True , include_u = True ): \"\"\"Show a chart that shows how parameter estimates have differed across the different estimation methods you have used. For example, if you have run two EM estimation sessions, blocking on different variables, and both result in parameter estimates for first_name, this chart will enable easy comparison of the different estimates Args: include_m (bool, optional): Show different estimates of m values. Defaults to True. include_u (bool, optional): Show different estimates of u values. Defaults to True. \"\"\" records = self . _settings_obj . _parameter_estimates_as_records to_retain = [] if include_m : to_retain . append ( \"m\" ) if include_u : to_retain . append ( \"u\" ) records = [ r for r in records if r [ \"m_or_u\" ] in to_retain ] return parameter_estimate_comparisons ( records ) def missingness_chart ( self , input_dataset : str = None ): \"\"\"Generate a summary chart of the missingness (prevalence of nulls) of columns in the input datasets. By default, missingness is assessed across all input datasets Args: input_dataset (str, optional): Name of one of the input tables in the database. If provided, missingness will be computed for this table alone. Defaults to None. Examples: >>> linker.missingness_chart() \"\"\" records = missingness_data ( self , input_dataset ) return missingness_chart ( records , input_dataset ) def compute_number_of_comparisons_generated_by_blocking_rule ( self , blocking_rule : str , link_type : str = None , unique_id_column_name : str = None , ) -> int : \"\"\"Compute the number of pairwise record comparisons that would be generated by a blocking rule Args: blocking_rule (str): The blocking rule to analyse link_type (str, optional): The link type. This is needed only if the linker has not yet been provided with a settings dictionary. Defaults to None. unique_id_column_name (str, optional): This is needed only if the linker has not yet been provided with a settings dictionary. Defaults to None. Examples: >>> br = \"l.first_name = r.first_name\" >>> linker.compute_number_of_comparisons_generated_by_blocking_rule(br) 19387 >>> br = \"l.name = r.name and substr(l.dob,1,4) = substr(r.dob,1,4)\" >>> linker.compute_number_of_comparisons_generated_by_blocking_rule(br) 394 Returns: int: The number of comparisons generated by the blocking rule \"\"\" sql = vertically_concatenate_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_concat\" ) sql = number_of_comparisons_generated_by_blocking_rule_sql ( self , blocking_rule , link_type , unique_id_column_name ) self . _enqueue_sql ( sql , \"__splink__analyse_blocking_rule\" ) res = self . _execute_sql_pipeline () . as_record_dict ()[ 0 ] return res [ \"count_of_pairwise_comparisons_generated\" ] def match_weights_chart ( self ): \"\"\"Display a chart of the (partial) match weights of the linkage model Examples: >>> linker.match_weights_chart() >>> >>> # To view offline (if you don't have an internet connection): >>> >>> from splink.charts import save_offline_chart >>> c = linker.match_weights_chart() >>> save_offline_chart(c.spec, \"test_chart.html\") >>> >>> # View resultant html file in Jupyter (or just load it in your browser) >>> from IPython.display import IFrame >>> IFrame(src=\"./test_chart.html\", width=1000, height=500) \"\"\" return self . _settings_obj . match_weights_chart () def m_u_parameters_chart ( self ): \"\"\"Display a chart of the m and u parameters of the linkage model Examples: >>> linker.m_u_parameters_chart() >>> >>> # To view offline (if you don't have an internet connection): >>> >>> from splink.charts import save_offline_chart >>> c = linker.match_weights_chart() >>> save_offline_chart(c.spec, \"test_chart.html\") >>> >>> # View resultant html file in Jupyter (or just load it in your browser) >>> from IPython.display import IFrame >>> IFrame(src=\"./test_chart.html\", width=1000, height=500) \"\"\" return self . _settings_obj . m_u_parameters_chart () def cluster_studio_dashboard ( self , df_predict : SplinkDataFrame , df_clustered : SplinkDataFrame , out_path : str , sampling_method = \"random\" , sample_size : int = 10 , cluster_ids : list = None , cluster_names : list = None , overwrite : bool = False , ): \"\"\"Generate an interactive html visualization of the predicted cluster and save to `out_path`. Args: df_predict (SplinkDataFrame): The outputs of `linker.predict()` df_clustered (SplinkDataFrame): The outputs of `linker.cluster_pairwise_predictions_at_threshold()` out_path (str): The path (including filename) to save the html file to. sampling_method (str, optional): `random` or `by_cluster_size`. Defaults to `random`. sample_size (int, optional): Number of clusters to show in the dahboard. Defaults to 10. cluster_ids (list): The IDs of the clusters that will be displayed in the dashboard. If provided, ignore the `sampling_method` and `sample_size` arguments. Defaults to None. overwrite (bool, optional): Overwrite the html file if it already exists? Defaults to False. cluster_names (list, optional): If provided, the dashboard will display these names in the selection box. Ony works in conjunction with `cluster_ids`. Defaults to None. Examples: >>> df_p = linker.predict() >>> df_c = linker.cluster_pairwise_predictions_at_threshold(df_p, 0.5) >>> linker.cluster_studio_dashboard( >>> df_p, df_c, [0, 4, 7], \"cluster_studio.html\" >>> ) >>> >>> # Optionally, in Jupyter, you can display the results inline >>> # Otherwise you can just load the html file in your browser >>> from IPython.display import IFrame >>> IFrame(src=\"./cluster_studio.html\", width=\"100%\", height=1200) \"\"\" self . _raise_error_if_necessary_waterfall_columns_not_computed () return render_splink_cluster_studio_html ( self , df_predict , df_clustered , out_path , sampling_method = sampling_method , sample_size = sample_size , cluster_ids = cluster_ids , overwrite = overwrite , cluster_names = cluster_names , ) def save_settings_to_json ( self , out_path : str , overwrite = False ) -> dict : \"\"\"Save the configuration and parameters the linkage model to a json file. Returns the model as a Python dictionary. If an out_path is specified, also saves the settings to a file Args: out_path (str): File path for json file overwrite (bool, optional): Overwrite if already exists? Defaults to False. \"\"\" model_dict = self . _settings_obj . as_dict () if out_path : if os . path . isfile ( out_path ) and not overwrite : raise ValueError ( f \"The path { out_path } already exists. Please provide a different \" \"path or set overwrite=True\" ) with open ( out_path , \"w\" ) as f : json . dump ( model_dict , f , indent = 4 ) def load_settings_from_json ( self , in_path : str ): \"\"\" Load settings from a file. Args: in_path (str): Path to settings json file \"\"\" with open ( in_path , \"r\" ) as f : model_dict = json . load ( f ) self . initialise_settings ( model_dict ) cluster_pairwise_predictions_at_threshold ( df_predict , threshold_match_probability ) \u00b6 Clusters the pairwise match predictions that result from linker.predict() into groups of connected record using the connected components graph clustering algorithm Records with an estimated match_probability above threshold_match_probability are considered to be a match (i.e. they represent the same entity). Parameters: Name Type Description Default df_predict SplinkDataFrame The results of linker.predict() required threshold_match_probability float Filter the pairwise match predictions to include only pairwise comparisons with a match_probability above this threshold. This dataframe is then fed into the clustering algorithm. required Returns: Name Type Description SplinkDataFrame SplinkDataFrame A SplinkDataFrame containing a list of all IDs, clustered into groups based on the desired match threshold. Source code in splink/linker.py 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 def cluster_pairwise_predictions_at_threshold ( self , df_predict : SplinkDataFrame , threshold_match_probability : float ) -> SplinkDataFrame : \"\"\"Clusters the pairwise match predictions that result from `linker.predict()` into groups of connected record using the connected components graph clustering algorithm Records with an estimated `match_probability` above `threshold_match_probability` are considered to be a match (i.e. they represent the same entity). Args: df_predict (SplinkDataFrame): The results of `linker.predict()` threshold_match_probability (float): Filter the pairwise match predictions to include only pairwise comparisons with a match_probability above this threshold. This dataframe is then fed into the clustering algorithm. Returns: SplinkDataFrame: A SplinkDataFrame containing a list of all IDs, clustered into groups based on the desired match threshold. \"\"\" self . _initialise_df_concat_with_tf ( df_predict ) edges_table = _cc_create_unique_id_cols ( self , df_predict , threshold_match_probability , ) cc = solve_connected_components ( self , edges_table ) return cc cluster_studio_dashboard ( df_predict , df_clustered , out_path , sampling_method = 'random' , sample_size = 10 , cluster_ids = None , cluster_names = None , overwrite = False ) \u00b6 Generate an interactive html visualization of the predicted cluster and save to out_path . Parameters: Name Type Description Default df_predict SplinkDataFrame The outputs of linker.predict() required df_clustered SplinkDataFrame The outputs of linker.cluster_pairwise_predictions_at_threshold() required out_path str The path (including filename) to save the html file to. required sampling_method str random or by_cluster_size . Defaults to random . 'random' sample_size int Number of clusters to show in the dahboard. Defaults to 10. 10 cluster_ids list The IDs of the clusters that will be displayed in the dashboard. If provided, ignore the sampling_method and sample_size arguments. Defaults to None. None overwrite bool Overwrite the html file if it already exists? Defaults to False. False cluster_names list If provided, the dashboard will display these names in the selection box. Ony works in conjunction with cluster_ids . Defaults to None. None Examples: >>> df_p = linker . predict () >>> df_c = linker . cluster_pairwise_predictions_at_threshold ( df_p , 0.5 ) >>> linker . cluster_studio_dashboard ( >>> df_p , df_c , [ 0 , 4 , 7 ], \"cluster_studio.html\" >>> ) >>> >>> # Optionally, in Jupyter, you can display the results inline >>> # Otherwise you can just load the html file in your browser >>> from IPython.display import IFrame >>> IFrame ( src = \"./cluster_studio.html\" , width = \"100%\" , height = 1200 ) Source code in splink/linker.py 1640 1641 1642 1643 1644 1645 1646 1647 1648 1649 1650 1651 1652 1653 1654 1655 1656 1657 1658 1659 1660 1661 1662 1663 1664 1665 1666 1667 1668 1669 1670 1671 1672 1673 1674 1675 1676 1677 1678 1679 1680 1681 1682 1683 1684 1685 1686 1687 1688 1689 1690 1691 1692 1693 1694 1695 1696 1697 def cluster_studio_dashboard ( self , df_predict : SplinkDataFrame , df_clustered : SplinkDataFrame , out_path : str , sampling_method = \"random\" , sample_size : int = 10 , cluster_ids : list = None , cluster_names : list = None , overwrite : bool = False , ): \"\"\"Generate an interactive html visualization of the predicted cluster and save to `out_path`. Args: df_predict (SplinkDataFrame): The outputs of `linker.predict()` df_clustered (SplinkDataFrame): The outputs of `linker.cluster_pairwise_predictions_at_threshold()` out_path (str): The path (including filename) to save the html file to. sampling_method (str, optional): `random` or `by_cluster_size`. Defaults to `random`. sample_size (int, optional): Number of clusters to show in the dahboard. Defaults to 10. cluster_ids (list): The IDs of the clusters that will be displayed in the dashboard. If provided, ignore the `sampling_method` and `sample_size` arguments. Defaults to None. overwrite (bool, optional): Overwrite the html file if it already exists? Defaults to False. cluster_names (list, optional): If provided, the dashboard will display these names in the selection box. Ony works in conjunction with `cluster_ids`. Defaults to None. Examples: >>> df_p = linker.predict() >>> df_c = linker.cluster_pairwise_predictions_at_threshold(df_p, 0.5) >>> linker.cluster_studio_dashboard( >>> df_p, df_c, [0, 4, 7], \"cluster_studio.html\" >>> ) >>> >>> # Optionally, in Jupyter, you can display the results inline >>> # Otherwise you can just load the html file in your browser >>> from IPython.display import IFrame >>> IFrame(src=\"./cluster_studio.html\", width=\"100%\", height=1200) \"\"\" self . _raise_error_if_necessary_waterfall_columns_not_computed () return render_splink_cluster_studio_html ( self , df_predict , df_clustered , out_path , sampling_method = sampling_method , sample_size = sample_size , cluster_ids = cluster_ids , overwrite = overwrite , cluster_names = cluster_names , ) compare_two_records ( record_1 , record_2 ) \u00b6 Use the linkage model to compare and score a pairwise record comparison based on the two input records provided Parameters: Name Type Description Default record_1 dict dictionary representing the first record. Columns names and data types must be the same as the columns in the settings object required record_2 dict dictionary representing the second record. Columns names and data types must be the same as the columns in the settings object required Examples: >>> linker = DuckDBLinker ( df ) >>> linker . load_settings_from_json ( \"saved_settings.json\" ) >>> linker . compare_two_records ( record_left , record_right ) Returns: Name Type Description SplinkDataFrame Pairwise comparison with scored prediction Source code in splink/linker.py 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 def compare_two_records ( self , record_1 : dict , record_2 : dict ): \"\"\"Use the linkage model to compare and score a pairwise record comparison based on the two input records provided Args: record_1 (dict): dictionary representing the first record. Columns names and data types must be the same as the columns in the settings object record_2 (dict): dictionary representing the second record. Columns names and data types must be the same as the columns in the settings object Examples: >>> linker = DuckDBLinker(df) >>> linker.load_settings_from_json(\"saved_settings.json\") >>> linker.compare_two_records(record_left, record_right) Returns: SplinkDataFrame: Pairwise comparison with scored prediction \"\"\" original_blocking_rules = ( self . _settings_obj . _blocking_rules_to_generate_predictions ) original_link_type = self . _settings_obj . _link_type self . _compare_two_records_mode = True self . _settings_obj . _blocking_rules_to_generate_predictions = [] self . _records_to_table ([ record_1 ], \"__splink__compare_two_records_left\" ) self . _records_to_table ([ record_2 ], \"__splink__compare_two_records_right\" ) sql_join_tf = _join_tf_to_input_df_sql ( self ) sql_join_tf = sql_join_tf . replace ( \"__splink__df_concat\" , \"__splink__compare_two_records_left\" ) self . _enqueue_sql ( sql_join_tf , \"__splink__compare_two_records_left_with_tf\" ) sql_join_tf = sql_join_tf . replace ( \"__splink__compare_two_records_left\" , \"__splink__compare_two_records_right\" ) self . _enqueue_sql ( sql_join_tf , \"__splink__compare_two_records_right_with_tf\" ) sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) sql = compute_comparison_vector_values_sql ( self . _settings_obj ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vectors\" ) sqls = predict_from_comparison_vectors_sqls ( self . _settings_obj ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) predictions = self . _execute_sql_pipeline ( use_cache = False ) self . _settings_obj . _blocking_rules_to_generate_predictions = ( original_blocking_rules ) self . _settings_obj . _link_type = original_link_type self . _compare_two_records_mode = False return predictions comparison_viewer_dashboard ( df_predict , out_path , overwrite = False , num_example_rows = 2 ) \u00b6 Generate an interactive html visualization of the linker's predictions and save to out_path . For more information see this video Parameters: Name Type Description Default df_predict SplinkDataFrame The outputs of linker.predict() required out_path str The path (including filename) to save the html file to. required overwrite bool Overwrite the html file if it already exists? Defaults to False. False num_example_rows int Number of example rows per comparison vector. Defaults to 2. 2 Examples: >>> df_predictions = linker . predict () >>> linker . comparison_viewer_dashboard ( df_predictions , \"scv.html\" , True , 2 ) >>> >>> # Optionally, in Jupyter, you can display the results inline >>> # Otherwise you can just load the html file in your browser >>> from IPython.display import IFrame >>> IFrame ( src = \"./scv.html\" , width = \"100%\" , height = 1200 ) Source code in splink/linker.py 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 def comparison_viewer_dashboard ( self , df_predict : SplinkDataFrame , out_path : str , overwrite = False , num_example_rows = 2 , ): \"\"\"Generate an interactive html visualization of the linker's predictions and save to `out_path`. For more information see [this video](https://www.youtube.com/watch?v=DNvCMqjipis) Args: df_predict (SplinkDataFrame): The outputs of `linker.predict()` out_path (str): The path (including filename) to save the html file to. overwrite (bool, optional): Overwrite the html file if it already exists? Defaults to False. num_example_rows (int, optional): Number of example rows per comparison vector. Defaults to 2. Examples: >>> df_predictions = linker.predict() >>> linker.comparison_viewer_dashboard(df_predictions, \"scv.html\", True, 2) >>> >>> # Optionally, in Jupyter, you can display the results inline >>> # Otherwise you can just load the html file in your browser >>> from IPython.display import IFrame >>> IFrame(src=\"./scv.html\", width=\"100%\", height=1200) \"\"\" self . _raise_error_if_necessary_waterfall_columns_not_computed () sql = comparison_vector_distribution_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vector_distribution\" ) sqls = comparison_viewer_table_sqls ( self , num_example_rows ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) df = self . _execute_sql_pipeline ([ df_predict ]) render_splink_comparison_viewer_html ( df . as_record_dict (), self . _settings_obj . _as_completed_dict (), out_path , overwrite , ) compute_number_of_comparisons_generated_by_blocking_rule ( blocking_rule , link_type = None , unique_id_column_name = None ) \u00b6 Compute the number of pairwise record comparisons that would be generated by a blocking rule Parameters: Name Type Description Default blocking_rule str The blocking rule to analyse required link_type str The link type. This is needed only if the linker has not yet been provided with a settings dictionary. Defaults to None. None unique_id_column_name str This is needed only if the linker has not yet been provided with a settings dictionary. Defaults to None. None Examples: >>> br = \"l.first_name = r.first_name\" >>> linker . compute_number_of_comparisons_generated_by_blocking_rule ( br ) 19387 >>> br = \"l.name = r.name and substr(l.dob,1,4) = substr(r.dob,1,4)\" >>> linker . compute_number_of_comparisons_generated_by_blocking_rule ( br ) 394 Returns: Name Type Description int int The number of comparisons generated by the blocking rule Source code in splink/linker.py 1563 1564 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 1601 def compute_number_of_comparisons_generated_by_blocking_rule ( self , blocking_rule : str , link_type : str = None , unique_id_column_name : str = None , ) -> int : \"\"\"Compute the number of pairwise record comparisons that would be generated by a blocking rule Args: blocking_rule (str): The blocking rule to analyse link_type (str, optional): The link type. This is needed only if the linker has not yet been provided with a settings dictionary. Defaults to None. unique_id_column_name (str, optional): This is needed only if the linker has not yet been provided with a settings dictionary. Defaults to None. Examples: >>> br = \"l.first_name = r.first_name\" >>> linker.compute_number_of_comparisons_generated_by_blocking_rule(br) 19387 >>> br = \"l.name = r.name and substr(l.dob,1,4) = substr(r.dob,1,4)\" >>> linker.compute_number_of_comparisons_generated_by_blocking_rule(br) 394 Returns: int: The number of comparisons generated by the blocking rule \"\"\" sql = vertically_concatenate_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_concat\" ) sql = number_of_comparisons_generated_by_blocking_rule_sql ( self , blocking_rule , link_type , unique_id_column_name ) self . _enqueue_sql ( sql , \"__splink__analyse_blocking_rule\" ) res = self . _execute_sql_pipeline () . as_record_dict ()[ 0 ] return res [ \"count_of_pairwise_comparisons_generated\" ] compute_tf_table ( column_name ) \u00b6 Compute a term frequency table for a given column and persist to the database This method is useful if you want to pre-compute term frequency tables e.g. so that real time linkage executes faster, or so that you can estimate various models without having to recompute term frequency tables each time Examples: >>> # Example 1: Real time linkage >>> linker = DuckDBLinker ( df , connection = \":memory:\" ) >>> linker . load_settings_from_json ( \"saved_settings.json\" ) >>> linker . compute_tf_table ( \"surname\" ) >>> linker . compare_two_records ( record_left , record_right ) >>> # Example 2: Pre-computed term frequency tables in Spark >>> linker = SparkLinker ( df ) >>> df_first_name_tf = linker . compute_tf_table ( \"first_name\" ) >>> df_first_name_tf . write . parquet ( \"folder/first_name_tf\" ) >>> >>> # On subsequent data linking job, read this table rather than recompute >>> df_first_name_tf = spark . read . parquet ( \"folder/first_name_tf\" ) >>> df_first_name_tf . createOrReplaceTempView ( \"__splink__df_tf_first_name\" ) Parameters: Name Type Description Default column_name str The column name in the input table required Returns: Name Type Description SplinkDataFrame SplinkDataFrame The resultant table as a splink data frame Source code in splink/linker.py 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 def compute_tf_table ( self , column_name : str ) -> SplinkDataFrame : \"\"\"Compute a term frequency table for a given column and persist to the database This method is useful if you want to pre-compute term frequency tables e.g. so that real time linkage executes faster, or so that you can estimate various models without having to recompute term frequency tables each time Examples: >>> # Example 1: Real time linkage >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> linker.load_settings_from_json(\"saved_settings.json\") >>> linker.compute_tf_table(\"surname\") >>> linker.compare_two_records(record_left, record_right) >>> # Example 2: Pre-computed term frequency tables in Spark >>> linker = SparkLinker(df) >>> df_first_name_tf = linker.compute_tf_table(\"first_name\") >>> df_first_name_tf.write.parquet(\"folder/first_name_tf\") >>> >>> # On subsequent data linking job, read this table rather than recompute >>> df_first_name_tf = spark.read.parquet(\"folder/first_name_tf\") >>> df_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\") Args: column_name (str): The column name in the input table Returns: SplinkDataFrame: The resultant table as a splink data frame \"\"\" sql = vertically_concatenate_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_concat\" ) input_col = InputColumn ( column_name , tf_adjustments = True ) sql = term_frequencies_for_single_column_sql ( input_col ) self . _enqueue_sql ( sql , colname_to_tf_tablename ( input_col )) return self . _execute_sql_pipeline ( materialise_as_hash = False ) delete_tables_created_by_splink_from_db ( retain_term_frequency = True , retain_df_concat_with_tf = True ) \u00b6 Source code in splink/linker.py 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 def delete_tables_created_by_splink_from_db ( self , retain_term_frequency = True , retain_df_concat_with_tf = True ): tables_remaining = [] current_tables = self . _names_of_tables_created_by_splink for splink_df in current_tables : name = splink_df . templated_name # Only delete tables explicitly marked as having been created by splink if \"__splink__\" not in name : tables_remaining . append ( splink_df ) continue if name == \"__splink__df_concat_with_tf\" : if retain_df_concat_with_tf : tables_remaining . append ( splink_df ) else : self . _delete_table_from_database ( name ) elif name . startswith ( \"__splink__df_tf_\" ): if retain_term_frequency : tables_remaining . append ( splink_df ) else : self . _delete_table_from_database ( name ) else : self . _delete_table_from_database ( name ) self . _names_of_tables_created_by_splink = tables_remaining deterministic_link () \u00b6 Uses the blocking rules specified by blocking_rules_to_generate_predictions in the settings dictionary to generate pairwise record comparisons. This should be a list of blocking rules which are strict enough to generate only true links. Deterministic linkage, however, is likely to result in missed links (false negatives). Examples: >>> linker = DuckDBLinker ( df , connection = \":memory:\" ) >>> >>> settings = { >>> \"link_type\" : \"dedupe_only\" , >>> \"blocking_rules_to_generate_predictions\" : [ >>> \"l.first_name = r.first_name\" , >>> \"l.surname = r.surname\" , >>> ], >>> \"comparisons\" : [] >>> } >>> >>> from splink.duckdb.duckdb_linker import DuckDBLinker >>> >>> linker = DuckDBLinker ( df , settings , connection = \":memory:\" ) >>> df = linker . deterministic_link () Returns: Name Type Description SplinkDataFrame SplinkDataFrame A SplinkDataFrame of the pairwise comparisons. This represents a table materialised in the database. Methods on the SplinkDataFrame allow you to access the underlying data. Source code in splink/linker.py 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 def deterministic_link ( self ) -> SplinkDataFrame : \"\"\"Uses the blocking rules specified by `blocking_rules_to_generate_predictions` in the settings dictionary to generate pairwise record comparisons. This should be a list of blocking rules which are strict enough to generate only true links. Deterministic linkage, however, is likely to result in missed links (false negatives). Examples: >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> >>> settings = { >>> \"link_type\": \"dedupe_only\", >>> \"blocking_rules_to_generate_predictions\": [ >>> \"l.first_name = r.first_name\", >>> \"l.surname = r.surname\", >>> ], >>> \"comparisons\": [] >>> } >>> >>> from splink.duckdb.duckdb_linker import DuckDBLinker >>> >>> linker = DuckDBLinker(df, settings, connection=\":memory:\") >>> df = linker.deterministic_link() Returns: SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons. This represents a table materialised in the database. Methods on the SplinkDataFrame allow you to access the underlying data. \"\"\" self . _initialise_df_concat_with_tf () sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) return self . _execute_sql_pipeline () estimate_m_from_label_column ( label_colname ) \u00b6 Estimate the m parameters of the linkage model from a label (ground truth) column in the input dataframe(s). The m parameters represent the proportion of record comparisons that fall into each comparison level amongst truly matching records. The ground truth column is used to generate pairwise record comparisons which are then assumed to be matches. For example, if the entity being matched is persons, and your input dataset(s) contain social security number, this could be used to estimate the m values for the model. Note that this column does not need to be fully populated. A common case is where a unique identifier such as social security number is only partially populated. Parameters: Name Type Description Default label_colname str The name of the column containing the ground truth label in the input data. required Examples: >>> linker . estimate_m_from_label_column ( \"social_security_number\" ) Returns: Type Description Updates the estimated m parameters within the linker object and returns nothing. Source code in splink/linker.py 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 def estimate_m_from_label_column ( self , label_colname : str ): \"\"\"Estimate the m parameters of the linkage model from a label (ground truth) column in the input dataframe(s). The m parameters represent the proportion of record comparisons that fall into each comparison level amongst truly matching records. The ground truth column is used to generate pairwise record comparisons which are then assumed to be matches. For example, if the entity being matched is persons, and your input dataset(s) contain social security number, this could be used to estimate the m values for the model. Note that this column does not need to be fully populated. A common case is where a unique identifier such as social security number is only partially populated. Args: label_colname (str): The name of the column containing the ground truth label in the input data. Examples: >>> linker.estimate_m_from_label_column(\"social_security_number\") Returns: Updates the estimated m parameters within the linker object and returns nothing. \"\"\" self . _initialise_df_concat_with_tf ( materialise = True ) estimate_m_values_from_label_column ( self , self . _input_tables_dict , label_colname ) self . _populate_m_u_from_trained_values () self . _settings_obj . _columns_without_estimated_parameters_message () estimate_parameters_using_expectation_maximisation ( blocking_rule , comparisons_to_deactivate = None , comparison_levels_to_reverse_blocking_rule = None , fix_probability_two_random_records_match = False , fix_m_probabilities = False , fix_u_probabilities = True ) \u00b6 Estimate the parameters of the linkage model using expectation maximisation. By default, the m probabilities are estimated, but not the u probabilities, because good estiamtes for the u probabilities can be obtained from linker.estimate_u_using_random_sampling() . You can change this by setting fix_u_probabilities to False. The blocking rule provided is used to generate pairwise record comparisons. Usually, this should be a blocking rule that results in a dataframe where matches are between about 1% and 99% of the comparisons. By default, m parameters are estimated for all comparisons except those which are included in the blocking rule. For example, if the blocking rule is l.first_name = r.first_name , then parameter esimates will be made for all comparison except those which use first_name in their sql_condition By default, the probability two random records match is estimated for the blocked data, and then the m and u parameters for the columns specified in the blocking rules are used to estiamte the global probability two random records match. To control which comparisons should have their parameter estimated, and the process of 'reversing out' the global probability two random records match, the user may specify comparisons_to_deactivate and comparison_levels_to_reverse_blocking_rule . This is useful, for example if you block on the dmetaphone of a column but match on the original column. Examples: >>> # Default behaviour >>> br_training = \"l.first_name = r.first_name and l.dob = r.dob\" >>> linker . estimate_parameters_using_expectation_maximisation ( br_training ) >>> # Specify which comparisons to deactivate >>> br_training = \"l.dmeta_first_name = r.dmeta_first_name\" >>> settings_obj = linker . _settings_obj >>> comp = settings_obj . _get_comparison_by_output_column_name ( \"first_name\" ) >>> dmeta_level = comp . _get_comparison_level_by_comparison_vector_value ( 1 ) >>> linker . estimate_parameters_using_expectation_maximisation ( >>> br_training , >>> comparisons_to_deactivate = [ \"first_name\" ], >>> comparison_levels_to_reverse_blocking_rule = [ dmeta_level ], >>> ) Parameters: Name Type Description Default blocking_rule str The blocking rule used to generate pairwise record comparisons. required comparisons_to_deactivate list By default, splink will analyse the blocking rule provided and estimate the m parameters for all comaprisons except those included in the blocking rule. If comparisons_to_deactivate are provided, spink will instead estimate m parameters for all comparison except those specified in the comparisons_to_deactivate list. This list can either contain the output_column_name of the Comparison as a string, or Comparison objects. Defaults to None. None comparison_levels_to_reverse_blocking_rule list By default, splink will analyse the blocking rule provided and adjust the global probability two random records match to account for the matches specified in the blocking rule. If provided, this argument will overrule this default behaviour. The user must provide a list of ComparisonLevel objects. Defaults to None. None fix_probability_two_random_records_match bool If True, do not update the probability two random records match after each iteration. Defaults to False. False fix_m_probabilities bool If True, do not update the m probabilities after each iteration. Defaults to False. False fix_u_probabilities bool If True, do not update the u probabilities after each iteration. Defaults to True. True Examples: >>> blocking_rule = \"l.first_name = r.first_name and l.dob = r.dob\" >>> linker . estimate_parameters_using_expectation_maximisation ( blocking_rule ) Returns: Name Type Description EMTrainingSession EMTrainingSession An object containing information about the training session such as how parameters changed during the iteration history Source code in splink/linker.py 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 def estimate_parameters_using_expectation_maximisation ( self , blocking_rule : str , comparisons_to_deactivate : List [ Union [ str , Comparison ]] = None , comparison_levels_to_reverse_blocking_rule : List [ ComparisonLevel ] = None , fix_probability_two_random_records_match : bool = False , fix_m_probabilities = False , fix_u_probabilities = True , ) -> EMTrainingSession : \"\"\"Estimate the parameters of the linkage model using expectation maximisation. By default, the m probabilities are estimated, but not the u probabilities, because good estiamtes for the u probabilities can be obtained from `linker.estimate_u_using_random_sampling()`. You can change this by setting `fix_u_probabilities` to False. The blocking rule provided is used to generate pairwise record comparisons. Usually, this should be a blocking rule that results in a dataframe where matches are between about 1% and 99% of the comparisons. By default, m parameters are estimated for all comparisons except those which are included in the blocking rule. For example, if the blocking rule is `l.first_name = r.first_name`, then parameter esimates will be made for all comparison except those which use `first_name` in their sql_condition By default, the probability two random records match is estimated for the blocked data, and then the m and u parameters for the columns specified in the blocking rules are used to estiamte the global probability two random records match. To control which comparisons should have their parameter estimated, and the process of 'reversing out' the global probability two random records match, the user may specify `comparisons_to_deactivate` and `comparison_levels_to_reverse_blocking_rule`. This is useful, for example if you block on the dmetaphone of a column but match on the original column. Examples: >>> # Default behaviour >>> br_training = \"l.first_name = r.first_name and l.dob = r.dob\" >>> linker.estimate_parameters_using_expectation_maximisation(br_training) >>> # Specify which comparisons to deactivate >>> br_training = \"l.dmeta_first_name = r.dmeta_first_name\" >>> settings_obj = linker._settings_obj >>> comp = settings_obj._get_comparison_by_output_column_name(\"first_name\") >>> dmeta_level = comp._get_comparison_level_by_comparison_vector_value(1) >>> linker.estimate_parameters_using_expectation_maximisation( >>> br_training, >>> comparisons_to_deactivate=[\"first_name\"], >>> comparison_levels_to_reverse_blocking_rule=[dmeta_level], >>> ) Args: blocking_rule (str): The blocking rule used to generate pairwise record comparisons. comparisons_to_deactivate (list, optional): By default, splink will analyse the blocking rule provided and estimate the m parameters for all comaprisons except those included in the blocking rule. If comparisons_to_deactivate are provided, spink will instead estimate m parameters for all comparison except those specified in the comparisons_to_deactivate list. This list can either contain the output_column_name of the Comparison as a string, or Comparison objects. Defaults to None. comparison_levels_to_reverse_blocking_rule (list, optional): By default, splink will analyse the blocking rule provided and adjust the global probability two random records match to account for the matches specified in the blocking rule. If provided, this argument will overrule this default behaviour. The user must provide a list of ComparisonLevel objects. Defaults to None. fix_probability_two_random_records_match (bool, optional): If True, do not update the probability two random records match after each iteration. Defaults to False. fix_m_probabilities (bool, optional): If True, do not update the m probabilities after each iteration. Defaults to False. fix_u_probabilities (bool, optional): If True, do not update the u probabilities after each iteration. Defaults to True. Examples: >>> blocking_rule = \"l.first_name = r.first_name and l.dob = r.dob\" >>> linker.estimate_parameters_using_expectation_maximisation(blocking_rule) Returns: EMTrainingSession: An object containing information about the training session such as how parameters changed during the iteration history \"\"\" self . _initialise_df_concat_with_tf ( materialise = True ) if comparisons_to_deactivate : # If user provided a string, convert to Comparison object comparisons_to_deactivate = [ self . _settings_obj . _get_comparison_by_output_column_name ( n ) if isinstance ( n , str ) else n for n in comparisons_to_deactivate ] if comparison_levels_to_reverse_blocking_rule is None : logger . warning ( \" \\n WARNING: \\n \" \"You have provided comparisons_to_deactivate but not \" \"comparison_levels_to_reverse_blocking_rule. \\n \" \"If comparisons_to_deactivate is provided, then \" \"you usually need to provide corresponding \" \"comparison_levels_to_reverse_blocking_rule. \" \"because each comparison to deactivate if effectively treated \" \"as an exact match.\" ) em_training_session = EMTrainingSession ( self , blocking_rule , fix_u_probabilities = fix_u_probabilities , fix_m_probabilities = fix_m_probabilities , fix_probability_two_random_records_match = fix_probability_two_random_records_match , # noqa 501 comparisons_to_deactivate = comparisons_to_deactivate , comparison_levels_to_reverse_blocking_rule = comparison_levels_to_reverse_blocking_rule , # noqa 501 ) em_training_session . _train () self . _populate_m_u_from_trained_values () self . _populate_probability_two_random_records_match_from_trained_values () self . _settings_obj . _columns_without_estimated_parameters_message () return em_training_session estimate_u_using_random_sampling ( target_rows ) \u00b6 Estimate the u parameters of the linkage model using random sampling. The u parameters represent the proportion of record comparisons that fall into each comparison level amongst truly non-matching records. This procedure takes a sample of the data and generates the cartesian product of pairwise record comparisons amongst the sampled records. The validity of the u values rests on the assumption that the resultant pairwise comparisons are non-matches (or at least, they are very unlikely to be matches). For large datasets, this is typically true. Parameters: Name Type Description Default target_rows int The target number of pairwise record comparisons from required Examples: >>> linker . estimate_u_using_random_sampling ( 1e8 ) Returns: Type Description Updates the estimated u parameters within the linker object and returns nothing. Source code in splink/linker.py 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 def estimate_u_using_random_sampling ( self , target_rows : int ): \"\"\"Estimate the u parameters of the linkage model using random sampling. The u parameters represent the proportion of record comparisons that fall into each comparison level amongst truly non-matching records. This procedure takes a sample of the data and generates the cartesian product of pairwise record comparisons amongst the sampled records. The validity of the u values rests on the assumption that the resultant pairwise comparisons are non-matches (or at least, they are very unlikely to be matches). For large datasets, this is typically true. Args: target_rows (int): The target number of pairwise record comparisons from which to derive the u values. Larger will give more accurate estimates but lead to longer runtimes. In our experience at least 1e9 (one billion) gives best results but can take a long time to compute. 1e7 (ten million) is often adequate whilst testing different model specifications, before the final model is estimated. Examples: >>> linker.estimate_u_using_random_sampling(1e8) Returns: Updates the estimated u parameters within the linker object and returns nothing. \"\"\" self . _initialise_df_concat_with_tf ( materialise = True ) estimate_u_values ( self , target_rows ) self . _populate_m_u_from_trained_values () self . _settings_obj . _columns_without_estimated_parameters_message () find_matches_to_new_records ( records , blocking_rules = None , match_weight_threshold =- 4 ) \u00b6 Given one or more records, find records in the input dataset(s) which match and return in order of the splink prediction score. This effectively provides a way of searching the input datasets for given record(s) Parameters: Name Type Description Default records List [ dict ] Input search record(s). required blocking_rules str Blocking rules to select which records to find and score. If None, do not use a blocking rule - meaning the input records will be compared to all records provided to the linker when it was instantiated. Defaults to None. None match_weight_threshold int Return matches with a match weight above this threshold. Defaults to -4. -4 Examples: >>> linker = DuckDBLinker ( df ) >>> linker . load_settings_from_json ( \"saved_settings.json\" ) >>> # Pre-compute tf tables for any tables with >>> # term frequency adjustments >>> linker . compute_tf_table ( \"first_name\" ) >>> record = { 'unique_id' : 1 , >>> 'first_name' : \"John\" , >>> 'surname' : \"Smith\" , >>> 'dob' : \"1971-05-24\" , >>> 'city' : \"London\" , >>> 'email' : \"john@smith.net\" >>> } >>> df = linker . find_matches_to_new_records ([ record ], blocking_rules = []) Returns: Name Type Description SplinkDataFrame SplinkDataFrame The pairwise comparisons. Source code in splink/linker.py 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 def find_matches_to_new_records ( self , records : List [ dict ], blocking_rules = None , match_weight_threshold =- 4 ) -> SplinkDataFrame : \"\"\"Given one or more records, find records in the input dataset(s) which match and return in order of the splink prediction score. This effectively provides a way of searching the input datasets for given record(s) Args: records (List[dict]): Input search record(s). blocking_rules (str, optional): Blocking rules to select which records to find and score. If None, do not use a blocking rule - meaning the input records will be compared to all records provided to the linker when it was instantiated. Defaults to None. match_weight_threshold (int, optional): Return matches with a match weight above this threshold. Defaults to -4. Examples: >>> linker = DuckDBLinker(df) >>> linker.load_settings_from_json(\"saved_settings.json\") >>> # Pre-compute tf tables for any tables with >>> # term frequency adjustments >>> linker.compute_tf_table(\"first_name\") >>> record = {'unique_id': 1, >>> 'first_name': \"John\", >>> 'surname': \"Smith\", >>> 'dob': \"1971-05-24\", >>> 'city': \"London\", >>> 'email': \"john@smith.net\" >>> } >>> df = linker.find_matches_to_new_records([record], blocking_rules=[]) Returns: SplinkDataFrame: The pairwise comparisons. \"\"\" original_blocking_rules = ( self . _settings_obj . _blocking_rules_to_generate_predictions ) original_link_type = self . _settings_obj . _link_type self . _records_to_table ( records , \"__splink__df_new_records\" ) if blocking_rules is not None : self . _settings_obj . _blocking_rules_to_generate_predictions = blocking_rules self . _settings_obj . _link_type = \"link_only_find_matches_to_new_records\" self . _find_new_matches_mode = True sql = _join_tf_to_input_df_sql ( self ) sql = sql . replace ( \"__splink__df_concat\" , \"__splink__df_new_records\" ) self . _enqueue_sql ( sql , \"__splink__df_new_records_with_tf\" ) sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) sql = compute_comparison_vector_values_sql ( self . _settings_obj ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vectors\" ) sqls = predict_from_comparison_vectors_sqls ( self . _settings_obj ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) sql = f \"\"\" select * from __splink__df_predict where match_weight > { match_weight_threshold } \"\"\" self . _enqueue_sql ( sql , \"__splink_find_matches_predictions\" ) predictions = self . _execute_sql_pipeline ( use_cache = False ) self . _settings_obj . _blocking_rules_to_generate_predictions = ( original_blocking_rules ) self . _settings_obj . _link_type = original_link_type self . _find_new_matches_mode = False return predictions initialise_settings ( settings_dict ) \u00b6 Initialise settings for the linker. To be used if settings were not passed to the linker on creation. Examples: >>> linker = DuckDBLinker ( df , connection = \":memory:\" ) >>> linker . profile_columns ( \"first_name\" , \"surname\" ) >>> linker . initialise_settings ( settings_dict ) Parameters: Name Type Description Default settings_dict dict A Splink settings dictionary required Source code in splink/linker.py 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 def initialise_settings ( self , settings_dict : dict ): \"\"\"Initialise settings for the linker. To be used if settings were not passed to the linker on creation. Examples: >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> linker.profile_columns(\"first_name\", \"surname\") >>> linker.initialise_settings(settings_dict) Args: settings_dict (dict): A Splink settings dictionary \"\"\" self . _settings_dict = settings_dict self . _settings_obj_ = Settings ( settings_dict ) self . _validate_input_dfs () m_u_parameters_chart () \u00b6 Display a chart of the m and u parameters of the linkage model Examples: >>> linker . m_u_parameters_chart () >>> >>> # To view offline (if you don't have an internet connection): >>> >>> from splink.charts import save_offline_chart >>> c = linker . match_weights_chart () >>> save_offline_chart ( c . spec , \"test_chart.html\" ) >>> >>> # View resultant html file in Jupyter (or just load it in your browser) >>> from IPython.display import IFrame >>> IFrame ( src = \"./test_chart.html\" , width = 1000 , height = 500 ) Source code in splink/linker.py 1621 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631 1632 1633 1634 1635 1636 1637 1638 def m_u_parameters_chart ( self ): \"\"\"Display a chart of the m and u parameters of the linkage model Examples: >>> linker.m_u_parameters_chart() >>> >>> # To view offline (if you don't have an internet connection): >>> >>> from splink.charts import save_offline_chart >>> c = linker.match_weights_chart() >>> save_offline_chart(c.spec, \"test_chart.html\") >>> >>> # View resultant html file in Jupyter (or just load it in your browser) >>> from IPython.display import IFrame >>> IFrame(src=\"./test_chart.html\", width=1000, height=500) \"\"\" return self . _settings_obj . m_u_parameters_chart () match_weight_histogram ( df_predict , target_bins = 30 , width = 600 , height = 250 ) \u00b6 Generate a histogram that shows the distribution of match weights in df_predict Parameters: Name Type Description Default df_predict SplinkDataFrame Output of linker.predict() required target_bins int Target number of bins in histogram. Defaults to 30. 30 width int Width of output. Defaults to 600. 600 height int Height of output chart. Defaults to 250. 250 Source code in splink/linker.py 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 def match_weight_histogram ( self , df_predict : SplinkDataFrame , target_bins : int = 30 , width = 600 , height = 250 ): \"\"\"Generate a histogram that shows the distribution of match weights in `df_predict` Args: df_predict (SplinkDataFrame): Output of `linker.predict()` target_bins (int, optional): Target number of bins in histogram. Defaults to 30. width (int, optional): Width of output. Defaults to 600. height (int, optional): Height of output chart. Defaults to 250. \"\"\" df = histogram_data ( self , df_predict , target_bins ) recs = df . as_record_dict () return match_weight_histogram ( recs , width = width , height = height ) match_weights_chart () \u00b6 Display a chart of the (partial) match weights of the linkage model Examples: >>> linker . match_weights_chart () >>> >>> # To view offline (if you don't have an internet connection): >>> >>> from splink.charts import save_offline_chart >>> c = linker . match_weights_chart () >>> save_offline_chart ( c . spec , \"test_chart.html\" ) >>> >>> # View resultant html file in Jupyter (or just load it in your browser) >>> from IPython.display import IFrame >>> IFrame ( src = \"./test_chart.html\" , width = 1000 , height = 500 ) Source code in splink/linker.py 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 1617 1618 1619 def match_weights_chart ( self ): \"\"\"Display a chart of the (partial) match weights of the linkage model Examples: >>> linker.match_weights_chart() >>> >>> # To view offline (if you don't have an internet connection): >>> >>> from splink.charts import save_offline_chart >>> c = linker.match_weights_chart() >>> save_offline_chart(c.spec, \"test_chart.html\") >>> >>> # View resultant html file in Jupyter (or just load it in your browser) >>> from IPython.display import IFrame >>> IFrame(src=\"./test_chart.html\", width=1000, height=500) \"\"\" return self . _settings_obj . match_weights_chart () missingness_chart ( input_dataset = None ) \u00b6 Generate a summary chart of the missingness (prevalence of nulls) of columns in the input datasets. By default, missingness is assessed across all input datasets Parameters: Name Type Description Default input_dataset str Name of one of the input tables in the None Examples: >>> linker . missingness_chart () Source code in splink/linker.py 1546 1547 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 def missingness_chart ( self , input_dataset : str = None ): \"\"\"Generate a summary chart of the missingness (prevalence of nulls) of columns in the input datasets. By default, missingness is assessed across all input datasets Args: input_dataset (str, optional): Name of one of the input tables in the database. If provided, missingness will be computed for this table alone. Defaults to None. Examples: >>> linker.missingness_chart() \"\"\" records = missingness_data ( self , input_dataset ) return missingness_chart ( records , input_dataset ) parameter_estimate_comparisons_chart ( include_m = True , include_u = True ) \u00b6 Show a chart that shows how parameter estimates have differed across the different estimation methods you have used. For example, if you have run two EM estimation sessions, blocking on different variables, and both result in parameter estimates for first_name, this chart will enable easy comparison of the different estimates Parameters: Name Type Description Default include_m bool Show different estimates of m values. Defaults to True. True include_u bool Show different estimates of u values. Defaults to True. True Source code in splink/linker.py 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 def parameter_estimate_comparisons_chart ( self , include_m = True , include_u = True ): \"\"\"Show a chart that shows how parameter estimates have differed across the different estimation methods you have used. For example, if you have run two EM estimation sessions, blocking on different variables, and both result in parameter estimates for first_name, this chart will enable easy comparison of the different estimates Args: include_m (bool, optional): Show different estimates of m values. Defaults to True. include_u (bool, optional): Show different estimates of u values. Defaults to True. \"\"\" records = self . _settings_obj . _parameter_estimates_as_records to_retain = [] if include_m : to_retain . append ( \"m\" ) if include_u : to_retain . append ( \"u\" ) records = [ r for r in records if r [ \"m_or_u\" ] in to_retain ] return parameter_estimate_comparisons ( records ) precision_recall_chart_from_labels ( labels_tablename ) \u00b6 Generate a precision-recall chart from labelled (ground truth) data. The table of labels should be in the following format, and should be registered as a table with your database: source_dataset_l unique_id_l source_dataset_r unique_id_r clerical_match_score df_1 1 df_2 2 0.99 df_1 1 df_2 3 0.2 Note that source_dataset and unique_id should correspond to the values specified in the settings dict, and the input_table_aliases passed to the linker object. For dedupe_only links, the source_dataset columns can be ommitted. Parameters: Name Type Description Default labels_tablename str Name of table containing labels in the database required threshold_actual float Where the clerical_match_score provided by the user is a probability rather than binary, this value is used as the threshold to classify clerical_match_score s as binary matches or non matches. Defaults to 0.5. required match_weight_round_to_nearest float When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. required Examples: >>> # DuckDBLinker >>> labels = pd . read_csv ( \"my_labels.csv\" ) >>> linker . _con . register ( \"labels\" , labels ) >>> linker . precision_recall_chart_from_labels ( \"labels\" ) >>> >>> # SparkLinker >>> labels = spark . read . csv ( \"my_labels.csv\" , header = True ) >>> labels . createDataFrame ( \"labels\" ) >>> linker . precision_recall_chart_from_labels ( \"labels\" ) Returns: Type Description SplinkDataFrame Source code in splink/linker.py 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 def precision_recall_chart_from_labels ( self , labels_tablename ): \"\"\"Generate a precision-recall chart from labelled (ground truth) data. The table of labels should be in the following format, and should be registered as a table with your database: |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score| |----------------|-----------|----------------|-----------|--------------------| |df_1 |1 |df_2 |2 |0.99 | |df_1 |1 |df_2 |3 |0.2 | Note that `source_dataset` and `unique_id` should correspond to the values specified in the settings dict, and the `input_table_aliases` passed to the `linker` object. For `dedupe_only` links, the `source_dataset` columns can be ommitted. Args: labels_tablename (str): Name of table containing labels in the database threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> # DuckDBLinker >>> labels = pd.read_csv(\"my_labels.csv\") >>> linker._con.register(\"labels\", labels) >>> linker.precision_recall_chart_from_labels(\"labels\") >>> >>> # SparkLinker >>> labels = spark.read.csv(\"my_labels.csv\", header=True) >>> labels.createDataFrame(\"labels\") >>> linker.precision_recall_chart_from_labels(\"labels\") Returns: SplinkDataFrame \"\"\" df_truth_space = roc_table ( self , labels_tablename ) recs = df_truth_space . as_record_dict () return precision_recall_chart ( recs ) predict ( threshold_match_probability = None , threshold_match_weight = None ) \u00b6 Create a dataframe of scored pairwise comparisons using the parameters of the linkage model. Uses the blocking rules specified in the blocking_rules_to_generate_predictions of the settings dictionary to generate the pairwise comparisons. Parameters: Name Type Description Default threshold_match_probability float If specified, filter the results to include only pairwise comparisons with a match_probability above this threshold. Defaults to None. None threshold_match_weight float If specified, filter the results to include only pairwise comparisons with a match_weight above this threshold. Defaults to None. None Examples: >>> linker = DuckDBLinker ( df , connection = \":memory:\" ) >>> linker . load_settings_from_json ( \"saved_settings.json\" ) >>> df = linker . predict ( threshold_match_probability = 0.95 ) >>> df . as_pandas_dataframe ( limit = 5 ) Returns: Name Type Description SplinkDataFrame SplinkDataFrame A SplinkDataFrame of the pairwise comparisons. This represents a table materialised in the database. Methods on the SplinkDataFrame allow you to access the underlying data. Source code in splink/linker.py 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 def predict ( self , threshold_match_probability : float = None , threshold_match_weight : float = None , ) -> SplinkDataFrame : \"\"\"Create a dataframe of scored pairwise comparisons using the parameters of the linkage model. Uses the blocking rules specified in the `blocking_rules_to_generate_predictions` of the settings dictionary to generate the pairwise comparisons. Args: threshold_match_probability (float, optional): If specified, filter the results to include only pairwise comparisons with a match_probability above this threshold. Defaults to None. threshold_match_weight (float, optional): If specified, filter the results to include only pairwise comparisons with a match_weight above this threshold. Defaults to None. Examples: >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> linker.load_settings_from_json(\"saved_settings.json\") >>> df = linker.predict(threshold_match_probability=0.95) >>> df.as_pandas_dataframe(limit=5) Returns: SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons. This represents a table materialised in the database. Methods on the SplinkDataFrame allow you to access the underlying data. \"\"\" # If the user only calls predict, it runs as a single pipeline with no # materialisation of anything self . _initialise_df_concat_with_tf ( materialise = False ) sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) sql = compute_comparison_vector_values_sql ( self . _settings_obj ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vectors\" ) sqls = predict_from_comparison_vectors_sqls ( self . _settings_obj , threshold_match_probability , threshold_match_weight ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) predictions = self . _execute_sql_pipeline () self . _predict_warning () return predictions profile_columns ( column_expressions , top_n = 10 , bottom_n = 10 ) \u00b6 Source code in splink/linker.py 1226 1227 1228 1229 1230 def profile_columns ( self , column_expressions : Union [ str , List [ str ]], top_n = 10 , bottom_n = 10 ): return profile_columns ( self , column_expressions , top_n = top_n , bottom_n = bottom_n ) roc_chart_from_labels ( labels_tablename , threshold_actual = 0.5 , match_weight_round_to_nearest = None ) \u00b6 Generate a ROC chart from labelled (ground truth) data. The table of labels should be in the following format, and should be registered with your database: source_dataset_l unique_id_l source_dataset_r unique_id_r clerical_match_score df_1 1 df_2 2 0.99 df_1 1 df_2 3 0.2 Note that source_dataset and unique_id should correspond to the values specified in the settings dict, and the input_table_aliases passed to the linker object. For dedupe_only links, the source_dataset columns can be ommitted. Parameters: Name Type Description Default labels_tablename str Name of table containing labels in the database required threshold_actual float Where the clerical_match_score provided by the user is a probability rather than binary, this value is used as the threshold to classify clerical_match_score s as binary matches or non matches. Defaults to 0.5. 0.5 match_weight_round_to_nearest float When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. None Examples: >>> # DuckDBLinker >>> labels = pd . read_csv ( \"my_labels.csv\" ) >>> linker . _con . register ( \"labels\" , labels ) >>> linker . roc_chart_from_labels ( \"labels\" ) >>> >>> # SparkLinker >>> labels = spark . read . csv ( \"my_labels.csv\" , header = True ) >>> labels . createDataFrame ( \"labels\" ) >>> linker . roc_chart_from_labels ( \"labels\" ) Returns: Type Description SplinkDataFrame Source code in splink/linker.py 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 def roc_chart_from_labels ( self , labels_tablename , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ): \"\"\"Generate a ROC chart from labelled (ground truth) data. The table of labels should be in the following format, and should be registered with your database: |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score| |----------------|-----------|----------------|-----------|--------------------| |df_1 |1 |df_2 |2 |0.99 | |df_1 |1 |df_2 |3 |0.2 | Note that `source_dataset` and `unique_id` should correspond to the values specified in the settings dict, and the `input_table_aliases` passed to the `linker` object. For `dedupe_only` links, the `source_dataset` columns can be ommitted. Args: labels_tablename (str): Name of table containing labels in the database threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> # DuckDBLinker >>> labels = pd.read_csv(\"my_labels.csv\") >>> linker._con.register(\"labels\", labels) >>> linker.roc_chart_from_labels(\"labels\") >>> >>> # SparkLinker >>> labels = spark.read.csv(\"my_labels.csv\", header=True) >>> labels.createDataFrame(\"labels\") >>> linker.roc_chart_from_labels(\"labels\") Returns: SplinkDataFrame \"\"\" df_truth_space = roc_table ( self , labels_tablename , threshold_actual = threshold_actual , match_weight_round_to_nearest = match_weight_round_to_nearest , ) recs = df_truth_space . as_record_dict () return roc_chart ( recs ) roc_table_from_labels ( labels_tablename , threshold_actual = 0.5 , match_weight_round_to_nearest = None ) \u00b6 Generate truth statistics (false positive etc.) for each threshold value of match_probability, suitable for plotting a ROC chart. The table of labels should be in the following format, and should be registered with your database: source_dataset_l unique_id_l source_dataset_r unique_id_r clerical_match_score df_1 1 df_2 2 0.99 df_1 1 df_2 3 0.2 Note that source_dataset and unique_id should correspond to the values specified in the settings dict, and the input_table_aliases passed to the linker object. For dedupe_only links, the source_dataset columns can be ommitted. Parameters: Name Type Description Default labels_tablename str Name of table containing labels in the database required threshold_actual float Where the clerical_match_score provided by the user is a probability rather than binary, this value is used as the threshold to classify clerical_match_score s as binary matches or non matches. Defaults to 0.5. 0.5 match_weight_round_to_nearest float When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. None Examples: >>> # DuckDBLinker >>> labels = pd . read_csv ( \"my_labels.csv\" ) >>> linker . _con . register ( \"labels\" , labels ) >>> linker . roc_table_from_labels ( \"labels\" ) >>> >>> # SparkLinker >>> labels = spark . read . csv ( \"my_labels.csv\" , header = True ) >>> labels . createDataFrame ( \"labels\" ) >>> linker . roc_table_from_labels ( \"labels\" ) Returns: Type Description SplinkDataFrame SplinkDataFrame Source code in splink/linker.py 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 def roc_table_from_labels ( self , labels_tablename , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ) -> SplinkDataFrame : \"\"\"Generate truth statistics (false positive etc.) for each threshold value of match_probability, suitable for plotting a ROC chart. The table of labels should be in the following format, and should be registered with your database: |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score| |----------------|-----------|----------------|-----------|--------------------| |df_1 |1 |df_2 |2 |0.99 | |df_1 |1 |df_2 |3 |0.2 | Note that `source_dataset` and `unique_id` should correspond to the values specified in the settings dict, and the `input_table_aliases` passed to the `linker` object. For `dedupe_only` links, the `source_dataset` columns can be ommitted. Args: labels_tablename (str): Name of table containing labels in the database threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> # DuckDBLinker >>> labels = pd.read_csv(\"my_labels.csv\") >>> linker._con.register(\"labels\", labels) >>> linker.roc_table_from_labels(\"labels\") >>> >>> # SparkLinker >>> labels = spark.read.csv(\"my_labels.csv\", header=True) >>> labels.createDataFrame(\"labels\") >>> linker.roc_table_from_labels(\"labels\") Returns: SplinkDataFrame \"\"\" return roc_table ( self , labels_tablename , threshold_actual = threshold_actual , match_weight_round_to_nearest = match_weight_round_to_nearest , ) train_m_from_pairwise_labels ( table_name ) \u00b6 Source code in splink/linker.py 1232 1233 1234 def train_m_from_pairwise_labels ( self , table_name ): self . _initialise_df_concat_with_tf ( materialise = True ) estimate_m_from_pairwise_labels ( self , table_name ) unlinkables_chart ( x_col = 'match_weight' , source_dataset = None , as_dict = False ) \u00b6 Generate an interactive chart displaying the proportion of records that are \"unlinkable\" for a given splink score threshold and model parameters. Unlinkable records are those that, even when compared with themselves, do not contain enough information to confirm a match. Parameters: Name Type Description Default x_col str Column to use for the x-axis. Defaults to \"match_weight\". 'match_weight' source_dataset str Name of the source dataset to use for the title of the output chart. None as_dict bool If True, return a dict version of the chart. False Examples: >>> # For the simplest code pipeline, load a pre-trained model >>> # and run this against the test data. >>> df = pd . read_csv ( \"./tests/datasets/fake_1000_from_splink_demos.csv\" ) >>> linker = DuckDBLinker ( df ) >>> linker . load_settings_from_json ( \"saved_settings.json\" ) >>> linker . unlinkables_chart () >>> >>> # For more complex code pipelines, you can run an entire pipeline >>> # that estimates your m and u values, before `unlinkables_chart(). Source code in splink/linker.py 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 def unlinkables_chart ( self , x_col = \"match_weight\" , source_dataset = None , as_dict = False , ): \"\"\"Generate an interactive chart displaying the proportion of records that are \"unlinkable\" for a given splink score threshold and model parameters. Unlinkable records are those that, even when compared with themselves, do not contain enough information to confirm a match. Args: x_col (str, optional): Column to use for the x-axis. Defaults to \"match_weight\". source_dataset (str, optional): Name of the source dataset to use for the title of the output chart. as_dict (bool, optional): If True, return a dict version of the chart. Examples: >>> # For the simplest code pipeline, load a pre-trained model >>> # and run this against the test data. >>> df = pd.read_csv(\"./tests/datasets/fake_1000_from_splink_demos.csv\") >>> linker = DuckDBLinker(df) >>> linker.load_settings_from_json(\"saved_settings.json\") >>> linker.unlinkables_chart() >>> >>> # For more complex code pipelines, you can run an entire pipeline >>> # that estimates your m and u values, before `unlinkables_chart(). \"\"\" # Link our initial df on itself and calculate the % of unlinkable entries records = unlinkables_data ( self , x_col ) return unlinkables_chart ( records , x_col , source_dataset ) waterfall_chart ( records , filter_nulls = True ) \u00b6 Visualise how the final match weight is computed for the provided pairwise record comparisons. Records must be provided as a list of dictionaries. This would usually be obtained from df.as_record_dict(limit=n) where df is a SplinkDataFrame. Examples: >>> df = linker . predict ( threshold_match_weight = 2 ) >>> records = df . as_record_dict ( limit = 10 ) >>> linker . waterfall_chart ( records ) Parameters: Name Type Description Default records List [ dict ] Usually be obtained from df.as_record_dict(limit=n) where df is a SplinkDataFrame. required filter_nulls bool Whether the visualiation shows null comparisons, which have no effect on final match weight. Defaults to True. True Source code in splink/linker.py 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 def waterfall_chart ( self , records : List [ dict ], filter_nulls = True ): \"\"\"Visualise how the final match weight is computed for the provided pairwise record comparisons. Records must be provided as a list of dictionaries. This would usually be obtained from `df.as_record_dict(limit=n)` where `df` is a SplinkDataFrame. Examples: >>> df = linker.predict(threshold_match_weight=2) >>> records = df.as_record_dict(limit=10) >>> linker.waterfall_chart(records) Args: records (List[dict]): Usually be obtained from `df.as_record_dict(limit=n)` where `df` is a SplinkDataFrame. filter_nulls (bool, optional): Whether the visualiation shows null comparisons, which have no effect on final match weight. Defaults to True. \"\"\" self . _raise_error_if_necessary_waterfall_columns_not_computed () return waterfall_chart ( records , self . _settings_obj , filter_nulls )","title":"Linker API"},{"location":"linker.html#documentation-for-linker-object","text":"Manages the data linkage process and holds the data linkage model. Source code in splink/linker.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 1601 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 1617 1618 1619 1620 1621 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631 1632 1633 1634 1635 1636 1637 1638 1639 1640 1641 1642 1643 1644 1645 1646 1647 1648 1649 1650 1651 1652 1653 1654 1655 1656 1657 1658 1659 1660 1661 1662 1663 1664 1665 1666 1667 1668 1669 1670 1671 1672 1673 1674 1675 1676 1677 1678 1679 1680 1681 1682 1683 1684 1685 1686 1687 1688 1689 1690 1691 1692 1693 1694 1695 1696 1697 1698 1699 1700 1701 1702 1703 1704 1705 1706 1707 1708 1709 1710 1711 1712 1713 1714 1715 1716 1717 1718 1719 1720 1721 1722 1723 1724 1725 1726 1727 1728 1729 1730 1731 1732 1733 1734 class Linker : \"\"\"Manages the data linkage process and holds the data linkage model.\"\"\" def __init__ ( self , input_table_or_tables : Union [ str , list ], settings_dict : dict = None , set_up_basic_logging : bool = True , input_table_aliases : Union [ str , list ] = None , ): \"\"\"The Linker object manages the data linkage process and holds the data linkage model. Most of Splink's functionality can be accessed by calling methods (functions) on the linker, such as `linker.predict()`, `linker.profile_columns()` etc. The Linker class is intended for subclassing for specific backends, e.g. a DuckDBLinker. Args: input_table_or_tables (Union[str, list]): Input data into the linkage model. Either a single string (the name of a table in a database) for deduplication jobs, or a list of strings (the name of tables in a database) for link_only or link_and_dedupe settings_dict (dict, optional): A Splink settings dictionary. If not provided when the object is created, can later be added using `linker.initialise_settings()` Defaults to None. set_up_basic_logging (bool, optional): If true, sets ups up basic logging so that Splink sends messages at INFO level to stdout. Defaults to True. input_table_aliases (Union[str, list], optional): Labels assigned to input tables in Splink outputs. If the names of the tables in the input database are long or unspecific, this argument can be used to attach more easily readable/interpretable names. Defaults to None. \"\"\" self . _pipeline = SQLPipeline () self . _settings_dict = settings_dict if settings_dict is None : self . _settings_obj_ = None else : self . _settings_obj_ = Settings ( settings_dict ) self . _input_tables_dict = self . _get_input_tables_dict ( input_table_or_tables , input_table_aliases ) self . _validate_input_dfs () self . _em_training_sessions = [] self . _names_of_tables_created_by_splink : list = [] self . _find_new_matches_mode = False self . _train_u_using_random_sample_mode = False self . _compare_two_records_mode = False self . _self_link_mode = False self . _output_schema = \"\" self . debug_mode = False if set_up_basic_logging : logging . basicConfig ( format = \" %(message)s \" , ) splink_logger = logging . getLogger ( \"splink\" ) splink_logger . setLevel ( logging . INFO ) @property def _settings_obj ( self ) -> Settings : if self . _settings_obj_ is None : raise ValueError ( \"You did not provide a settings dictionary when you \" \"created the linker. To continue, you need to provide a settings \" \"dictionary using the `initialise_settings()` method on your linker \" \"object. i.e. linker.initialise_settings(settings_dict)\" ) return self . _settings_obj_ @property def _input_tablename_l ( self ): if self . _find_new_matches_mode : return \"__splink__df_concat_with_tf\" if self . _self_link_mode : return \"__splink__df_concat_with_tf\" if self . _compare_two_records_mode : return \"__splink__compare_two_records_left_with_tf\" if self . _train_u_using_random_sample_mode : return \"__splink__df_concat_with_tf_sample\" if self . _two_dataset_link_only : return \"__splink_df_concat_with_tf_left\" return \"__splink__df_concat_with_tf\" @property def _input_tablename_r ( self ): if self . _find_new_matches_mode : return \"__splink__df_new_records_with_tf\" if self . _self_link_mode : return \"__splink__df_concat_with_tf\" if self . _compare_two_records_mode : return \"__splink__compare_two_records_right_with_tf\" if self . _train_u_using_random_sample_mode : return \"__splink__df_concat_with_tf_sample\" if self . _two_dataset_link_only : return \"__splink_df_concat_with_tf_right\" return \"__splink__df_concat_with_tf\" @property def _two_dataset_link_only ( self ): # Two dataset link only join is a special case where an inner join of the # two datasets is much more efficient than self-joining the vertically # concatenation of all input datasets if self . _find_new_matches_mode : return True if self . _compare_two_records_mode : return True if ( len ( self . _input_tables_dict ) == 2 and self . _settings_obj . _link_type == \"link_only\" ): return True else : return False def _prepend_schema_to_table_name ( self , table_name ): if self . _output_schema : return f \" { self . _output_schema } . { table_name } \" else : return table_name def _initialise_df_concat ( self , materialise = True ): if self . _table_exists_in_database ( \"__splink__df_concat\" ): return sql = vertically_concatenate_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_concat\" ) self . _execute_sql_pipeline ( materialise_as_hash = False ) def _initialise_df_concat_with_tf ( self , materialise = True ): if self . _table_exists_in_database ( \"__splink__df_concat_with_tf\" ): return sql = vertically_concatenate_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_concat\" ) sqls = compute_all_term_frequencies_sqls ( self ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) if self . _two_dataset_link_only : # If we do not materialise __splink_df_concat_with_tf # we'd have to run all the code up to this point twice self . _execute_sql_pipeline ( materialise_as_hash = False ) source_dataset_col = self . _settings_obj . _source_dataset_column_name # Need df_l to be the one with the lowest id to preeserve the property # that the left dataset is the one with the lowest concatenated id keys = self . _input_tables_dict . keys () keys = list ( sorted ( keys )) df_l = self . _input_tables_dict [ keys [ 0 ]] df_r = self . _input_tables_dict [ keys [ 1 ]] sql = f \"\"\" select * from __splink__df_concat_with_tf where { source_dataset_col } = ' { df_l . templated_name } ' \"\"\" self . _enqueue_sql ( sql , \"__splink_df_concat_with_tf_left\" ) self . _execute_sql_pipeline ( materialise_as_hash = False ) sql = f \"\"\" select * from __splink__df_concat_with_tf where { source_dataset_col } = ' { df_r . templated_name } ' \"\"\" self . _enqueue_sql ( sql , \"__splink_df_concat_with_tf_right\" ) self . _execute_sql_pipeline ( materialise_as_hash = False ) else : if materialise : self . _execute_sql_pipeline ( materialise_as_hash = False ) def _table_to_splink_dataframe ( self , templated_name , physical_name ) -> SplinkDataFrame : \"\"\"Create a SplinkDataframe from a table in the underlying database called `physical_name`. Associate a `templated_name` with this table, which signifies the purpose or 'meaning' of this table to splink. (e.g. `__splink__df_blocked`) Args: templated_name (str): The purpose of the table to Splink physical_name (str): The name of the table in the underlying databse \"\"\" raise NotImplementedError ( \"_table_to_splink_dataframe not implemented on this linker\" ) def _enqueue_sql ( self , sql , output_table_name ): \"\"\"Add sql to the current pipeline, but do not execute the pipeline.\"\"\" self . _pipeline . enqueue_sql ( sql , output_table_name ) def _execute_sql_pipeline ( self , input_dataframes : List [ SplinkDataFrame ] = [], materialise_as_hash = True , use_cache = True , transpile = True , ) -> SplinkDataFrame : \"\"\"Execute the SQL queued in the current pipeline as a single statement e.g. `with a as (), b as , c as (), select ... from c`, then execute the pipeline, returning the resultant table as a SplinkDataFrame Args: input_dataframes (List[SplinkDataFrame], optional): A 'starting point' of SplinkDataFrames if needed. Defaults to []. materialise_as_hash (bool, optional): If true, the output tablename will end in a unique identifer. Defaults to True. use_cache (bool, optional): If true, look at whether the SQL pipeline has been executed before, and if so, use the existing result. Defaults to True. transpile (bool, optional): Transpile the SQL using SQLGlot. Defaults to True. Returns: SplinkDataFrame: An abstraction representing the table created by the sql pipeline \"\"\" if not self . debug_mode : sql_gen = self . _pipeline . _generate_pipeline ( input_dataframes ) output_tablename_templated = self . _pipeline . queue [ - 1 ] . output_table_name dataframe = self . _sql_to_splink_dataframe ( sql_gen , output_tablename_templated , materialise_as_hash , use_cache , transpile , ) return dataframe else : # In debug mode, we do not pipeline the sql and print the # results of each part of the pipeline for task in self . _pipeline . _generate_pipeline_parts ( input_dataframes ): output_tablename = task . output_table_name sql = task . sql print ( \"------\" ) print ( f \"--------Creating table: { output_tablename } --------\" ) dataframe = self . _sql_to_splink_dataframe ( sql , output_tablename , materialise_as_hash = False , use_cache = False , transpile = transpile , ) return dataframe def _execute_sql ( self , sql , templated_name , physical_name , transpile = True ): raise NotImplementedError ( f \"execute_sql not implemented for { type ( self ) } \" ) def _enqueue_and_execute_sql_pipeline ( self , sql , output_table_name , materialise_as_hash = True , use_cache = True , transpile = True , ) -> SplinkDataFrame : \"\"\"Wrapper method to enqueue and execute a sql pipeline in a single call.\"\"\" self . _enqueue_sql ( sql , output_table_name ) return self . _execute_sql_pipeline ([], materialise_as_hash , use_cache , transpile ) def _sql_to_splink_dataframe ( self , sql , output_tablename_templated , materialise_as_hash = True , use_cache = True , transpile = True , ) -> SplinkDataFrame : \"\"\"Execute sql (or if identical sql has been run before, return cached results), reset pipeline, and return a SplinkDataFrame representing the results of the sql\"\"\" self . _pipeline . reset () hash = hashlib . sha256 ( sql . encode ()) . hexdigest ()[: 7 ] # Ensure hash is valid sql table name table_name_hash = f \" { output_tablename_templated } _ { hash } \" if use_cache : if self . _table_exists_in_database ( output_tablename_templated ): logger . debug ( f \"Using existing table { output_tablename_templated } \" ) return self . _table_to_splink_dataframe ( output_tablename_templated , output_tablename_templated ) if self . _table_exists_in_database ( table_name_hash ): logger . debug ( f \"Using cache for { output_tablename_templated } \" f \" with physical name { table_name_hash } \" ) return self . _table_to_splink_dataframe ( output_tablename_templated , table_name_hash ) if self . debug_mode : print ( sql ) if materialise_as_hash : splink_dataframe = self . _execute_sql ( sql , output_tablename_templated , table_name_hash , transpile = transpile ) else : splink_dataframe = self . _execute_sql ( sql , output_tablename_templated , output_tablename_templated , transpile = transpile , ) self . _names_of_tables_created_by_splink . append ( splink_dataframe . physical_name ) if self . debug_mode : df_pd = splink_dataframe . as_pandas_dataframe () try : from IPython.display import display display ( df_pd ) except ModuleNotFoundError : print ( df_pd ) return splink_dataframe def __deepcopy__ ( self , memo ): \"\"\"When we do EM training, we need a copy of the linker which is independent of the main linker e.g. setting parameters on the copy will not affect the main linker. This method implements ensures linker can be deepcopied. \"\"\" new_linker = copy ( self ) new_linker . _em_training_sessions = [] new_settings = deepcopy ( self . _settings_obj ) new_linker . _settings_obj_ = new_settings return new_linker def _ensure_aliases_populated_and_is_list ( self , input_table_or_tables , input_table_aliases ): if input_table_aliases is None : input_table_aliases = input_table_or_tables input_table_aliases = ensure_is_list ( input_table_aliases ) return input_table_aliases def _get_input_tables_dict ( self , input_table_or_tables , input_table_aliases ): input_table_or_tables = ensure_is_list ( input_table_or_tables ) input_table_aliases = self . _ensure_aliases_populated_and_is_list ( input_table_or_tables , input_table_aliases ) d = {} for table_name , table_alias in zip ( input_table_or_tables , input_table_aliases ): d [ table_alias ] = self . _table_to_splink_dataframe ( table_alias , table_name ) return d def _get_input_tf_dict ( self , df_dict ): d = {} for df_name , df_value in df_dict . items (): renamed = colname_to_tf_tablename ( df_name ) d [ renamed ] = self . _table_to_splink_dataframe ( renamed , df_value ) return d def _predict_warning ( self ): if not self . _settings_obj . _is_fully_trained : msg = ( \" \\n -- WARNING -- \\n \" \"You have called predict(), but there are some parameter \" \"estimates which have neither been estimated or specified in your \" \"settings dictionary. To produce predictions the following\" \" untrained trained parameters will use default values.\" ) messages = self . _settings_obj . _not_trained_messages () warn_message = \" \\n \" . join ([ msg ] + messages ) logger . warning ( warn_message ) def _table_exists_in_database ( self , table_name ): raise NotImplementedError ( f \"table_exists_in_database not implemented for { type ( self ) } \" ) def _validate_input_dfs ( self ): for df in self . _input_tables_dict . values (): df . validate () if self . _settings_obj_ is not None : if self . _settings_obj . _link_type == \"dedupe_only\" : if len ( self . _input_tables_dict ) > 1 : raise ValueError ( 'If link_type = \"dedupe only\" then input tables must contain' \"only a single input table\" , ) def _populate_probability_two_random_records_match_from_trained_values ( self ): recip_prop_matches_estimates = [] logger . log ( 15 , ( \"---- Using training sessions to compute \" \"probability two random records match ----\" ), ) for em_training_session in self . _em_training_sessions : training_lambda = ( em_training_session . _settings_obj . _probability_two_random_records_match ) training_lambda_bf = prob_to_bayes_factor ( training_lambda ) reverse_levels = ( em_training_session . _comparison_levels_to_reverse_blocking_rule ) logger . log ( 15 , \" \\n \" f \"Probability two random records match from trained model blocking on \" f \" { em_training_session . _blocking_rule_for_training } : \" f \" { training_lambda : ,.3f } \" , ) for reverse_level in reverse_levels : # Get comparison level on current settings obj cc = self . _settings_obj . _get_comparison_by_output_column_name ( reverse_level . comparison . _output_column_name ) cl = cc . _get_comparison_level_by_comparison_vector_value ( reverse_level . _comparison_vector_value ) if cl . _has_estimated_values : bf = cl . _trained_m_median / cl . _trained_u_median else : bf = cl . _bayes_factor logger . log ( 15 , f \"Reversing comparison level { cc . _output_column_name } \" f \" using bayes factor { bf : ,.3f } \" , ) training_lambda_bf = training_lambda_bf / bf as_prob = bayes_factor_to_prob ( training_lambda_bf ) logger . log ( 15 , ( \"This estimate of probability two random records match now: \" f \" { as_prob : ,.3f } \" f \"with reciprocal { ( 1 / as_prob ) : ,.3f } \" ), ) logger . log ( 15 , \" \\n ---------\" ) p = bayes_factor_to_prob ( training_lambda_bf ) recip_prop_matches_estimates . append ( 1 / p ) prop_matches_estimate = 1 / median ( recip_prop_matches_estimates ) self . _settings_obj . _probability_two_random_records_match = prop_matches_estimate logger . log ( 15 , \" \\n Median of prop of matches estimates: \" f \" { self . _settings_obj . _probability_two_random_records_match : ,.3f } \" \"reciprocal \" f \" { 1 / self . _settings_obj . _probability_two_random_records_match : ,.3f } \" , ) def _records_to_table ( records , as_table_name ): # Create table in database containing records # Probably quite difficult to implement correctly # Due to data type issues. raise NotImplementedError def _populate_m_u_from_trained_values ( self ): ccs = self . _settings_obj . comparisons for cc in ccs : for cl in cc . _comparison_levels_excluding_null : if cl . _has_estimated_u_values : cl . u_probability = cl . _trained_u_median if cl . _has_estimated_m_values : cl . m_probability = cl . _trained_m_median def _delete_tables_created_by_splink_from_db ( self , retain_term_frequency = True , retain_df_concat_with_tf = True ): tables_remaining = [] for name in self . _names_of_tables_created_by_splink : # Only delete tables explicitly marked as having been created by splink if \"__splink__\" not in name : tables_remaining . append ( name ) continue if name == \"__splink__df_concat_with_tf\" : if retain_df_concat_with_tf : tables_remaining . append ( name ) else : self . _delete_table_from_database ( name ) elif name . startswith ( \"__splink__df_tf_\" ): if retain_term_frequency : tables_remaining . append ( name ) else : self . _delete_table_from_database ( name ) else : self . _delete_table_from_database ( name ) self . _names_of_tables_created_by_splink = tables_remaining def _raise_error_if_necessary_waterfall_columns_not_computed ( self ): ricc = self . _settings_obj . _retain_intermediate_calculation_columns rmc = self . _settings_obj . _retain_matching_columns if not ( ricc and rmc ): raise ValueError ( \"retain_intermediate_calculation_columns and \" \"retain_matching_columns must both be set to True in your settings\" \" dictionary to use this function, because otherwise the necessary \" \"columns will not be available in the input records.\" f \" Their current values are { ricc } and { rmc } , respectively. \" \"Please re-run your linkage with them both set to True.\" ) def initialise_settings ( self , settings_dict : dict ): \"\"\"Initialise settings for the linker. To be used if settings were not passed to the linker on creation. Examples: >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> linker.profile_columns(\"first_name\", \"surname\") >>> linker.initialise_settings(settings_dict) Args: settings_dict (dict): A Splink settings dictionary \"\"\" self . _settings_dict = settings_dict self . _settings_obj_ = Settings ( settings_dict ) self . _validate_input_dfs () def compute_tf_table ( self , column_name : str ) -> SplinkDataFrame : \"\"\"Compute a term frequency table for a given column and persist to the database This method is useful if you want to pre-compute term frequency tables e.g. so that real time linkage executes faster, or so that you can estimate various models without having to recompute term frequency tables each time Examples: >>> # Example 1: Real time linkage >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> linker.load_settings_from_json(\"saved_settings.json\") >>> linker.compute_tf_table(\"surname\") >>> linker.compare_two_records(record_left, record_right) >>> # Example 2: Pre-computed term frequency tables in Spark >>> linker = SparkLinker(df) >>> df_first_name_tf = linker.compute_tf_table(\"first_name\") >>> df_first_name_tf.write.parquet(\"folder/first_name_tf\") >>> >>> # On subsequent data linking job, read this table rather than recompute >>> df_first_name_tf = spark.read.parquet(\"folder/first_name_tf\") >>> df_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\") Args: column_name (str): The column name in the input table Returns: SplinkDataFrame: The resultant table as a splink data frame \"\"\" sql = vertically_concatenate_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_concat\" ) input_col = InputColumn ( column_name , tf_adjustments = True ) sql = term_frequencies_for_single_column_sql ( input_col ) self . _enqueue_sql ( sql , colname_to_tf_tablename ( input_col )) return self . _execute_sql_pipeline ( materialise_as_hash = False ) def deterministic_link ( self ) -> SplinkDataFrame : \"\"\"Uses the blocking rules specified by `blocking_rules_to_generate_predictions` in the settings dictionary to generate pairwise record comparisons. This should be a list of blocking rules which are strict enough to generate only true links. Deterministic linkage, however, is likely to result in missed links (false negatives). Examples: >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> >>> settings = { >>> \"link_type\": \"dedupe_only\", >>> \"blocking_rules_to_generate_predictions\": [ >>> \"l.first_name = r.first_name\", >>> \"l.surname = r.surname\", >>> ], >>> \"comparisons\": [] >>> } >>> >>> from splink.duckdb.duckdb_linker import DuckDBLinker >>> >>> linker = DuckDBLinker(df, settings, connection=\":memory:\") >>> df = linker.deterministic_link() Returns: SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons. This represents a table materialised in the database. Methods on the SplinkDataFrame allow you to access the underlying data. \"\"\" self . _initialise_df_concat_with_tf () sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) return self . _execute_sql_pipeline () def estimate_u_using_random_sampling ( self , target_rows : int ): \"\"\"Estimate the u parameters of the linkage model using random sampling. The u parameters represent the proportion of record comparisons that fall into each comparison level amongst truly non-matching records. This procedure takes a sample of the data and generates the cartesian product of pairwise record comparisons amongst the sampled records. The validity of the u values rests on the assumption that the resultant pairwise comparisons are non-matches (or at least, they are very unlikely to be matches). For large datasets, this is typically true. Args: target_rows (int): The target number of pairwise record comparisons from which to derive the u values. Larger will give more accurate estimates but lead to longer runtimes. In our experience at least 1e9 (one billion) gives best results but can take a long time to compute. 1e7 (ten million) is often adequate whilst testing different model specifications, before the final model is estimated. Examples: >>> linker.estimate_u_using_random_sampling(1e8) Returns: Updates the estimated u parameters within the linker object and returns nothing. \"\"\" self . _initialise_df_concat_with_tf ( materialise = True ) estimate_u_values ( self , target_rows ) self . _populate_m_u_from_trained_values () self . _settings_obj . _columns_without_estimated_parameters_message () def estimate_m_from_label_column ( self , label_colname : str ): \"\"\"Estimate the m parameters of the linkage model from a label (ground truth) column in the input dataframe(s). The m parameters represent the proportion of record comparisons that fall into each comparison level amongst truly matching records. The ground truth column is used to generate pairwise record comparisons which are then assumed to be matches. For example, if the entity being matched is persons, and your input dataset(s) contain social security number, this could be used to estimate the m values for the model. Note that this column does not need to be fully populated. A common case is where a unique identifier such as social security number is only partially populated. Args: label_colname (str): The name of the column containing the ground truth label in the input data. Examples: >>> linker.estimate_m_from_label_column(\"social_security_number\") Returns: Updates the estimated m parameters within the linker object and returns nothing. \"\"\" self . _initialise_df_concat_with_tf ( materialise = True ) estimate_m_values_from_label_column ( self , self . _input_tables_dict , label_colname ) self . _populate_m_u_from_trained_values () self . _settings_obj . _columns_without_estimated_parameters_message () def estimate_parameters_using_expectation_maximisation ( self , blocking_rule : str , comparisons_to_deactivate : List [ Union [ str , Comparison ]] = None , comparison_levels_to_reverse_blocking_rule : List [ ComparisonLevel ] = None , fix_probability_two_random_records_match : bool = False , fix_m_probabilities = False , fix_u_probabilities = True , ) -> EMTrainingSession : \"\"\"Estimate the parameters of the linkage model using expectation maximisation. By default, the m probabilities are estimated, but not the u probabilities, because good estiamtes for the u probabilities can be obtained from `linker.estimate_u_using_random_sampling()`. You can change this by setting `fix_u_probabilities` to False. The blocking rule provided is used to generate pairwise record comparisons. Usually, this should be a blocking rule that results in a dataframe where matches are between about 1% and 99% of the comparisons. By default, m parameters are estimated for all comparisons except those which are included in the blocking rule. For example, if the blocking rule is `l.first_name = r.first_name`, then parameter esimates will be made for all comparison except those which use `first_name` in their sql_condition By default, the probability two random records match is estimated for the blocked data, and then the m and u parameters for the columns specified in the blocking rules are used to estiamte the global probability two random records match. To control which comparisons should have their parameter estimated, and the process of 'reversing out' the global probability two random records match, the user may specify `comparisons_to_deactivate` and `comparison_levels_to_reverse_blocking_rule`. This is useful, for example if you block on the dmetaphone of a column but match on the original column. Examples: >>> # Default behaviour >>> br_training = \"l.first_name = r.first_name and l.dob = r.dob\" >>> linker.estimate_parameters_using_expectation_maximisation(br_training) >>> # Specify which comparisons to deactivate >>> br_training = \"l.dmeta_first_name = r.dmeta_first_name\" >>> settings_obj = linker._settings_obj >>> comp = settings_obj._get_comparison_by_output_column_name(\"first_name\") >>> dmeta_level = comp._get_comparison_level_by_comparison_vector_value(1) >>> linker.estimate_parameters_using_expectation_maximisation( >>> br_training, >>> comparisons_to_deactivate=[\"first_name\"], >>> comparison_levels_to_reverse_blocking_rule=[dmeta_level], >>> ) Args: blocking_rule (str): The blocking rule used to generate pairwise record comparisons. comparisons_to_deactivate (list, optional): By default, splink will analyse the blocking rule provided and estimate the m parameters for all comaprisons except those included in the blocking rule. If comparisons_to_deactivate are provided, spink will instead estimate m parameters for all comparison except those specified in the comparisons_to_deactivate list. This list can either contain the output_column_name of the Comparison as a string, or Comparison objects. Defaults to None. comparison_levels_to_reverse_blocking_rule (list, optional): By default, splink will analyse the blocking rule provided and adjust the global probability two random records match to account for the matches specified in the blocking rule. If provided, this argument will overrule this default behaviour. The user must provide a list of ComparisonLevel objects. Defaults to None. fix_probability_two_random_records_match (bool, optional): If True, do not update the probability two random records match after each iteration. Defaults to False. fix_m_probabilities (bool, optional): If True, do not update the m probabilities after each iteration. Defaults to False. fix_u_probabilities (bool, optional): If True, do not update the u probabilities after each iteration. Defaults to True. Examples: >>> blocking_rule = \"l.first_name = r.first_name and l.dob = r.dob\" >>> linker.estimate_parameters_using_expectation_maximisation(blocking_rule) Returns: EMTrainingSession: An object containing information about the training session such as how parameters changed during the iteration history \"\"\" self . _initialise_df_concat_with_tf ( materialise = True ) if comparisons_to_deactivate : # If user provided a string, convert to Comparison object comparisons_to_deactivate = [ self . _settings_obj . _get_comparison_by_output_column_name ( n ) if isinstance ( n , str ) else n for n in comparisons_to_deactivate ] if comparison_levels_to_reverse_blocking_rule is None : logger . warning ( \" \\n WARNING: \\n \" \"You have provided comparisons_to_deactivate but not \" \"comparison_levels_to_reverse_blocking_rule. \\n \" \"If comparisons_to_deactivate is provided, then \" \"you usually need to provide corresponding \" \"comparison_levels_to_reverse_blocking_rule. \" \"because each comparison to deactivate if effectively treated \" \"as an exact match.\" ) em_training_session = EMTrainingSession ( self , blocking_rule , fix_u_probabilities = fix_u_probabilities , fix_m_probabilities = fix_m_probabilities , fix_probability_two_random_records_match = fix_probability_two_random_records_match , # noqa 501 comparisons_to_deactivate = comparisons_to_deactivate , comparison_levels_to_reverse_blocking_rule = comparison_levels_to_reverse_blocking_rule , # noqa 501 ) em_training_session . _train () self . _populate_m_u_from_trained_values () self . _populate_probability_two_random_records_match_from_trained_values () self . _settings_obj . _columns_without_estimated_parameters_message () return em_training_session def predict ( self , threshold_match_probability : float = None , threshold_match_weight : float = None , ) -> SplinkDataFrame : \"\"\"Create a dataframe of scored pairwise comparisons using the parameters of the linkage model. Uses the blocking rules specified in the `blocking_rules_to_generate_predictions` of the settings dictionary to generate the pairwise comparisons. Args: threshold_match_probability (float, optional): If specified, filter the results to include only pairwise comparisons with a match_probability above this threshold. Defaults to None. threshold_match_weight (float, optional): If specified, filter the results to include only pairwise comparisons with a match_weight above this threshold. Defaults to None. Examples: >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> linker.load_settings_from_json(\"saved_settings.json\") >>> df = linker.predict(threshold_match_probability=0.95) >>> df.as_pandas_dataframe(limit=5) Returns: SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons. This represents a table materialised in the database. Methods on the SplinkDataFrame allow you to access the underlying data. \"\"\" # If the user only calls predict, it runs as a single pipeline with no # materialisation of anything self . _initialise_df_concat_with_tf ( materialise = False ) sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) sql = compute_comparison_vector_values_sql ( self . _settings_obj ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vectors\" ) sqls = predict_from_comparison_vectors_sqls ( self . _settings_obj , threshold_match_probability , threshold_match_weight ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) predictions = self . _execute_sql_pipeline () self . _predict_warning () return predictions def find_matches_to_new_records ( self , records : List [ dict ], blocking_rules = None , match_weight_threshold =- 4 ) -> SplinkDataFrame : \"\"\"Given one or more records, find records in the input dataset(s) which match and return in order of the splink prediction score. This effectively provides a way of searching the input datasets for given record(s) Args: records (List[dict]): Input search record(s). blocking_rules (str, optional): Blocking rules to select which records to find and score. If None, do not use a blocking rule - meaning the input records will be compared to all records provided to the linker when it was instantiated. Defaults to None. match_weight_threshold (int, optional): Return matches with a match weight above this threshold. Defaults to -4. Examples: >>> linker = DuckDBLinker(df) >>> linker.load_settings_from_json(\"saved_settings.json\") >>> # Pre-compute tf tables for any tables with >>> # term frequency adjustments >>> linker.compute_tf_table(\"first_name\") >>> record = {'unique_id': 1, >>> 'first_name': \"John\", >>> 'surname': \"Smith\", >>> 'dob': \"1971-05-24\", >>> 'city': \"London\", >>> 'email': \"john@smith.net\" >>> } >>> df = linker.find_matches_to_new_records([record], blocking_rules=[]) Returns: SplinkDataFrame: The pairwise comparisons. \"\"\" original_blocking_rules = ( self . _settings_obj . _blocking_rules_to_generate_predictions ) original_link_type = self . _settings_obj . _link_type self . _records_to_table ( records , \"__splink__df_new_records\" ) if blocking_rules is not None : self . _settings_obj . _blocking_rules_to_generate_predictions = blocking_rules self . _settings_obj . _link_type = \"link_only_find_matches_to_new_records\" self . _find_new_matches_mode = True sql = _join_tf_to_input_df_sql ( self ) sql = sql . replace ( \"__splink__df_concat\" , \"__splink__df_new_records\" ) self . _enqueue_sql ( sql , \"__splink__df_new_records_with_tf\" ) sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) sql = compute_comparison_vector_values_sql ( self . _settings_obj ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vectors\" ) sqls = predict_from_comparison_vectors_sqls ( self . _settings_obj ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) sql = f \"\"\" select * from __splink__df_predict where match_weight > { match_weight_threshold } \"\"\" self . _enqueue_sql ( sql , \"__splink_find_matches_predictions\" ) predictions = self . _execute_sql_pipeline ( use_cache = False ) self . _settings_obj . _blocking_rules_to_generate_predictions = ( original_blocking_rules ) self . _settings_obj . _link_type = original_link_type self . _find_new_matches_mode = False return predictions def compare_two_records ( self , record_1 : dict , record_2 : dict ): \"\"\"Use the linkage model to compare and score a pairwise record comparison based on the two input records provided Args: record_1 (dict): dictionary representing the first record. Columns names and data types must be the same as the columns in the settings object record_2 (dict): dictionary representing the second record. Columns names and data types must be the same as the columns in the settings object Examples: >>> linker = DuckDBLinker(df) >>> linker.load_settings_from_json(\"saved_settings.json\") >>> linker.compare_two_records(record_left, record_right) Returns: SplinkDataFrame: Pairwise comparison with scored prediction \"\"\" original_blocking_rules = ( self . _settings_obj . _blocking_rules_to_generate_predictions ) original_link_type = self . _settings_obj . _link_type self . _compare_two_records_mode = True self . _settings_obj . _blocking_rules_to_generate_predictions = [] self . _records_to_table ([ record_1 ], \"__splink__compare_two_records_left\" ) self . _records_to_table ([ record_2 ], \"__splink__compare_two_records_right\" ) sql_join_tf = _join_tf_to_input_df_sql ( self ) sql_join_tf = sql_join_tf . replace ( \"__splink__df_concat\" , \"__splink__compare_two_records_left\" ) self . _enqueue_sql ( sql_join_tf , \"__splink__compare_two_records_left_with_tf\" ) sql_join_tf = sql_join_tf . replace ( \"__splink__compare_two_records_left\" , \"__splink__compare_two_records_right\" ) self . _enqueue_sql ( sql_join_tf , \"__splink__compare_two_records_right_with_tf\" ) sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) sql = compute_comparison_vector_values_sql ( self . _settings_obj ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vectors\" ) sqls = predict_from_comparison_vectors_sqls ( self . _settings_obj ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) predictions = self . _execute_sql_pipeline ( use_cache = False ) self . _settings_obj . _blocking_rules_to_generate_predictions = ( original_blocking_rules ) self . _settings_obj . _link_type = original_link_type self . _compare_two_records_mode = False return predictions def _self_link ( self ) -> SplinkDataFrame : \"\"\"Use the linkage model to compare and score all records in our input df with themselves. Returns: SplinkDataFrame: Scored pairwise comparisons of the input records to themselves. \"\"\" original_blocking_rules = ( self . _settings_obj . _blocking_rules_to_generate_predictions ) original_link_type = self . _settings_obj . _link_type # Changes our sql to allow for a self link. # This is used in `_sql_gen_where_condition` in blocking.py # to remove any 'where' clauses when blocking (normally when blocking # we want to *remove* self links!) self . _self_link_mode = True # Block on uid i.e. create pairwise record comparisons where the uid matches uid_cols = self . _settings_obj . _unique_id_input_columns uid_l = _composite_unique_id_from_edges_sql ( uid_cols , None , \"l\" ) uid_r = _composite_unique_id_from_edges_sql ( uid_cols , None , \"r\" ) self . _settings_obj . _blocking_rules_to_generate_predictions = [ f \" { uid_l } = { uid_r } \" ] self . _initialise_df_concat_with_tf () sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) sql = compute_comparison_vector_values_sql ( self . _settings_obj ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vectors\" ) sqls = predict_from_comparison_vectors_sqls ( self . _settings_obj ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) predictions = self . _execute_sql_pipeline ( use_cache = False ) self . _settings_obj . _blocking_rules_to_generate_predictions = ( original_blocking_rules ) self . _settings_obj . _link_type = original_link_type self . _self_link_mode = False return predictions def cluster_pairwise_predictions_at_threshold ( self , df_predict : SplinkDataFrame , threshold_match_probability : float ) -> SplinkDataFrame : \"\"\"Clusters the pairwise match predictions that result from `linker.predict()` into groups of connected record using the connected components graph clustering algorithm Records with an estimated `match_probability` above `threshold_match_probability` are considered to be a match (i.e. they represent the same entity). Args: df_predict (SplinkDataFrame): The results of `linker.predict()` threshold_match_probability (float): Filter the pairwise match predictions to include only pairwise comparisons with a match_probability above this threshold. This dataframe is then fed into the clustering algorithm. Returns: SplinkDataFrame: A SplinkDataFrame containing a list of all IDs, clustered into groups based on the desired match threshold. \"\"\" self . _initialise_df_concat_with_tf ( df_predict ) edges_table = _cc_create_unique_id_cols ( self , df_predict , threshold_match_probability , ) cc = solve_connected_components ( self , edges_table ) return cc def delete_tables_created_by_splink_from_db ( self , retain_term_frequency = True , retain_df_concat_with_tf = True ): tables_remaining = [] current_tables = self . _names_of_tables_created_by_splink for splink_df in current_tables : name = splink_df . templated_name # Only delete tables explicitly marked as having been created by splink if \"__splink__\" not in name : tables_remaining . append ( splink_df ) continue if name == \"__splink__df_concat_with_tf\" : if retain_df_concat_with_tf : tables_remaining . append ( splink_df ) else : self . _delete_table_from_database ( name ) elif name . startswith ( \"__splink__df_tf_\" ): if retain_term_frequency : tables_remaining . append ( splink_df ) else : self . _delete_table_from_database ( name ) else : self . _delete_table_from_database ( name ) self . _names_of_tables_created_by_splink = tables_remaining def profile_columns ( self , column_expressions : Union [ str , List [ str ]], top_n = 10 , bottom_n = 10 ): return profile_columns ( self , column_expressions , top_n = top_n , bottom_n = bottom_n ) def train_m_from_pairwise_labels ( self , table_name ): self . _initialise_df_concat_with_tf ( materialise = True ) estimate_m_from_pairwise_labels ( self , table_name ) def roc_chart_from_labels ( self , labels_tablename , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ): \"\"\"Generate a ROC chart from labelled (ground truth) data. The table of labels should be in the following format, and should be registered with your database: |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score| |----------------|-----------|----------------|-----------|--------------------| |df_1 |1 |df_2 |2 |0.99 | |df_1 |1 |df_2 |3 |0.2 | Note that `source_dataset` and `unique_id` should correspond to the values specified in the settings dict, and the `input_table_aliases` passed to the `linker` object. For `dedupe_only` links, the `source_dataset` columns can be ommitted. Args: labels_tablename (str): Name of table containing labels in the database threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> # DuckDBLinker >>> labels = pd.read_csv(\"my_labels.csv\") >>> linker._con.register(\"labels\", labels) >>> linker.roc_chart_from_labels(\"labels\") >>> >>> # SparkLinker >>> labels = spark.read.csv(\"my_labels.csv\", header=True) >>> labels.createDataFrame(\"labels\") >>> linker.roc_chart_from_labels(\"labels\") Returns: SplinkDataFrame \"\"\" df_truth_space = roc_table ( self , labels_tablename , threshold_actual = threshold_actual , match_weight_round_to_nearest = match_weight_round_to_nearest , ) recs = df_truth_space . as_record_dict () return roc_chart ( recs ) def precision_recall_chart_from_labels ( self , labels_tablename ): \"\"\"Generate a precision-recall chart from labelled (ground truth) data. The table of labels should be in the following format, and should be registered as a table with your database: |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score| |----------------|-----------|----------------|-----------|--------------------| |df_1 |1 |df_2 |2 |0.99 | |df_1 |1 |df_2 |3 |0.2 | Note that `source_dataset` and `unique_id` should correspond to the values specified in the settings dict, and the `input_table_aliases` passed to the `linker` object. For `dedupe_only` links, the `source_dataset` columns can be ommitted. Args: labels_tablename (str): Name of table containing labels in the database threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> # DuckDBLinker >>> labels = pd.read_csv(\"my_labels.csv\") >>> linker._con.register(\"labels\", labels) >>> linker.precision_recall_chart_from_labels(\"labels\") >>> >>> # SparkLinker >>> labels = spark.read.csv(\"my_labels.csv\", header=True) >>> labels.createDataFrame(\"labels\") >>> linker.precision_recall_chart_from_labels(\"labels\") Returns: SplinkDataFrame \"\"\" df_truth_space = roc_table ( self , labels_tablename ) recs = df_truth_space . as_record_dict () return precision_recall_chart ( recs ) def roc_table_from_labels ( self , labels_tablename , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ) -> SplinkDataFrame : \"\"\"Generate truth statistics (false positive etc.) for each threshold value of match_probability, suitable for plotting a ROC chart. The table of labels should be in the following format, and should be registered with your database: |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score| |----------------|-----------|----------------|-----------|--------------------| |df_1 |1 |df_2 |2 |0.99 | |df_1 |1 |df_2 |3 |0.2 | Note that `source_dataset` and `unique_id` should correspond to the values specified in the settings dict, and the `input_table_aliases` passed to the `linker` object. For `dedupe_only` links, the `source_dataset` columns can be ommitted. Args: labels_tablename (str): Name of table containing labels in the database threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> # DuckDBLinker >>> labels = pd.read_csv(\"my_labels.csv\") >>> linker._con.register(\"labels\", labels) >>> linker.roc_table_from_labels(\"labels\") >>> >>> # SparkLinker >>> labels = spark.read.csv(\"my_labels.csv\", header=True) >>> labels.createDataFrame(\"labels\") >>> linker.roc_table_from_labels(\"labels\") Returns: SplinkDataFrame \"\"\" return roc_table ( self , labels_tablename , threshold_actual = threshold_actual , match_weight_round_to_nearest = match_weight_round_to_nearest , ) def match_weight_histogram ( self , df_predict : SplinkDataFrame , target_bins : int = 30 , width = 600 , height = 250 ): \"\"\"Generate a histogram that shows the distribution of match weights in `df_predict` Args: df_predict (SplinkDataFrame): Output of `linker.predict()` target_bins (int, optional): Target number of bins in histogram. Defaults to 30. width (int, optional): Width of output. Defaults to 600. height (int, optional): Height of output chart. Defaults to 250. \"\"\" df = histogram_data ( self , df_predict , target_bins ) recs = df . as_record_dict () return match_weight_histogram ( recs , width = width , height = height ) def waterfall_chart ( self , records : List [ dict ], filter_nulls = True ): \"\"\"Visualise how the final match weight is computed for the provided pairwise record comparisons. Records must be provided as a list of dictionaries. This would usually be obtained from `df.as_record_dict(limit=n)` where `df` is a SplinkDataFrame. Examples: >>> df = linker.predict(threshold_match_weight=2) >>> records = df.as_record_dict(limit=10) >>> linker.waterfall_chart(records) Args: records (List[dict]): Usually be obtained from `df.as_record_dict(limit=n)` where `df` is a SplinkDataFrame. filter_nulls (bool, optional): Whether the visualiation shows null comparisons, which have no effect on final match weight. Defaults to True. \"\"\" self . _raise_error_if_necessary_waterfall_columns_not_computed () return waterfall_chart ( records , self . _settings_obj , filter_nulls ) def unlinkables_chart ( self , x_col = \"match_weight\" , source_dataset = None , as_dict = False , ): \"\"\"Generate an interactive chart displaying the proportion of records that are \"unlinkable\" for a given splink score threshold and model parameters. Unlinkable records are those that, even when compared with themselves, do not contain enough information to confirm a match. Args: x_col (str, optional): Column to use for the x-axis. Defaults to \"match_weight\". source_dataset (str, optional): Name of the source dataset to use for the title of the output chart. as_dict (bool, optional): If True, return a dict version of the chart. Examples: >>> # For the simplest code pipeline, load a pre-trained model >>> # and run this against the test data. >>> df = pd.read_csv(\"./tests/datasets/fake_1000_from_splink_demos.csv\") >>> linker = DuckDBLinker(df) >>> linker.load_settings_from_json(\"saved_settings.json\") >>> linker.unlinkables_chart() >>> >>> # For more complex code pipelines, you can run an entire pipeline >>> # that estimates your m and u values, before `unlinkables_chart(). \"\"\" # Link our initial df on itself and calculate the % of unlinkable entries records = unlinkables_data ( self , x_col ) return unlinkables_chart ( records , x_col , source_dataset ) def comparison_viewer_dashboard ( self , df_predict : SplinkDataFrame , out_path : str , overwrite = False , num_example_rows = 2 , ): \"\"\"Generate an interactive html visualization of the linker's predictions and save to `out_path`. For more information see [this video](https://www.youtube.com/watch?v=DNvCMqjipis) Args: df_predict (SplinkDataFrame): The outputs of `linker.predict()` out_path (str): The path (including filename) to save the html file to. overwrite (bool, optional): Overwrite the html file if it already exists? Defaults to False. num_example_rows (int, optional): Number of example rows per comparison vector. Defaults to 2. Examples: >>> df_predictions = linker.predict() >>> linker.comparison_viewer_dashboard(df_predictions, \"scv.html\", True, 2) >>> >>> # Optionally, in Jupyter, you can display the results inline >>> # Otherwise you can just load the html file in your browser >>> from IPython.display import IFrame >>> IFrame(src=\"./scv.html\", width=\"100%\", height=1200) \"\"\" self . _raise_error_if_necessary_waterfall_columns_not_computed () sql = comparison_vector_distribution_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vector_distribution\" ) sqls = comparison_viewer_table_sqls ( self , num_example_rows ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) df = self . _execute_sql_pipeline ([ df_predict ]) render_splink_comparison_viewer_html ( df . as_record_dict (), self . _settings_obj . _as_completed_dict (), out_path , overwrite , ) def parameter_estimate_comparisons_chart ( self , include_m = True , include_u = True ): \"\"\"Show a chart that shows how parameter estimates have differed across the different estimation methods you have used. For example, if you have run two EM estimation sessions, blocking on different variables, and both result in parameter estimates for first_name, this chart will enable easy comparison of the different estimates Args: include_m (bool, optional): Show different estimates of m values. Defaults to True. include_u (bool, optional): Show different estimates of u values. Defaults to True. \"\"\" records = self . _settings_obj . _parameter_estimates_as_records to_retain = [] if include_m : to_retain . append ( \"m\" ) if include_u : to_retain . append ( \"u\" ) records = [ r for r in records if r [ \"m_or_u\" ] in to_retain ] return parameter_estimate_comparisons ( records ) def missingness_chart ( self , input_dataset : str = None ): \"\"\"Generate a summary chart of the missingness (prevalence of nulls) of columns in the input datasets. By default, missingness is assessed across all input datasets Args: input_dataset (str, optional): Name of one of the input tables in the database. If provided, missingness will be computed for this table alone. Defaults to None. Examples: >>> linker.missingness_chart() \"\"\" records = missingness_data ( self , input_dataset ) return missingness_chart ( records , input_dataset ) def compute_number_of_comparisons_generated_by_blocking_rule ( self , blocking_rule : str , link_type : str = None , unique_id_column_name : str = None , ) -> int : \"\"\"Compute the number of pairwise record comparisons that would be generated by a blocking rule Args: blocking_rule (str): The blocking rule to analyse link_type (str, optional): The link type. This is needed only if the linker has not yet been provided with a settings dictionary. Defaults to None. unique_id_column_name (str, optional): This is needed only if the linker has not yet been provided with a settings dictionary. Defaults to None. Examples: >>> br = \"l.first_name = r.first_name\" >>> linker.compute_number_of_comparisons_generated_by_blocking_rule(br) 19387 >>> br = \"l.name = r.name and substr(l.dob,1,4) = substr(r.dob,1,4)\" >>> linker.compute_number_of_comparisons_generated_by_blocking_rule(br) 394 Returns: int: The number of comparisons generated by the blocking rule \"\"\" sql = vertically_concatenate_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_concat\" ) sql = number_of_comparisons_generated_by_blocking_rule_sql ( self , blocking_rule , link_type , unique_id_column_name ) self . _enqueue_sql ( sql , \"__splink__analyse_blocking_rule\" ) res = self . _execute_sql_pipeline () . as_record_dict ()[ 0 ] return res [ \"count_of_pairwise_comparisons_generated\" ] def match_weights_chart ( self ): \"\"\"Display a chart of the (partial) match weights of the linkage model Examples: >>> linker.match_weights_chart() >>> >>> # To view offline (if you don't have an internet connection): >>> >>> from splink.charts import save_offline_chart >>> c = linker.match_weights_chart() >>> save_offline_chart(c.spec, \"test_chart.html\") >>> >>> # View resultant html file in Jupyter (or just load it in your browser) >>> from IPython.display import IFrame >>> IFrame(src=\"./test_chart.html\", width=1000, height=500) \"\"\" return self . _settings_obj . match_weights_chart () def m_u_parameters_chart ( self ): \"\"\"Display a chart of the m and u parameters of the linkage model Examples: >>> linker.m_u_parameters_chart() >>> >>> # To view offline (if you don't have an internet connection): >>> >>> from splink.charts import save_offline_chart >>> c = linker.match_weights_chart() >>> save_offline_chart(c.spec, \"test_chart.html\") >>> >>> # View resultant html file in Jupyter (or just load it in your browser) >>> from IPython.display import IFrame >>> IFrame(src=\"./test_chart.html\", width=1000, height=500) \"\"\" return self . _settings_obj . m_u_parameters_chart () def cluster_studio_dashboard ( self , df_predict : SplinkDataFrame , df_clustered : SplinkDataFrame , out_path : str , sampling_method = \"random\" , sample_size : int = 10 , cluster_ids : list = None , cluster_names : list = None , overwrite : bool = False , ): \"\"\"Generate an interactive html visualization of the predicted cluster and save to `out_path`. Args: df_predict (SplinkDataFrame): The outputs of `linker.predict()` df_clustered (SplinkDataFrame): The outputs of `linker.cluster_pairwise_predictions_at_threshold()` out_path (str): The path (including filename) to save the html file to. sampling_method (str, optional): `random` or `by_cluster_size`. Defaults to `random`. sample_size (int, optional): Number of clusters to show in the dahboard. Defaults to 10. cluster_ids (list): The IDs of the clusters that will be displayed in the dashboard. If provided, ignore the `sampling_method` and `sample_size` arguments. Defaults to None. overwrite (bool, optional): Overwrite the html file if it already exists? Defaults to False. cluster_names (list, optional): If provided, the dashboard will display these names in the selection box. Ony works in conjunction with `cluster_ids`. Defaults to None. Examples: >>> df_p = linker.predict() >>> df_c = linker.cluster_pairwise_predictions_at_threshold(df_p, 0.5) >>> linker.cluster_studio_dashboard( >>> df_p, df_c, [0, 4, 7], \"cluster_studio.html\" >>> ) >>> >>> # Optionally, in Jupyter, you can display the results inline >>> # Otherwise you can just load the html file in your browser >>> from IPython.display import IFrame >>> IFrame(src=\"./cluster_studio.html\", width=\"100%\", height=1200) \"\"\" self . _raise_error_if_necessary_waterfall_columns_not_computed () return render_splink_cluster_studio_html ( self , df_predict , df_clustered , out_path , sampling_method = sampling_method , sample_size = sample_size , cluster_ids = cluster_ids , overwrite = overwrite , cluster_names = cluster_names , ) def save_settings_to_json ( self , out_path : str , overwrite = False ) -> dict : \"\"\"Save the configuration and parameters the linkage model to a json file. Returns the model as a Python dictionary. If an out_path is specified, also saves the settings to a file Args: out_path (str): File path for json file overwrite (bool, optional): Overwrite if already exists? Defaults to False. \"\"\" model_dict = self . _settings_obj . as_dict () if out_path : if os . path . isfile ( out_path ) and not overwrite : raise ValueError ( f \"The path { out_path } already exists. Please provide a different \" \"path or set overwrite=True\" ) with open ( out_path , \"w\" ) as f : json . dump ( model_dict , f , indent = 4 ) def load_settings_from_json ( self , in_path : str ): \"\"\" Load settings from a file. Args: in_path (str): Path to settings json file \"\"\" with open ( in_path , \"r\" ) as f : model_dict = json . load ( f ) self . initialise_settings ( model_dict )","title":"Documentation for Linker object"},{"location":"linker.html#splink.linker.Linker.cluster_pairwise_predictions_at_threshold","text":"Clusters the pairwise match predictions that result from linker.predict() into groups of connected record using the connected components graph clustering algorithm Records with an estimated match_probability above threshold_match_probability are considered to be a match (i.e. they represent the same entity). Parameters: Name Type Description Default df_predict SplinkDataFrame The results of linker.predict() required threshold_match_probability float Filter the pairwise match predictions to include only pairwise comparisons with a match_probability above this threshold. This dataframe is then fed into the clustering algorithm. required Returns: Name Type Description SplinkDataFrame SplinkDataFrame A SplinkDataFrame containing a list of all IDs, clustered into groups based on the desired match threshold. Source code in splink/linker.py 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 def cluster_pairwise_predictions_at_threshold ( self , df_predict : SplinkDataFrame , threshold_match_probability : float ) -> SplinkDataFrame : \"\"\"Clusters the pairwise match predictions that result from `linker.predict()` into groups of connected record using the connected components graph clustering algorithm Records with an estimated `match_probability` above `threshold_match_probability` are considered to be a match (i.e. they represent the same entity). Args: df_predict (SplinkDataFrame): The results of `linker.predict()` threshold_match_probability (float): Filter the pairwise match predictions to include only pairwise comparisons with a match_probability above this threshold. This dataframe is then fed into the clustering algorithm. Returns: SplinkDataFrame: A SplinkDataFrame containing a list of all IDs, clustered into groups based on the desired match threshold. \"\"\" self . _initialise_df_concat_with_tf ( df_predict ) edges_table = _cc_create_unique_id_cols ( self , df_predict , threshold_match_probability , ) cc = solve_connected_components ( self , edges_table ) return cc","title":"cluster_pairwise_predictions_at_threshold()"},{"location":"linker.html#splink.linker.Linker.cluster_studio_dashboard","text":"Generate an interactive html visualization of the predicted cluster and save to out_path . Parameters: Name Type Description Default df_predict SplinkDataFrame The outputs of linker.predict() required df_clustered SplinkDataFrame The outputs of linker.cluster_pairwise_predictions_at_threshold() required out_path str The path (including filename) to save the html file to. required sampling_method str random or by_cluster_size . Defaults to random . 'random' sample_size int Number of clusters to show in the dahboard. Defaults to 10. 10 cluster_ids list The IDs of the clusters that will be displayed in the dashboard. If provided, ignore the sampling_method and sample_size arguments. Defaults to None. None overwrite bool Overwrite the html file if it already exists? Defaults to False. False cluster_names list If provided, the dashboard will display these names in the selection box. Ony works in conjunction with cluster_ids . Defaults to None. None Examples: >>> df_p = linker . predict () >>> df_c = linker . cluster_pairwise_predictions_at_threshold ( df_p , 0.5 ) >>> linker . cluster_studio_dashboard ( >>> df_p , df_c , [ 0 , 4 , 7 ], \"cluster_studio.html\" >>> ) >>> >>> # Optionally, in Jupyter, you can display the results inline >>> # Otherwise you can just load the html file in your browser >>> from IPython.display import IFrame >>> IFrame ( src = \"./cluster_studio.html\" , width = \"100%\" , height = 1200 ) Source code in splink/linker.py 1640 1641 1642 1643 1644 1645 1646 1647 1648 1649 1650 1651 1652 1653 1654 1655 1656 1657 1658 1659 1660 1661 1662 1663 1664 1665 1666 1667 1668 1669 1670 1671 1672 1673 1674 1675 1676 1677 1678 1679 1680 1681 1682 1683 1684 1685 1686 1687 1688 1689 1690 1691 1692 1693 1694 1695 1696 1697 def cluster_studio_dashboard ( self , df_predict : SplinkDataFrame , df_clustered : SplinkDataFrame , out_path : str , sampling_method = \"random\" , sample_size : int = 10 , cluster_ids : list = None , cluster_names : list = None , overwrite : bool = False , ): \"\"\"Generate an interactive html visualization of the predicted cluster and save to `out_path`. Args: df_predict (SplinkDataFrame): The outputs of `linker.predict()` df_clustered (SplinkDataFrame): The outputs of `linker.cluster_pairwise_predictions_at_threshold()` out_path (str): The path (including filename) to save the html file to. sampling_method (str, optional): `random` or `by_cluster_size`. Defaults to `random`. sample_size (int, optional): Number of clusters to show in the dahboard. Defaults to 10. cluster_ids (list): The IDs of the clusters that will be displayed in the dashboard. If provided, ignore the `sampling_method` and `sample_size` arguments. Defaults to None. overwrite (bool, optional): Overwrite the html file if it already exists? Defaults to False. cluster_names (list, optional): If provided, the dashboard will display these names in the selection box. Ony works in conjunction with `cluster_ids`. Defaults to None. Examples: >>> df_p = linker.predict() >>> df_c = linker.cluster_pairwise_predictions_at_threshold(df_p, 0.5) >>> linker.cluster_studio_dashboard( >>> df_p, df_c, [0, 4, 7], \"cluster_studio.html\" >>> ) >>> >>> # Optionally, in Jupyter, you can display the results inline >>> # Otherwise you can just load the html file in your browser >>> from IPython.display import IFrame >>> IFrame(src=\"./cluster_studio.html\", width=\"100%\", height=1200) \"\"\" self . _raise_error_if_necessary_waterfall_columns_not_computed () return render_splink_cluster_studio_html ( self , df_predict , df_clustered , out_path , sampling_method = sampling_method , sample_size = sample_size , cluster_ids = cluster_ids , overwrite = overwrite , cluster_names = cluster_names , )","title":"cluster_studio_dashboard()"},{"location":"linker.html#splink.linker.Linker.compare_two_records","text":"Use the linkage model to compare and score a pairwise record comparison based on the two input records provided Parameters: Name Type Description Default record_1 dict dictionary representing the first record. Columns names and data types must be the same as the columns in the settings object required record_2 dict dictionary representing the second record. Columns names and data types must be the same as the columns in the settings object required Examples: >>> linker = DuckDBLinker ( df ) >>> linker . load_settings_from_json ( \"saved_settings.json\" ) >>> linker . compare_two_records ( record_left , record_right ) Returns: Name Type Description SplinkDataFrame Pairwise comparison with scored prediction Source code in splink/linker.py 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 def compare_two_records ( self , record_1 : dict , record_2 : dict ): \"\"\"Use the linkage model to compare and score a pairwise record comparison based on the two input records provided Args: record_1 (dict): dictionary representing the first record. Columns names and data types must be the same as the columns in the settings object record_2 (dict): dictionary representing the second record. Columns names and data types must be the same as the columns in the settings object Examples: >>> linker = DuckDBLinker(df) >>> linker.load_settings_from_json(\"saved_settings.json\") >>> linker.compare_two_records(record_left, record_right) Returns: SplinkDataFrame: Pairwise comparison with scored prediction \"\"\" original_blocking_rules = ( self . _settings_obj . _blocking_rules_to_generate_predictions ) original_link_type = self . _settings_obj . _link_type self . _compare_two_records_mode = True self . _settings_obj . _blocking_rules_to_generate_predictions = [] self . _records_to_table ([ record_1 ], \"__splink__compare_two_records_left\" ) self . _records_to_table ([ record_2 ], \"__splink__compare_two_records_right\" ) sql_join_tf = _join_tf_to_input_df_sql ( self ) sql_join_tf = sql_join_tf . replace ( \"__splink__df_concat\" , \"__splink__compare_two_records_left\" ) self . _enqueue_sql ( sql_join_tf , \"__splink__compare_two_records_left_with_tf\" ) sql_join_tf = sql_join_tf . replace ( \"__splink__compare_two_records_left\" , \"__splink__compare_two_records_right\" ) self . _enqueue_sql ( sql_join_tf , \"__splink__compare_two_records_right_with_tf\" ) sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) sql = compute_comparison_vector_values_sql ( self . _settings_obj ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vectors\" ) sqls = predict_from_comparison_vectors_sqls ( self . _settings_obj ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) predictions = self . _execute_sql_pipeline ( use_cache = False ) self . _settings_obj . _blocking_rules_to_generate_predictions = ( original_blocking_rules ) self . _settings_obj . _link_type = original_link_type self . _compare_two_records_mode = False return predictions","title":"compare_two_records()"},{"location":"linker.html#splink.linker.Linker.comparison_viewer_dashboard","text":"Generate an interactive html visualization of the linker's predictions and save to out_path . For more information see this video Parameters: Name Type Description Default df_predict SplinkDataFrame The outputs of linker.predict() required out_path str The path (including filename) to save the html file to. required overwrite bool Overwrite the html file if it already exists? Defaults to False. False num_example_rows int Number of example rows per comparison vector. Defaults to 2. 2 Examples: >>> df_predictions = linker . predict () >>> linker . comparison_viewer_dashboard ( df_predictions , \"scv.html\" , True , 2 ) >>> >>> # Optionally, in Jupyter, you can display the results inline >>> # Otherwise you can just load the html file in your browser >>> from IPython.display import IFrame >>> IFrame ( src = \"./scv.html\" , width = \"100%\" , height = 1200 ) Source code in splink/linker.py 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 def comparison_viewer_dashboard ( self , df_predict : SplinkDataFrame , out_path : str , overwrite = False , num_example_rows = 2 , ): \"\"\"Generate an interactive html visualization of the linker's predictions and save to `out_path`. For more information see [this video](https://www.youtube.com/watch?v=DNvCMqjipis) Args: df_predict (SplinkDataFrame): The outputs of `linker.predict()` out_path (str): The path (including filename) to save the html file to. overwrite (bool, optional): Overwrite the html file if it already exists? Defaults to False. num_example_rows (int, optional): Number of example rows per comparison vector. Defaults to 2. Examples: >>> df_predictions = linker.predict() >>> linker.comparison_viewer_dashboard(df_predictions, \"scv.html\", True, 2) >>> >>> # Optionally, in Jupyter, you can display the results inline >>> # Otherwise you can just load the html file in your browser >>> from IPython.display import IFrame >>> IFrame(src=\"./scv.html\", width=\"100%\", height=1200) \"\"\" self . _raise_error_if_necessary_waterfall_columns_not_computed () sql = comparison_vector_distribution_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vector_distribution\" ) sqls = comparison_viewer_table_sqls ( self , num_example_rows ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) df = self . _execute_sql_pipeline ([ df_predict ]) render_splink_comparison_viewer_html ( df . as_record_dict (), self . _settings_obj . _as_completed_dict (), out_path , overwrite , )","title":"comparison_viewer_dashboard()"},{"location":"linker.html#splink.linker.Linker.compute_number_of_comparisons_generated_by_blocking_rule","text":"Compute the number of pairwise record comparisons that would be generated by a blocking rule Parameters: Name Type Description Default blocking_rule str The blocking rule to analyse required link_type str The link type. This is needed only if the linker has not yet been provided with a settings dictionary. Defaults to None. None unique_id_column_name str This is needed only if the linker has not yet been provided with a settings dictionary. Defaults to None. None Examples: >>> br = \"l.first_name = r.first_name\" >>> linker . compute_number_of_comparisons_generated_by_blocking_rule ( br ) 19387 >>> br = \"l.name = r.name and substr(l.dob,1,4) = substr(r.dob,1,4)\" >>> linker . compute_number_of_comparisons_generated_by_blocking_rule ( br ) 394 Returns: Name Type Description int int The number of comparisons generated by the blocking rule Source code in splink/linker.py 1563 1564 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 1601 def compute_number_of_comparisons_generated_by_blocking_rule ( self , blocking_rule : str , link_type : str = None , unique_id_column_name : str = None , ) -> int : \"\"\"Compute the number of pairwise record comparisons that would be generated by a blocking rule Args: blocking_rule (str): The blocking rule to analyse link_type (str, optional): The link type. This is needed only if the linker has not yet been provided with a settings dictionary. Defaults to None. unique_id_column_name (str, optional): This is needed only if the linker has not yet been provided with a settings dictionary. Defaults to None. Examples: >>> br = \"l.first_name = r.first_name\" >>> linker.compute_number_of_comparisons_generated_by_blocking_rule(br) 19387 >>> br = \"l.name = r.name and substr(l.dob,1,4) = substr(r.dob,1,4)\" >>> linker.compute_number_of_comparisons_generated_by_blocking_rule(br) 394 Returns: int: The number of comparisons generated by the blocking rule \"\"\" sql = vertically_concatenate_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_concat\" ) sql = number_of_comparisons_generated_by_blocking_rule_sql ( self , blocking_rule , link_type , unique_id_column_name ) self . _enqueue_sql ( sql , \"__splink__analyse_blocking_rule\" ) res = self . _execute_sql_pipeline () . as_record_dict ()[ 0 ] return res [ \"count_of_pairwise_comparisons_generated\" ]","title":"compute_number_of_comparisons_generated_by_blocking_rule()"},{"location":"linker.html#splink.linker.Linker.compute_tf_table","text":"Compute a term frequency table for a given column and persist to the database This method is useful if you want to pre-compute term frequency tables e.g. so that real time linkage executes faster, or so that you can estimate various models without having to recompute term frequency tables each time Examples: >>> # Example 1: Real time linkage >>> linker = DuckDBLinker ( df , connection = \":memory:\" ) >>> linker . load_settings_from_json ( \"saved_settings.json\" ) >>> linker . compute_tf_table ( \"surname\" ) >>> linker . compare_two_records ( record_left , record_right ) >>> # Example 2: Pre-computed term frequency tables in Spark >>> linker = SparkLinker ( df ) >>> df_first_name_tf = linker . compute_tf_table ( \"first_name\" ) >>> df_first_name_tf . write . parquet ( \"folder/first_name_tf\" ) >>> >>> # On subsequent data linking job, read this table rather than recompute >>> df_first_name_tf = spark . read . parquet ( \"folder/first_name_tf\" ) >>> df_first_name_tf . createOrReplaceTempView ( \"__splink__df_tf_first_name\" ) Parameters: Name Type Description Default column_name str The column name in the input table required Returns: Name Type Description SplinkDataFrame SplinkDataFrame The resultant table as a splink data frame Source code in splink/linker.py 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 def compute_tf_table ( self , column_name : str ) -> SplinkDataFrame : \"\"\"Compute a term frequency table for a given column and persist to the database This method is useful if you want to pre-compute term frequency tables e.g. so that real time linkage executes faster, or so that you can estimate various models without having to recompute term frequency tables each time Examples: >>> # Example 1: Real time linkage >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> linker.load_settings_from_json(\"saved_settings.json\") >>> linker.compute_tf_table(\"surname\") >>> linker.compare_two_records(record_left, record_right) >>> # Example 2: Pre-computed term frequency tables in Spark >>> linker = SparkLinker(df) >>> df_first_name_tf = linker.compute_tf_table(\"first_name\") >>> df_first_name_tf.write.parquet(\"folder/first_name_tf\") >>> >>> # On subsequent data linking job, read this table rather than recompute >>> df_first_name_tf = spark.read.parquet(\"folder/first_name_tf\") >>> df_first_name_tf.createOrReplaceTempView(\"__splink__df_tf_first_name\") Args: column_name (str): The column name in the input table Returns: SplinkDataFrame: The resultant table as a splink data frame \"\"\" sql = vertically_concatenate_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_concat\" ) input_col = InputColumn ( column_name , tf_adjustments = True ) sql = term_frequencies_for_single_column_sql ( input_col ) self . _enqueue_sql ( sql , colname_to_tf_tablename ( input_col )) return self . _execute_sql_pipeline ( materialise_as_hash = False )","title":"compute_tf_table()"},{"location":"linker.html#splink.linker.Linker.delete_tables_created_by_splink_from_db","text":"Source code in splink/linker.py 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 def delete_tables_created_by_splink_from_db ( self , retain_term_frequency = True , retain_df_concat_with_tf = True ): tables_remaining = [] current_tables = self . _names_of_tables_created_by_splink for splink_df in current_tables : name = splink_df . templated_name # Only delete tables explicitly marked as having been created by splink if \"__splink__\" not in name : tables_remaining . append ( splink_df ) continue if name == \"__splink__df_concat_with_tf\" : if retain_df_concat_with_tf : tables_remaining . append ( splink_df ) else : self . _delete_table_from_database ( name ) elif name . startswith ( \"__splink__df_tf_\" ): if retain_term_frequency : tables_remaining . append ( splink_df ) else : self . _delete_table_from_database ( name ) else : self . _delete_table_from_database ( name ) self . _names_of_tables_created_by_splink = tables_remaining","title":"delete_tables_created_by_splink_from_db()"},{"location":"linker.html#splink.linker.Linker.deterministic_link","text":"Uses the blocking rules specified by blocking_rules_to_generate_predictions in the settings dictionary to generate pairwise record comparisons. This should be a list of blocking rules which are strict enough to generate only true links. Deterministic linkage, however, is likely to result in missed links (false negatives). Examples: >>> linker = DuckDBLinker ( df , connection = \":memory:\" ) >>> >>> settings = { >>> \"link_type\" : \"dedupe_only\" , >>> \"blocking_rules_to_generate_predictions\" : [ >>> \"l.first_name = r.first_name\" , >>> \"l.surname = r.surname\" , >>> ], >>> \"comparisons\" : [] >>> } >>> >>> from splink.duckdb.duckdb_linker import DuckDBLinker >>> >>> linker = DuckDBLinker ( df , settings , connection = \":memory:\" ) >>> df = linker . deterministic_link () Returns: Name Type Description SplinkDataFrame SplinkDataFrame A SplinkDataFrame of the pairwise comparisons. This represents a table materialised in the database. Methods on the SplinkDataFrame allow you to access the underlying data. Source code in splink/linker.py 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 def deterministic_link ( self ) -> SplinkDataFrame : \"\"\"Uses the blocking rules specified by `blocking_rules_to_generate_predictions` in the settings dictionary to generate pairwise record comparisons. This should be a list of blocking rules which are strict enough to generate only true links. Deterministic linkage, however, is likely to result in missed links (false negatives). Examples: >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> >>> settings = { >>> \"link_type\": \"dedupe_only\", >>> \"blocking_rules_to_generate_predictions\": [ >>> \"l.first_name = r.first_name\", >>> \"l.surname = r.surname\", >>> ], >>> \"comparisons\": [] >>> } >>> >>> from splink.duckdb.duckdb_linker import DuckDBLinker >>> >>> linker = DuckDBLinker(df, settings, connection=\":memory:\") >>> df = linker.deterministic_link() Returns: SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons. This represents a table materialised in the database. Methods on the SplinkDataFrame allow you to access the underlying data. \"\"\" self . _initialise_df_concat_with_tf () sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) return self . _execute_sql_pipeline ()","title":"deterministic_link()"},{"location":"linker.html#splink.linker.Linker.estimate_m_from_label_column","text":"Estimate the m parameters of the linkage model from a label (ground truth) column in the input dataframe(s). The m parameters represent the proportion of record comparisons that fall into each comparison level amongst truly matching records. The ground truth column is used to generate pairwise record comparisons which are then assumed to be matches. For example, if the entity being matched is persons, and your input dataset(s) contain social security number, this could be used to estimate the m values for the model. Note that this column does not need to be fully populated. A common case is where a unique identifier such as social security number is only partially populated. Parameters: Name Type Description Default label_colname str The name of the column containing the ground truth label in the input data. required Examples: >>> linker . estimate_m_from_label_column ( \"social_security_number\" ) Returns: Type Description Updates the estimated m parameters within the linker object and returns nothing. Source code in splink/linker.py 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 def estimate_m_from_label_column ( self , label_colname : str ): \"\"\"Estimate the m parameters of the linkage model from a label (ground truth) column in the input dataframe(s). The m parameters represent the proportion of record comparisons that fall into each comparison level amongst truly matching records. The ground truth column is used to generate pairwise record comparisons which are then assumed to be matches. For example, if the entity being matched is persons, and your input dataset(s) contain social security number, this could be used to estimate the m values for the model. Note that this column does not need to be fully populated. A common case is where a unique identifier such as social security number is only partially populated. Args: label_colname (str): The name of the column containing the ground truth label in the input data. Examples: >>> linker.estimate_m_from_label_column(\"social_security_number\") Returns: Updates the estimated m parameters within the linker object and returns nothing. \"\"\" self . _initialise_df_concat_with_tf ( materialise = True ) estimate_m_values_from_label_column ( self , self . _input_tables_dict , label_colname ) self . _populate_m_u_from_trained_values () self . _settings_obj . _columns_without_estimated_parameters_message ()","title":"estimate_m_from_label_column()"},{"location":"linker.html#splink.linker.Linker.estimate_parameters_using_expectation_maximisation","text":"Estimate the parameters of the linkage model using expectation maximisation. By default, the m probabilities are estimated, but not the u probabilities, because good estiamtes for the u probabilities can be obtained from linker.estimate_u_using_random_sampling() . You can change this by setting fix_u_probabilities to False. The blocking rule provided is used to generate pairwise record comparisons. Usually, this should be a blocking rule that results in a dataframe where matches are between about 1% and 99% of the comparisons. By default, m parameters are estimated for all comparisons except those which are included in the blocking rule. For example, if the blocking rule is l.first_name = r.first_name , then parameter esimates will be made for all comparison except those which use first_name in their sql_condition By default, the probability two random records match is estimated for the blocked data, and then the m and u parameters for the columns specified in the blocking rules are used to estiamte the global probability two random records match. To control which comparisons should have their parameter estimated, and the process of 'reversing out' the global probability two random records match, the user may specify comparisons_to_deactivate and comparison_levels_to_reverse_blocking_rule . This is useful, for example if you block on the dmetaphone of a column but match on the original column. Examples: >>> # Default behaviour >>> br_training = \"l.first_name = r.first_name and l.dob = r.dob\" >>> linker . estimate_parameters_using_expectation_maximisation ( br_training ) >>> # Specify which comparisons to deactivate >>> br_training = \"l.dmeta_first_name = r.dmeta_first_name\" >>> settings_obj = linker . _settings_obj >>> comp = settings_obj . _get_comparison_by_output_column_name ( \"first_name\" ) >>> dmeta_level = comp . _get_comparison_level_by_comparison_vector_value ( 1 ) >>> linker . estimate_parameters_using_expectation_maximisation ( >>> br_training , >>> comparisons_to_deactivate = [ \"first_name\" ], >>> comparison_levels_to_reverse_blocking_rule = [ dmeta_level ], >>> ) Parameters: Name Type Description Default blocking_rule str The blocking rule used to generate pairwise record comparisons. required comparisons_to_deactivate list By default, splink will analyse the blocking rule provided and estimate the m parameters for all comaprisons except those included in the blocking rule. If comparisons_to_deactivate are provided, spink will instead estimate m parameters for all comparison except those specified in the comparisons_to_deactivate list. This list can either contain the output_column_name of the Comparison as a string, or Comparison objects. Defaults to None. None comparison_levels_to_reverse_blocking_rule list By default, splink will analyse the blocking rule provided and adjust the global probability two random records match to account for the matches specified in the blocking rule. If provided, this argument will overrule this default behaviour. The user must provide a list of ComparisonLevel objects. Defaults to None. None fix_probability_two_random_records_match bool If True, do not update the probability two random records match after each iteration. Defaults to False. False fix_m_probabilities bool If True, do not update the m probabilities after each iteration. Defaults to False. False fix_u_probabilities bool If True, do not update the u probabilities after each iteration. Defaults to True. True Examples: >>> blocking_rule = \"l.first_name = r.first_name and l.dob = r.dob\" >>> linker . estimate_parameters_using_expectation_maximisation ( blocking_rule ) Returns: Name Type Description EMTrainingSession EMTrainingSession An object containing information about the training session such as how parameters changed during the iteration history Source code in splink/linker.py 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 def estimate_parameters_using_expectation_maximisation ( self , blocking_rule : str , comparisons_to_deactivate : List [ Union [ str , Comparison ]] = None , comparison_levels_to_reverse_blocking_rule : List [ ComparisonLevel ] = None , fix_probability_two_random_records_match : bool = False , fix_m_probabilities = False , fix_u_probabilities = True , ) -> EMTrainingSession : \"\"\"Estimate the parameters of the linkage model using expectation maximisation. By default, the m probabilities are estimated, but not the u probabilities, because good estiamtes for the u probabilities can be obtained from `linker.estimate_u_using_random_sampling()`. You can change this by setting `fix_u_probabilities` to False. The blocking rule provided is used to generate pairwise record comparisons. Usually, this should be a blocking rule that results in a dataframe where matches are between about 1% and 99% of the comparisons. By default, m parameters are estimated for all comparisons except those which are included in the blocking rule. For example, if the blocking rule is `l.first_name = r.first_name`, then parameter esimates will be made for all comparison except those which use `first_name` in their sql_condition By default, the probability two random records match is estimated for the blocked data, and then the m and u parameters for the columns specified in the blocking rules are used to estiamte the global probability two random records match. To control which comparisons should have their parameter estimated, and the process of 'reversing out' the global probability two random records match, the user may specify `comparisons_to_deactivate` and `comparison_levels_to_reverse_blocking_rule`. This is useful, for example if you block on the dmetaphone of a column but match on the original column. Examples: >>> # Default behaviour >>> br_training = \"l.first_name = r.first_name and l.dob = r.dob\" >>> linker.estimate_parameters_using_expectation_maximisation(br_training) >>> # Specify which comparisons to deactivate >>> br_training = \"l.dmeta_first_name = r.dmeta_first_name\" >>> settings_obj = linker._settings_obj >>> comp = settings_obj._get_comparison_by_output_column_name(\"first_name\") >>> dmeta_level = comp._get_comparison_level_by_comparison_vector_value(1) >>> linker.estimate_parameters_using_expectation_maximisation( >>> br_training, >>> comparisons_to_deactivate=[\"first_name\"], >>> comparison_levels_to_reverse_blocking_rule=[dmeta_level], >>> ) Args: blocking_rule (str): The blocking rule used to generate pairwise record comparisons. comparisons_to_deactivate (list, optional): By default, splink will analyse the blocking rule provided and estimate the m parameters for all comaprisons except those included in the blocking rule. If comparisons_to_deactivate are provided, spink will instead estimate m parameters for all comparison except those specified in the comparisons_to_deactivate list. This list can either contain the output_column_name of the Comparison as a string, or Comparison objects. Defaults to None. comparison_levels_to_reverse_blocking_rule (list, optional): By default, splink will analyse the blocking rule provided and adjust the global probability two random records match to account for the matches specified in the blocking rule. If provided, this argument will overrule this default behaviour. The user must provide a list of ComparisonLevel objects. Defaults to None. fix_probability_two_random_records_match (bool, optional): If True, do not update the probability two random records match after each iteration. Defaults to False. fix_m_probabilities (bool, optional): If True, do not update the m probabilities after each iteration. Defaults to False. fix_u_probabilities (bool, optional): If True, do not update the u probabilities after each iteration. Defaults to True. Examples: >>> blocking_rule = \"l.first_name = r.first_name and l.dob = r.dob\" >>> linker.estimate_parameters_using_expectation_maximisation(blocking_rule) Returns: EMTrainingSession: An object containing information about the training session such as how parameters changed during the iteration history \"\"\" self . _initialise_df_concat_with_tf ( materialise = True ) if comparisons_to_deactivate : # If user provided a string, convert to Comparison object comparisons_to_deactivate = [ self . _settings_obj . _get_comparison_by_output_column_name ( n ) if isinstance ( n , str ) else n for n in comparisons_to_deactivate ] if comparison_levels_to_reverse_blocking_rule is None : logger . warning ( \" \\n WARNING: \\n \" \"You have provided comparisons_to_deactivate but not \" \"comparison_levels_to_reverse_blocking_rule. \\n \" \"If comparisons_to_deactivate is provided, then \" \"you usually need to provide corresponding \" \"comparison_levels_to_reverse_blocking_rule. \" \"because each comparison to deactivate if effectively treated \" \"as an exact match.\" ) em_training_session = EMTrainingSession ( self , blocking_rule , fix_u_probabilities = fix_u_probabilities , fix_m_probabilities = fix_m_probabilities , fix_probability_two_random_records_match = fix_probability_two_random_records_match , # noqa 501 comparisons_to_deactivate = comparisons_to_deactivate , comparison_levels_to_reverse_blocking_rule = comparison_levels_to_reverse_blocking_rule , # noqa 501 ) em_training_session . _train () self . _populate_m_u_from_trained_values () self . _populate_probability_two_random_records_match_from_trained_values () self . _settings_obj . _columns_without_estimated_parameters_message () return em_training_session","title":"estimate_parameters_using_expectation_maximisation()"},{"location":"linker.html#splink.linker.Linker.estimate_u_using_random_sampling","text":"Estimate the u parameters of the linkage model using random sampling. The u parameters represent the proportion of record comparisons that fall into each comparison level amongst truly non-matching records. This procedure takes a sample of the data and generates the cartesian product of pairwise record comparisons amongst the sampled records. The validity of the u values rests on the assumption that the resultant pairwise comparisons are non-matches (or at least, they are very unlikely to be matches). For large datasets, this is typically true. Parameters: Name Type Description Default target_rows int The target number of pairwise record comparisons from required Examples: >>> linker . estimate_u_using_random_sampling ( 1e8 ) Returns: Type Description Updates the estimated u parameters within the linker object and returns nothing. Source code in splink/linker.py 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 def estimate_u_using_random_sampling ( self , target_rows : int ): \"\"\"Estimate the u parameters of the linkage model using random sampling. The u parameters represent the proportion of record comparisons that fall into each comparison level amongst truly non-matching records. This procedure takes a sample of the data and generates the cartesian product of pairwise record comparisons amongst the sampled records. The validity of the u values rests on the assumption that the resultant pairwise comparisons are non-matches (or at least, they are very unlikely to be matches). For large datasets, this is typically true. Args: target_rows (int): The target number of pairwise record comparisons from which to derive the u values. Larger will give more accurate estimates but lead to longer runtimes. In our experience at least 1e9 (one billion) gives best results but can take a long time to compute. 1e7 (ten million) is often adequate whilst testing different model specifications, before the final model is estimated. Examples: >>> linker.estimate_u_using_random_sampling(1e8) Returns: Updates the estimated u parameters within the linker object and returns nothing. \"\"\" self . _initialise_df_concat_with_tf ( materialise = True ) estimate_u_values ( self , target_rows ) self . _populate_m_u_from_trained_values () self . _settings_obj . _columns_without_estimated_parameters_message ()","title":"estimate_u_using_random_sampling()"},{"location":"linker.html#splink.linker.Linker.find_matches_to_new_records","text":"Given one or more records, find records in the input dataset(s) which match and return in order of the splink prediction score. This effectively provides a way of searching the input datasets for given record(s) Parameters: Name Type Description Default records List [ dict ] Input search record(s). required blocking_rules str Blocking rules to select which records to find and score. If None, do not use a blocking rule - meaning the input records will be compared to all records provided to the linker when it was instantiated. Defaults to None. None match_weight_threshold int Return matches with a match weight above this threshold. Defaults to -4. -4 Examples: >>> linker = DuckDBLinker ( df ) >>> linker . load_settings_from_json ( \"saved_settings.json\" ) >>> # Pre-compute tf tables for any tables with >>> # term frequency adjustments >>> linker . compute_tf_table ( \"first_name\" ) >>> record = { 'unique_id' : 1 , >>> 'first_name' : \"John\" , >>> 'surname' : \"Smith\" , >>> 'dob' : \"1971-05-24\" , >>> 'city' : \"London\" , >>> 'email' : \"john@smith.net\" >>> } >>> df = linker . find_matches_to_new_records ([ record ], blocking_rules = []) Returns: Name Type Description SplinkDataFrame SplinkDataFrame The pairwise comparisons. Source code in splink/linker.py 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 def find_matches_to_new_records ( self , records : List [ dict ], blocking_rules = None , match_weight_threshold =- 4 ) -> SplinkDataFrame : \"\"\"Given one or more records, find records in the input dataset(s) which match and return in order of the splink prediction score. This effectively provides a way of searching the input datasets for given record(s) Args: records (List[dict]): Input search record(s). blocking_rules (str, optional): Blocking rules to select which records to find and score. If None, do not use a blocking rule - meaning the input records will be compared to all records provided to the linker when it was instantiated. Defaults to None. match_weight_threshold (int, optional): Return matches with a match weight above this threshold. Defaults to -4. Examples: >>> linker = DuckDBLinker(df) >>> linker.load_settings_from_json(\"saved_settings.json\") >>> # Pre-compute tf tables for any tables with >>> # term frequency adjustments >>> linker.compute_tf_table(\"first_name\") >>> record = {'unique_id': 1, >>> 'first_name': \"John\", >>> 'surname': \"Smith\", >>> 'dob': \"1971-05-24\", >>> 'city': \"London\", >>> 'email': \"john@smith.net\" >>> } >>> df = linker.find_matches_to_new_records([record], blocking_rules=[]) Returns: SplinkDataFrame: The pairwise comparisons. \"\"\" original_blocking_rules = ( self . _settings_obj . _blocking_rules_to_generate_predictions ) original_link_type = self . _settings_obj . _link_type self . _records_to_table ( records , \"__splink__df_new_records\" ) if blocking_rules is not None : self . _settings_obj . _blocking_rules_to_generate_predictions = blocking_rules self . _settings_obj . _link_type = \"link_only_find_matches_to_new_records\" self . _find_new_matches_mode = True sql = _join_tf_to_input_df_sql ( self ) sql = sql . replace ( \"__splink__df_concat\" , \"__splink__df_new_records\" ) self . _enqueue_sql ( sql , \"__splink__df_new_records_with_tf\" ) sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) sql = compute_comparison_vector_values_sql ( self . _settings_obj ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vectors\" ) sqls = predict_from_comparison_vectors_sqls ( self . _settings_obj ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) sql = f \"\"\" select * from __splink__df_predict where match_weight > { match_weight_threshold } \"\"\" self . _enqueue_sql ( sql , \"__splink_find_matches_predictions\" ) predictions = self . _execute_sql_pipeline ( use_cache = False ) self . _settings_obj . _blocking_rules_to_generate_predictions = ( original_blocking_rules ) self . _settings_obj . _link_type = original_link_type self . _find_new_matches_mode = False return predictions","title":"find_matches_to_new_records()"},{"location":"linker.html#splink.linker.Linker.initialise_settings","text":"Initialise settings for the linker. To be used if settings were not passed to the linker on creation. Examples: >>> linker = DuckDBLinker ( df , connection = \":memory:\" ) >>> linker . profile_columns ( \"first_name\" , \"surname\" ) >>> linker . initialise_settings ( settings_dict ) Parameters: Name Type Description Default settings_dict dict A Splink settings dictionary required Source code in splink/linker.py 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 def initialise_settings ( self , settings_dict : dict ): \"\"\"Initialise settings for the linker. To be used if settings were not passed to the linker on creation. Examples: >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> linker.profile_columns(\"first_name\", \"surname\") >>> linker.initialise_settings(settings_dict) Args: settings_dict (dict): A Splink settings dictionary \"\"\" self . _settings_dict = settings_dict self . _settings_obj_ = Settings ( settings_dict ) self . _validate_input_dfs ()","title":"initialise_settings()"},{"location":"linker.html#splink.linker.Linker.m_u_parameters_chart","text":"Display a chart of the m and u parameters of the linkage model Examples: >>> linker . m_u_parameters_chart () >>> >>> # To view offline (if you don't have an internet connection): >>> >>> from splink.charts import save_offline_chart >>> c = linker . match_weights_chart () >>> save_offline_chart ( c . spec , \"test_chart.html\" ) >>> >>> # View resultant html file in Jupyter (or just load it in your browser) >>> from IPython.display import IFrame >>> IFrame ( src = \"./test_chart.html\" , width = 1000 , height = 500 ) Source code in splink/linker.py 1621 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631 1632 1633 1634 1635 1636 1637 1638 def m_u_parameters_chart ( self ): \"\"\"Display a chart of the m and u parameters of the linkage model Examples: >>> linker.m_u_parameters_chart() >>> >>> # To view offline (if you don't have an internet connection): >>> >>> from splink.charts import save_offline_chart >>> c = linker.match_weights_chart() >>> save_offline_chart(c.spec, \"test_chart.html\") >>> >>> # View resultant html file in Jupyter (or just load it in your browser) >>> from IPython.display import IFrame >>> IFrame(src=\"./test_chart.html\", width=1000, height=500) \"\"\" return self . _settings_obj . m_u_parameters_chart ()","title":"m_u_parameters_chart()"},{"location":"linker.html#splink.linker.Linker.match_weight_histogram","text":"Generate a histogram that shows the distribution of match weights in df_predict Parameters: Name Type Description Default df_predict SplinkDataFrame Output of linker.predict() required target_bins int Target number of bins in histogram. Defaults to 30. 30 width int Width of output. Defaults to 600. 600 height int Height of output chart. Defaults to 250. 250 Source code in splink/linker.py 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 def match_weight_histogram ( self , df_predict : SplinkDataFrame , target_bins : int = 30 , width = 600 , height = 250 ): \"\"\"Generate a histogram that shows the distribution of match weights in `df_predict` Args: df_predict (SplinkDataFrame): Output of `linker.predict()` target_bins (int, optional): Target number of bins in histogram. Defaults to 30. width (int, optional): Width of output. Defaults to 600. height (int, optional): Height of output chart. Defaults to 250. \"\"\" df = histogram_data ( self , df_predict , target_bins ) recs = df . as_record_dict () return match_weight_histogram ( recs , width = width , height = height )","title":"match_weight_histogram()"},{"location":"linker.html#splink.linker.Linker.match_weights_chart","text":"Display a chart of the (partial) match weights of the linkage model Examples: >>> linker . match_weights_chart () >>> >>> # To view offline (if you don't have an internet connection): >>> >>> from splink.charts import save_offline_chart >>> c = linker . match_weights_chart () >>> save_offline_chart ( c . spec , \"test_chart.html\" ) >>> >>> # View resultant html file in Jupyter (or just load it in your browser) >>> from IPython.display import IFrame >>> IFrame ( src = \"./test_chart.html\" , width = 1000 , height = 500 ) Source code in splink/linker.py 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 1617 1618 1619 def match_weights_chart ( self ): \"\"\"Display a chart of the (partial) match weights of the linkage model Examples: >>> linker.match_weights_chart() >>> >>> # To view offline (if you don't have an internet connection): >>> >>> from splink.charts import save_offline_chart >>> c = linker.match_weights_chart() >>> save_offline_chart(c.spec, \"test_chart.html\") >>> >>> # View resultant html file in Jupyter (or just load it in your browser) >>> from IPython.display import IFrame >>> IFrame(src=\"./test_chart.html\", width=1000, height=500) \"\"\" return self . _settings_obj . match_weights_chart ()","title":"match_weights_chart()"},{"location":"linker.html#splink.linker.Linker.missingness_chart","text":"Generate a summary chart of the missingness (prevalence of nulls) of columns in the input datasets. By default, missingness is assessed across all input datasets Parameters: Name Type Description Default input_dataset str Name of one of the input tables in the None Examples: >>> linker . missingness_chart () Source code in splink/linker.py 1546 1547 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 def missingness_chart ( self , input_dataset : str = None ): \"\"\"Generate a summary chart of the missingness (prevalence of nulls) of columns in the input datasets. By default, missingness is assessed across all input datasets Args: input_dataset (str, optional): Name of one of the input tables in the database. If provided, missingness will be computed for this table alone. Defaults to None. Examples: >>> linker.missingness_chart() \"\"\" records = missingness_data ( self , input_dataset ) return missingness_chart ( records , input_dataset )","title":"missingness_chart()"},{"location":"linker.html#splink.linker.Linker.parameter_estimate_comparisons_chart","text":"Show a chart that shows how parameter estimates have differed across the different estimation methods you have used. For example, if you have run two EM estimation sessions, blocking on different variables, and both result in parameter estimates for first_name, this chart will enable easy comparison of the different estimates Parameters: Name Type Description Default include_m bool Show different estimates of m values. Defaults to True. True include_u bool Show different estimates of u values. Defaults to True. True Source code in splink/linker.py 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 def parameter_estimate_comparisons_chart ( self , include_m = True , include_u = True ): \"\"\"Show a chart that shows how parameter estimates have differed across the different estimation methods you have used. For example, if you have run two EM estimation sessions, blocking on different variables, and both result in parameter estimates for first_name, this chart will enable easy comparison of the different estimates Args: include_m (bool, optional): Show different estimates of m values. Defaults to True. include_u (bool, optional): Show different estimates of u values. Defaults to True. \"\"\" records = self . _settings_obj . _parameter_estimates_as_records to_retain = [] if include_m : to_retain . append ( \"m\" ) if include_u : to_retain . append ( \"u\" ) records = [ r for r in records if r [ \"m_or_u\" ] in to_retain ] return parameter_estimate_comparisons ( records )","title":"parameter_estimate_comparisons_chart()"},{"location":"linker.html#splink.linker.Linker.precision_recall_chart_from_labels","text":"Generate a precision-recall chart from labelled (ground truth) data. The table of labels should be in the following format, and should be registered as a table with your database: source_dataset_l unique_id_l source_dataset_r unique_id_r clerical_match_score df_1 1 df_2 2 0.99 df_1 1 df_2 3 0.2 Note that source_dataset and unique_id should correspond to the values specified in the settings dict, and the input_table_aliases passed to the linker object. For dedupe_only links, the source_dataset columns can be ommitted. Parameters: Name Type Description Default labels_tablename str Name of table containing labels in the database required threshold_actual float Where the clerical_match_score provided by the user is a probability rather than binary, this value is used as the threshold to classify clerical_match_score s as binary matches or non matches. Defaults to 0.5. required match_weight_round_to_nearest float When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. required Examples: >>> # DuckDBLinker >>> labels = pd . read_csv ( \"my_labels.csv\" ) >>> linker . _con . register ( \"labels\" , labels ) >>> linker . precision_recall_chart_from_labels ( \"labels\" ) >>> >>> # SparkLinker >>> labels = spark . read . csv ( \"my_labels.csv\" , header = True ) >>> labels . createDataFrame ( \"labels\" ) >>> linker . precision_recall_chart_from_labels ( \"labels\" ) Returns: Type Description SplinkDataFrame Source code in splink/linker.py 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 def precision_recall_chart_from_labels ( self , labels_tablename ): \"\"\"Generate a precision-recall chart from labelled (ground truth) data. The table of labels should be in the following format, and should be registered as a table with your database: |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score| |----------------|-----------|----------------|-----------|--------------------| |df_1 |1 |df_2 |2 |0.99 | |df_1 |1 |df_2 |3 |0.2 | Note that `source_dataset` and `unique_id` should correspond to the values specified in the settings dict, and the `input_table_aliases` passed to the `linker` object. For `dedupe_only` links, the `source_dataset` columns can be ommitted. Args: labels_tablename (str): Name of table containing labels in the database threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> # DuckDBLinker >>> labels = pd.read_csv(\"my_labels.csv\") >>> linker._con.register(\"labels\", labels) >>> linker.precision_recall_chart_from_labels(\"labels\") >>> >>> # SparkLinker >>> labels = spark.read.csv(\"my_labels.csv\", header=True) >>> labels.createDataFrame(\"labels\") >>> linker.precision_recall_chart_from_labels(\"labels\") Returns: SplinkDataFrame \"\"\" df_truth_space = roc_table ( self , labels_tablename ) recs = df_truth_space . as_record_dict () return precision_recall_chart ( recs )","title":"precision_recall_chart_from_labels()"},{"location":"linker.html#splink.linker.Linker.predict","text":"Create a dataframe of scored pairwise comparisons using the parameters of the linkage model. Uses the blocking rules specified in the blocking_rules_to_generate_predictions of the settings dictionary to generate the pairwise comparisons. Parameters: Name Type Description Default threshold_match_probability float If specified, filter the results to include only pairwise comparisons with a match_probability above this threshold. Defaults to None. None threshold_match_weight float If specified, filter the results to include only pairwise comparisons with a match_weight above this threshold. Defaults to None. None Examples: >>> linker = DuckDBLinker ( df , connection = \":memory:\" ) >>> linker . load_settings_from_json ( \"saved_settings.json\" ) >>> df = linker . predict ( threshold_match_probability = 0.95 ) >>> df . as_pandas_dataframe ( limit = 5 ) Returns: Name Type Description SplinkDataFrame SplinkDataFrame A SplinkDataFrame of the pairwise comparisons. This represents a table materialised in the database. Methods on the SplinkDataFrame allow you to access the underlying data. Source code in splink/linker.py 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 def predict ( self , threshold_match_probability : float = None , threshold_match_weight : float = None , ) -> SplinkDataFrame : \"\"\"Create a dataframe of scored pairwise comparisons using the parameters of the linkage model. Uses the blocking rules specified in the `blocking_rules_to_generate_predictions` of the settings dictionary to generate the pairwise comparisons. Args: threshold_match_probability (float, optional): If specified, filter the results to include only pairwise comparisons with a match_probability above this threshold. Defaults to None. threshold_match_weight (float, optional): If specified, filter the results to include only pairwise comparisons with a match_weight above this threshold. Defaults to None. Examples: >>> linker = DuckDBLinker(df, connection=\":memory:\") >>> linker.load_settings_from_json(\"saved_settings.json\") >>> df = linker.predict(threshold_match_probability=0.95) >>> df.as_pandas_dataframe(limit=5) Returns: SplinkDataFrame: A SplinkDataFrame of the pairwise comparisons. This represents a table materialised in the database. Methods on the SplinkDataFrame allow you to access the underlying data. \"\"\" # If the user only calls predict, it runs as a single pipeline with no # materialisation of anything self . _initialise_df_concat_with_tf ( materialise = False ) sql = block_using_rules_sql ( self ) self . _enqueue_sql ( sql , \"__splink__df_blocked\" ) sql = compute_comparison_vector_values_sql ( self . _settings_obj ) self . _enqueue_sql ( sql , \"__splink__df_comparison_vectors\" ) sqls = predict_from_comparison_vectors_sqls ( self . _settings_obj , threshold_match_probability , threshold_match_weight ) for sql in sqls : self . _enqueue_sql ( sql [ \"sql\" ], sql [ \"output_table_name\" ]) predictions = self . _execute_sql_pipeline () self . _predict_warning () return predictions","title":"predict()"},{"location":"linker.html#splink.linker.Linker.profile_columns","text":"Source code in splink/linker.py 1226 1227 1228 1229 1230 def profile_columns ( self , column_expressions : Union [ str , List [ str ]], top_n = 10 , bottom_n = 10 ): return profile_columns ( self , column_expressions , top_n = top_n , bottom_n = bottom_n )","title":"profile_columns()"},{"location":"linker.html#splink.linker.Linker.roc_chart_from_labels","text":"Generate a ROC chart from labelled (ground truth) data. The table of labels should be in the following format, and should be registered with your database: source_dataset_l unique_id_l source_dataset_r unique_id_r clerical_match_score df_1 1 df_2 2 0.99 df_1 1 df_2 3 0.2 Note that source_dataset and unique_id should correspond to the values specified in the settings dict, and the input_table_aliases passed to the linker object. For dedupe_only links, the source_dataset columns can be ommitted. Parameters: Name Type Description Default labels_tablename str Name of table containing labels in the database required threshold_actual float Where the clerical_match_score provided by the user is a probability rather than binary, this value is used as the threshold to classify clerical_match_score s as binary matches or non matches. Defaults to 0.5. 0.5 match_weight_round_to_nearest float When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. None Examples: >>> # DuckDBLinker >>> labels = pd . read_csv ( \"my_labels.csv\" ) >>> linker . _con . register ( \"labels\" , labels ) >>> linker . roc_chart_from_labels ( \"labels\" ) >>> >>> # SparkLinker >>> labels = spark . read . csv ( \"my_labels.csv\" , header = True ) >>> labels . createDataFrame ( \"labels\" ) >>> linker . roc_chart_from_labels ( \"labels\" ) Returns: Type Description SplinkDataFrame Source code in splink/linker.py 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 def roc_chart_from_labels ( self , labels_tablename , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ): \"\"\"Generate a ROC chart from labelled (ground truth) data. The table of labels should be in the following format, and should be registered with your database: |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score| |----------------|-----------|----------------|-----------|--------------------| |df_1 |1 |df_2 |2 |0.99 | |df_1 |1 |df_2 |3 |0.2 | Note that `source_dataset` and `unique_id` should correspond to the values specified in the settings dict, and the `input_table_aliases` passed to the `linker` object. For `dedupe_only` links, the `source_dataset` columns can be ommitted. Args: labels_tablename (str): Name of table containing labels in the database threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> # DuckDBLinker >>> labels = pd.read_csv(\"my_labels.csv\") >>> linker._con.register(\"labels\", labels) >>> linker.roc_chart_from_labels(\"labels\") >>> >>> # SparkLinker >>> labels = spark.read.csv(\"my_labels.csv\", header=True) >>> labels.createDataFrame(\"labels\") >>> linker.roc_chart_from_labels(\"labels\") Returns: SplinkDataFrame \"\"\" df_truth_space = roc_table ( self , labels_tablename , threshold_actual = threshold_actual , match_weight_round_to_nearest = match_weight_round_to_nearest , ) recs = df_truth_space . as_record_dict () return roc_chart ( recs )","title":"roc_chart_from_labels()"},{"location":"linker.html#splink.linker.Linker.roc_table_from_labels","text":"Generate truth statistics (false positive etc.) for each threshold value of match_probability, suitable for plotting a ROC chart. The table of labels should be in the following format, and should be registered with your database: source_dataset_l unique_id_l source_dataset_r unique_id_r clerical_match_score df_1 1 df_2 2 0.99 df_1 1 df_2 3 0.2 Note that source_dataset and unique_id should correspond to the values specified in the settings dict, and the input_table_aliases passed to the linker object. For dedupe_only links, the source_dataset columns can be ommitted. Parameters: Name Type Description Default labels_tablename str Name of table containing labels in the database required threshold_actual float Where the clerical_match_score provided by the user is a probability rather than binary, this value is used as the threshold to classify clerical_match_score s as binary matches or non matches. Defaults to 0.5. 0.5 match_weight_round_to_nearest float When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. None Examples: >>> # DuckDBLinker >>> labels = pd . read_csv ( \"my_labels.csv\" ) >>> linker . _con . register ( \"labels\" , labels ) >>> linker . roc_table_from_labels ( \"labels\" ) >>> >>> # SparkLinker >>> labels = spark . read . csv ( \"my_labels.csv\" , header = True ) >>> labels . createDataFrame ( \"labels\" ) >>> linker . roc_table_from_labels ( \"labels\" ) Returns: Type Description SplinkDataFrame SplinkDataFrame Source code in splink/linker.py 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 def roc_table_from_labels ( self , labels_tablename , threshold_actual = 0.5 , match_weight_round_to_nearest : float = None , ) -> SplinkDataFrame : \"\"\"Generate truth statistics (false positive etc.) for each threshold value of match_probability, suitable for plotting a ROC chart. The table of labels should be in the following format, and should be registered with your database: |source_dataset_l|unique_id_l|source_dataset_r|unique_id_r|clerical_match_score| |----------------|-----------|----------------|-----------|--------------------| |df_1 |1 |df_2 |2 |0.99 | |df_1 |1 |df_2 |3 |0.2 | Note that `source_dataset` and `unique_id` should correspond to the values specified in the settings dict, and the `input_table_aliases` passed to the `linker` object. For `dedupe_only` links, the `source_dataset` columns can be ommitted. Args: labels_tablename (str): Name of table containing labels in the database threshold_actual (float, optional): Where the `clerical_match_score` provided by the user is a probability rather than binary, this value is used as the threshold to classify `clerical_match_score`s as binary matches or non matches. Defaults to 0.5. match_weight_round_to_nearest (float, optional): When provided, thresholds are rounded. When large numbers of labels are provided, this is sometimes necessary to reduce the size of the ROC table, and therefore the number of points plotted on the ROC chart. Defaults to None. Examples: >>> # DuckDBLinker >>> labels = pd.read_csv(\"my_labels.csv\") >>> linker._con.register(\"labels\", labels) >>> linker.roc_table_from_labels(\"labels\") >>> >>> # SparkLinker >>> labels = spark.read.csv(\"my_labels.csv\", header=True) >>> labels.createDataFrame(\"labels\") >>> linker.roc_table_from_labels(\"labels\") Returns: SplinkDataFrame \"\"\" return roc_table ( self , labels_tablename , threshold_actual = threshold_actual , match_weight_round_to_nearest = match_weight_round_to_nearest , )","title":"roc_table_from_labels()"},{"location":"linker.html#splink.linker.Linker.train_m_from_pairwise_labels","text":"Source code in splink/linker.py 1232 1233 1234 def train_m_from_pairwise_labels ( self , table_name ): self . _initialise_df_concat_with_tf ( materialise = True ) estimate_m_from_pairwise_labels ( self , table_name )","title":"train_m_from_pairwise_labels()"},{"location":"linker.html#splink.linker.Linker.unlinkables_chart","text":"Generate an interactive chart displaying the proportion of records that are \"unlinkable\" for a given splink score threshold and model parameters. Unlinkable records are those that, even when compared with themselves, do not contain enough information to confirm a match. Parameters: Name Type Description Default x_col str Column to use for the x-axis. Defaults to \"match_weight\". 'match_weight' source_dataset str Name of the source dataset to use for the title of the output chart. None as_dict bool If True, return a dict version of the chart. False Examples: >>> # For the simplest code pipeline, load a pre-trained model >>> # and run this against the test data. >>> df = pd . read_csv ( \"./tests/datasets/fake_1000_from_splink_demos.csv\" ) >>> linker = DuckDBLinker ( df ) >>> linker . load_settings_from_json ( \"saved_settings.json\" ) >>> linker . unlinkables_chart () >>> >>> # For more complex code pipelines, you can run an entire pipeline >>> # that estimates your m and u values, before `unlinkables_chart(). Source code in splink/linker.py 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 def unlinkables_chart ( self , x_col = \"match_weight\" , source_dataset = None , as_dict = False , ): \"\"\"Generate an interactive chart displaying the proportion of records that are \"unlinkable\" for a given splink score threshold and model parameters. Unlinkable records are those that, even when compared with themselves, do not contain enough information to confirm a match. Args: x_col (str, optional): Column to use for the x-axis. Defaults to \"match_weight\". source_dataset (str, optional): Name of the source dataset to use for the title of the output chart. as_dict (bool, optional): If True, return a dict version of the chart. Examples: >>> # For the simplest code pipeline, load a pre-trained model >>> # and run this against the test data. >>> df = pd.read_csv(\"./tests/datasets/fake_1000_from_splink_demos.csv\") >>> linker = DuckDBLinker(df) >>> linker.load_settings_from_json(\"saved_settings.json\") >>> linker.unlinkables_chart() >>> >>> # For more complex code pipelines, you can run an entire pipeline >>> # that estimates your m and u values, before `unlinkables_chart(). \"\"\" # Link our initial df on itself and calculate the % of unlinkable entries records = unlinkables_data ( self , x_col ) return unlinkables_chart ( records , x_col , source_dataset )","title":"unlinkables_chart()"},{"location":"linker.html#splink.linker.Linker.waterfall_chart","text":"Visualise how the final match weight is computed for the provided pairwise record comparisons. Records must be provided as a list of dictionaries. This would usually be obtained from df.as_record_dict(limit=n) where df is a SplinkDataFrame. Examples: >>> df = linker . predict ( threshold_match_weight = 2 ) >>> records = df . as_record_dict ( limit = 10 ) >>> linker . waterfall_chart ( records ) Parameters: Name Type Description Default records List [ dict ] Usually be obtained from df.as_record_dict(limit=n) where df is a SplinkDataFrame. required filter_nulls bool Whether the visualiation shows null comparisons, which have no effect on final match weight. Defaults to True. True Source code in splink/linker.py 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 def waterfall_chart ( self , records : List [ dict ], filter_nulls = True ): \"\"\"Visualise how the final match weight is computed for the provided pairwise record comparisons. Records must be provided as a list of dictionaries. This would usually be obtained from `df.as_record_dict(limit=n)` where `df` is a SplinkDataFrame. Examples: >>> df = linker.predict(threshold_match_weight=2) >>> records = df.as_record_dict(limit=10) >>> linker.waterfall_chart(records) Args: records (List[dict]): Usually be obtained from `df.as_record_dict(limit=n)` where `df` is a SplinkDataFrame. filter_nulls (bool, optional): Whether the visualiation shows null comparisons, which have no effect on final match weight. Defaults to True. \"\"\" self . _raise_error_if_necessary_waterfall_columns_not_computed () return waterfall_chart ( records , self . _settings_obj , filter_nulls )","title":"waterfall_chart()"},{"location":"userguide.html","text":"Splink User Guide \u00b6 Background \u00b6 Record linkage is the process of using statistical and computational tools used to identify related records, remove duplicated entries, and aggregate information. This process has also been have been referred to as entity resolution , record linkage , data matching , instance matching , data linkage , data cleaning , data fusion and/or data merging . The matching status of a candidate record pair is calculated through either deterministic linking method or probabilistic ones. Candidate record pairs are compared using a set of comparison functions called comparators that allow for approximate (not exact) similarities. However in order to make this process computationally tractable usually the process of blocking is used where in order to reduce the number of record pairs that need to be compared, only the most relevant pairs are processed. SQL backends \u00b6 graph LR A[Start] --> B{backend?}; B -->|spark| C[pyspark backend]; B --> |sqlite| D[sqite3 backend]; B ---->|duckdb| E[duckdb backend];","title":"User Guide"},{"location":"userguide.html#splink-user-guide","text":"","title":"Splink User Guide"},{"location":"userguide.html#background","text":"Record linkage is the process of using statistical and computational tools used to identify related records, remove duplicated entries, and aggregate information. This process has also been have been referred to as entity resolution , record linkage , data matching , instance matching , data linkage , data cleaning , data fusion and/or data merging . The matching status of a candidate record pair is calculated through either deterministic linking method or probabilistic ones. Candidate record pairs are compared using a set of comparison functions called comparators that allow for approximate (not exact) similarities. However in order to make this process computationally tractable usually the process of blocking is used where in order to reduce the number of record pairs that need to be compared, only the most relevant pairs are processed.","title":"Background"},{"location":"userguide.html#sql-backends","text":"graph LR A[Start] --> B{backend?}; B -->|spark| C[pyspark backend]; B --> |sqlite| D[sqite3 backend]; B ---->|duckdb| E[duckdb backend];","title":"SQL backends"},{"location":"demos/index.html","text":"splink_demos \u00b6 This repo contains interactive notebooks containing demonstration and tutorial for version 3 of the Splink record linking library, the homepage for which is here . Running these notebooks interactively \u00b6 You can run these notebooks in an interactive Jupyter notebook by clicking the button below: Running these notebooks locally in VSCode \u00b6 If you don't already have it, you'll need to install java on your system in order to run pyspark , which splink currently depends on. Download java for your specific OS from here . You can check the installation went correctly by using: java -version within a terminal instance. It should return details of your java installation. If you have multiple java installations, you may need to change the version of java you're currently using. To download the example notebooks, simply clone this repository: git clone git@github.com:moj-analytical-services/splink_demos.git Create a virtual environment using: python3 -m venv venv source venv/bin/activate Install the package list (which includes pyspark ) with: pip3 install -r requirements.txt and, if you want to use jupyter, add a kernel corresopnding to your venv: python -m ipykernel install --user --name=splink_demos jupyter lab","title":"splink_demos"},{"location":"demos/index.html#splink_demos","text":"This repo contains interactive notebooks containing demonstration and tutorial for version 3 of the Splink record linking library, the homepage for which is here .","title":"splink_demos"},{"location":"demos/index.html#running-these-notebooks-interactively","text":"You can run these notebooks in an interactive Jupyter notebook by clicking the button below:","title":"Running these notebooks interactively"},{"location":"demos/index.html#running-these-notebooks-locally-in-vscode","text":"If you don't already have it, you'll need to install java on your system in order to run pyspark , which splink currently depends on. Download java for your specific OS from here . You can check the installation went correctly by using: java -version within a terminal instance. It should return details of your java installation. If you have multiple java installations, you may need to change the version of java you're currently using. To download the example notebooks, simply clone this repository: git clone git@github.com:moj-analytical-services/splink_demos.git Create a virtual environment using: python3 -m venv venv source venv/bin/activate Install the package list (which includes pyspark ) with: pip3 install -r requirements.txt and, if you want to use jupyter, add a kernel corresopnding to your venv: python -m ipykernel install --user --name=splink_demos jupyter lab","title":"Running these notebooks locally in VSCode"},{"location":"demos/deduplicate_50k_synthetic.html","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); from splink.duckdb.duckdb_linker import DuckDBLinker import pandas as pd pd . options . display . max_rows = 1000 df = pd . read_parquet ( \"./data/historical_figures_with_errors_50k.parquet\" ) df . head ( 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } uncorrupted_record cluster full_name dob birth_place postcode_fake lat lng gender occupation unique_id 0 True Q2296770 thomas clifford, 1st baron clifford of chudleigh 1630-08-01 Devon TQ13 8DF 50.692449 -3.813964 male politician Q2296770-1 1 False Q2296770 thomas of chudleigh 1630-08-01 Devon TQ13 8DF 50.692449 -3.813964 male politician Q2296770-2 2 False Q2296770 tom 1st baron clifford of chudleigh 1630-08-01 Devon TQ13 8DF 50.692449 -3.813964 male politician Q2296770-3 3 False Q2296770 thomas 1st chudleigh 1630-08-01 Devon TQ13 8HU 50.687638 -3.895877 None politician Q2296770-4 4 False Q2296770 thomas clifford, 1st baron chudleigh 1630-08-01 Devon TQ13 8DF 50.692449 -3.813964 None politician Q2296770-5 import numpy as np import pandas as pd def clean_df ( df ): cols = [ \"unique_id\" , \"cluster\" , \"full_name\" , \"dob\" , \"birth_place\" , \"postcode_fake\" , \"gender\" , \"occupation\" , ] df = df [ cols ] . copy () df [ \"name_split\" ] = df [ \"full_name\" ] . str . strip () . str . split ( \" \" ) df [ \"name_split_length\" ] = df [ \"name_split\" ] . str . len () df [ \"first_name\" ] = df [ \"name_split\" ] . str [ 0 ] df [ \"surname\" ] = df [ \"name_split\" ] . str [ - 1 ] df [ \"surname\" ] = np . where ( df [ \"name_split_length\" ] > 1 , df [ \"surname\" ], \"\" ) # df[\"middle_names\"] = df[\"name_split\"].str[1:-1] df [ \"first_and_surname\" ] = df [ \"first_name\" ] + \" \" + df [ \"surname\" ] for col in [ \"full_name\" , \"first_and_surname\" , \"first_name\" , \"surname\" , \"dob\" , \"birth_place\" , \"postcode_fake\" , \"gender\" , \"occupation\" , ]: df [ col ] = df [ col ] . str . lower () . str . strip () df [ col ] = df [ col ] . replace ({ \"\" : None }) cols = [ \"unique_id\" , \"cluster\" , \"full_name\" , \"first_and_surname\" , \"first_name\" , \"surname\" , \"dob\" , \"birth_place\" , \"postcode_fake\" , \"gender\" , \"occupation\" , ] return df [ cols ] df_clean = clean_df ( df ) df_clean . head ( 2 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } unique_id cluster full_name first_and_surname first_name surname dob birth_place postcode_fake gender occupation 0 Q2296770-1 Q2296770 thomas clifford, 1st baron clifford of chudleigh thomas chudleigh thomas chudleigh 1630-08-01 devon tq13 8df male politician 1 Q2296770-2 Q2296770 thomas of chudleigh thomas chudleigh thomas chudleigh 1630-08-01 devon tq13 8df male politician # Initialise the linker, passing in the input dataset(s) linker = DuckDBLinker ( df_clean , connection = \":temporary:\" ) import altair as alt alt . renderers . enable ( 'mimetype' ) linker . profile_columns ([ \"first_name\" , \"postcode_fake\" , \"substr(dob, 1,4)\" ], top_n = 10 , bottom_n = 5 ) linker . compute_number_of_comparisons_generated_by_blocking_rule ( \"l.first_name = r.first_name\" ) {'count_of_pairwise_comparisons_generated': 16372982} linker . compute_number_of_comparisons_generated_by_blocking_rule ( \"l.first_name = r.first_name and l.surname = r.surname\" ,) {'count_of_pairwise_comparisons_generated': 243656} import splink.duckdb.duckdb_comparison_library as cl settings = { \"probability_two_random_records_match\" : 9 / 50_000 , \"link_type\" : \"dedupe_only\" , \"blocking_rules_to_generate_predictions\" : [ \"l.first_name = r.first_name and l.surname = r.surname\" , \"l.surname = r.surname and l.dob = r.dob\" , \"l.first_name = r.first_name and l.dob = r.dob\" , \"l.postcode_fake = r.postcode_fake and l.first_name = r.first_name\" , ], \"comparisons\" : [ cl . jaccard_at_thresholds ( \"first_name\" , [ 0.9 , 0.5 ], term_frequency_adjustments = False ), cl . jaccard_at_thresholds ( \"surname\" , [ 0.9 , 0.5 ], term_frequency_adjustments = False ), cl . levenshtein_at_thresholds ( \"dob\" , [ 1 , 2 ], term_frequency_adjustments = False ), cl . levenshtein_at_thresholds ( \"postcode_fake\" , 2 ), cl . exact_match ( \"birth_place\" , term_frequency_adjustments = False ), cl . exact_match ( \"occupation\" , term_frequency_adjustments = False ), ], \"retain_matching_columns\" : True , \"retain_intermediate_calculation_columns\" : True , \"max_iterations\" : 10 , \"em_convergence\" : 0.01 } linker . initialise_settings ( settings ) linker . estimate_u_using_random_sampling ( target_rows = 5e6 ) ----- Estimating u probabilities using random sampling ----- Estimated u probabilities using random sampling Your model is not yet fully trained. Missing estimates for: - first_name (no m values are trained). - surname (no m values are trained). - dob (no m values are trained). - postcode_fake (no m values are trained). - birth_place (no m values are trained). - occupation (no m values are trained). blocking_rule = \"l.first_name = r.first_name and l.surname = r.surname\" training_session_names = linker . estimate_parameters_using_expectation_maximisation ( blocking_rule ) training_session_names . match_weights_interactive_history_chart () ----- Starting EM training session ----- Estimating the m probabilities of the model by blocking on: l.first_name = r.first_name and l.surname = r.surname Parameter estimates will be made for the following comparison(s): - dob - postcode_fake - birth_place - occupation Parameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: - first_name - surname Iteration 1: Largest change in params was -0.527 in probability_two_random_records_match Iteration 2: Largest change in params was -0.0345 in probability_two_random_records_match Iteration 3: Largest change in params was -0.0147 in the m_probability of birth_place, level `All other comparisons` Iteration 4: Largest change in params was -0.00748 in the m_probability of dob, level `All other comparisons` EM converged after 4 iterations Your model is not yet fully trained. Missing estimates for: - first_name (no m values are trained). - surname (no m values are trained). blocking_rule = \"l.dob = r.dob\" training_session_dob = linker . estimate_parameters_using_expectation_maximisation ( blocking_rule ) training_session_dob . match_weights_interactive_history_chart () ----- Starting EM training session ----- Estimating the m probabilities of the model by blocking on: l.dob = r.dob Parameter estimates will be made for the following comparison(s): - first_name - surname - postcode_fake - birth_place - occupation Parameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: - dob Iteration 1: Largest change in params was -0.312 in the m_probability of first_name, level `Exact match` Iteration 2: Largest change in params was -0.0708 in the m_probability of first_name, level `Exact match` Iteration 3: Largest change in params was -0.0115 in the m_probability of surname, level `Exact match` Iteration 4: Largest change in params was -0.00293 in the m_probability of surname, level `Exact match` EM converged after 4 iterations Your model is fully trained. All comparisons have at least one estimate for their m and u values The final match weights can be viewed in the match weights chart: linker . match_weights_chart () df_predict = linker . predict () df_e = df_predict . as_pandas_dataframe ( limit = 5 ) df_e .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } match_weight match_probability unique_id_l unique_id_r first_name_l first_name_r gamma_first_name bf_first_name surname_l surname_r ... bf_postcode_fake birth_place_l birth_place_r gamma_birth_place bf_birth_place occupation_l occupation_r gamma_occupation bf_occupation match_key 0 16.545601 0.999990 Q2296770-1 Q2296770-14 thomas thomas 3 42.57919 chudleigh chudleigh ... 231.365812 devon NaN -1 1.000000 politician politician 1 25.912637 0 1 2.292304 0.830462 Q2296770-10 Q2296770-14 thomas thomas 3 42.57919 chudleigh chudleigh ... 0.172624 devon NaN -1 1.000000 politician politician 1 25.912637 0 2 22.370232 1.000000 Q1443188-1 Q1443188-3 frank frank 3 42.57919 brightman brightman ... 4435.362998 bristol bristol, city of 0 0.162352 liturgist liturgist 1 25.912637 0 3 22.370232 1.000000 Q1443188-2 Q1443188-3 frank frank 3 42.57919 brightman brightman ... 4435.362998 bristol bristol, city of 0 0.162352 liturgist liturgist 1 25.912637 0 4 6.157277 0.986182 Q1443188-4 Q1443188-5 francis francis 3 42.57919 brightman brightman ... 0.172624 NaN bristol, city of -1 1.000000 liturgist liturgist 1 25.912637 0 5 rows \u00d7 29 columns You can also view rows in this dataset as a waterfall chart as follows: from splink.charts import waterfall_chart records_to_plot = df_e . to_dict ( orient = \"records\" ) linker . waterfall_chart ( records_to_plot , filter_nulls = False ) clusters = linker . cluster_pairwise_predictions_at_threshold ( df_predict , threshold_match_probability = 0.95 ) Completed iteration 1, root rows count 669 Completed iteration 2, root rows count 147 Completed iteration 3, root rows count 43 Completed iteration 4, root rows count 11 Completed iteration 5, root rows count 1 Completed iteration 6, root rows count 0 linker . cluster_studio_dashboard ( df_predict , clusters , \"50k_cluster.html\" , sampling_method = 'by_cluster_size' , overwrite = True ) from IPython.display import IFrame IFrame ( src = \"./50k_cluster.html\" , width = \"100%\" , height = 1200 )","title":"Deduplicate 50k synthetic"},{"location":"demos/quickstart_demo_deduplication.html","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Deduplication tutorial \u00b6 This example shows how to de-duplicate a small dataset using simple settings. The aim is to demonstarate core Splink functionality succinctly, rather that comprehensively document all configuration options. We use the duckdb backend, which is the recommended option for smaller datasets of up to around 1 million records on a normal laptop. from splink.duckdb.duckdb_linker import DuckDBLinker Step 1: Read in data \u00b6 import pandas as pd pd . options . display . max_rows = 1000 df = pd . read_csv ( \"./data/fake_1000.csv\" ) df . head ( 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } unique_id first_name surname dob city email cluster 0 0 Robert Alan 1971-06-24 NaN robert255@smith.net 0 1 1 Robert Allen 1971-05-24 NaN roberta25@smith.net 0 2 2 Rob Allen 1971-06-24 London roberta25@smith.net 0 3 3 Robert Alen 1971-06-24 Lonon NaN 0 4 4 Grace NaN 1997-04-26 Hull grace.kelly52@jones.com 1 Note that the cluster column represents the 'ground truth' - a column which tells us with which rows refer to the same person. In most real linkage scenarios, we wouldn't have this column (this is what Splink is trying to estimate.) Step 2: Exploratory analysis \u00b6 Splink contains exploratory analysis tools designed to highlight the aspects of your data most relevant to data linking - things like missingness, skew, and whether further data cleaning may be needed prior to linking. This is useful for understanding your data, whether it suffers from skew, and whether additional data cleaning may be necessary. # Initialise the linker, passing in the input dataset(s) linker = DuckDBLinker ( df ) import altair as alt alt . renderers . enable ( 'jupyterlab' ) linker . missingness_chart () The profile_columns method creates summary charts. You may input column names (e.g. first_name ), or arbitrary sql expressions like concat(first_name, surname) ). linker . profile_columns ([ \"first_name\" , \"city\" , \"substr(dob, 1,4)\" ], top_n = 10 , bottom_n = 5 ) linker . compute_number_of_comparisons_generated_by_blocking_rule ( \"l.first_name = r.first_name\" ) {'count_of_pairwise_comparisons_generated': 1998} Step 3: Configure how Splink compares records using a settings dictionary \u00b6 splink needs to know how to compare pairs records from the input dataset, with the aim of computing an overall score that quantifies the similarity. For example, here is a pair of records from our input dataset. unique_id first_name surname dob city email 1 Robert Allen 1971-05-24 nan roberta25@smith.net 2 Rob Allen 1971-06-24 London roberta25@smith.net What functions should we use to assess the similarity of Rob vs. Robert in the the first_name field? This is configured using a settings dictionary. Rules are defined that map similarity to discrete number levels, known as \"comparison levels\". In this introductory example, we will make these comparisons simple. (In a real linkage model, they'd usually be more sophisticated). For the first_name column, there will be three comparison levels: an 'exact match' (e.g. John vs John ) similar but not the exactly the same (e.g. John vs Jon ). Specifically this will be defined as a levenshtein distance of either 1 or 2. all other comparisons For all other comparisons, we will model just two comparison levels: an 'exact match' (e.g. Smith vs Smith ), or 'anything else' (e.g. Smith vs Jones , or even Smith vs Smyth ). For city , we enable term frequency comparisons because we observed significant skew in the distribution of values import splink.duckdb.duckdb_comparison_library as cl settings = { \"probability_two_random_records_match\" : 4 / 1000 , \"link_type\" : \"dedupe_only\" , \"comparisons\" : [ cl . levenshtein_at_thresholds ( \"first_name\" , 2 ), cl . exact_match ( \"surname\" ), cl . exact_match ( \"dob\" ), cl . exact_match ( \"city\" , term_frequency_adjustments = True ), cl . exact_match ( \"email\" ), ], \"blocking_rules_to_generate_predictions\" : [ \"l.first_name = r.first_name\" , \"l.surname = r.surname\" , ], \"retain_matching_columns\" : True , \"retain_intermediate_calculation_columns\" : True , \"additional_columns_to_retain\" : [ \"cluster\" ], } In words, this setting dictionary says: We are performing a deduplication task (the other options are link_only , or link_and_dedupe , which may be used if there are multiple input datasets) When comparing records, we will use information from the first_name , surname , dob , city and email columns to compute a match score. The blocking_rules_to_generate_predictions states that we will only check for duplicates amongst records where either the first_name or surname is identical. We have enabled term frequency adjustments for the 'city' column, because some values (e.g. London ) appear much more frequently than others We will retain the cluster column in the results even though this is not used as part of comparisons. Later we'll be able to use this to compare Splink scores to the ground truth. We have set retain_intermediate_calculation_columns and additional_columns_to_retain to True for the purposes of the demo, because this will mean the output datasets contain additional information that, whilst not strictly needed by Splink, helps the user understand the calculations. If these were omitted from the settings dictionary, they would be set to False (their default value). Step 4: Estimate the parameters of the model \u00b6 Use the train_u_using_random_sampling to compute the u values of the model. linker . initialise_settings ( settings ) linker . estimate_u_using_random_sampling ( target_rows = 1e6 ) ----- Estimating u probabilities using random sampling ----- Estimated u probabilities using random sampling Your model is not yet fully trained. Missing estimates for: - first_name (no m values are trained). - surname (no m values are trained). - dob (no m values are trained). - city (no m values are trained). - email (no m values are trained). We then use the expectation maximisation algorithm to train the m values. Note that in this first EM training session we block on first_name and surname , meaning that all comparisons will have first_name and surname exactly equal. This means that, in this training session, we cannot estimate parameter estimates for the first_name or surname columns, as seen in the log messages, and their absence from the match weights chart. training_blocking_rule = \"l.first_name = r.first_name and l.surname = r.surname\" training_session_names = linker . estimate_parameters_using_expectation_maximisation ( training_blocking_rule ) training_session_names . match_weights_interactive_history_chart () ----- Starting EM training session ----- Estimating the m probabilities of the model by blocking on: l.first_name = r.first_name and l.surname = r.surname Parameter estimates will be made for the following comparison(s): - dob - city - email Parameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: - first_name - surname Iteration 1: Largest change in params was 0.479 in the m_probability of dob, level `All other comparisons` Iteration 2: Largest change in params was -0.0334 in the m_probability of email, level `Exact match` Iteration 3: Largest change in params was 0.023 in probability_two_random_records_match Iteration 4: Largest change in params was 0.0157 in probability_two_random_records_match Iteration 5: Largest change in params was 0.0113 in probability_two_random_records_match Iteration 6: Largest change in params was 0.00851 in probability_two_random_records_match Iteration 7: Largest change in params was 0.00662 in probability_two_random_records_match Iteration 8: Largest change in params was 0.00528 in probability_two_random_records_match Iteration 9: Largest change in params was 0.00429 in probability_two_random_records_match Iteration 10: Largest change in params was 0.00354 in probability_two_random_records_match Iteration 11: Largest change in params was 0.00297 in probability_two_random_records_match Iteration 12: Largest change in params was 0.00251 in probability_two_random_records_match Iteration 13: Largest change in params was 0.00214 in probability_two_random_records_match Iteration 14: Largest change in params was 0.00184 in probability_two_random_records_match Iteration 15: Largest change in params was 0.0016 in probability_two_random_records_match Iteration 16: Largest change in params was 0.00139 in probability_two_random_records_match Iteration 17: Largest change in params was 0.00122 in probability_two_random_records_match Iteration 18: Largest change in params was 0.00107 in probability_two_random_records_match Iteration 19: Largest change in params was 0.000946 in probability_two_random_records_match Iteration 20: Largest change in params was 0.000838 in probability_two_random_records_match Iteration 21: Largest change in params was 0.000745 in probability_two_random_records_match Iteration 22: Largest change in params was 0.000663 in probability_two_random_records_match Iteration 23: Largest change in params was 0.000593 in probability_two_random_records_match Iteration 24: Largest change in params was 0.00053 in probability_two_random_records_match Iteration 25: Largest change in params was 0.000476 in probability_two_random_records_match EM converged after 25 iterations Your model is not yet fully trained. Missing estimates for: - first_name (no m values are trained). - surname (no m values are trained). In a second training session, we block on dob . This allows us to estimate parameters for the first_name and surname comparisons. Between the two training sessions, we now have parameter estimates for all comparisons. training_blocking_rule = \"l.dob = r.dob\" training_session_dob = linker . estimate_parameters_using_expectation_maximisation ( training_blocking_rule ) training_session_dob . match_weights_interactive_history_chart () ----- Starting EM training session ----- Estimating the m probabilities of the model by blocking on: l.dob = r.dob Parameter estimates will be made for the following comparison(s): - first_name - surname - city - email Parameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: - dob Iteration 1: Largest change in params was 0.456 in probability_two_random_records_match Iteration 2: Largest change in params was 0.207 in probability_two_random_records_match Iteration 3: Largest change in params was 0.0733 in probability_two_random_records_match Iteration 4: Largest change in params was 0.0326 in probability_two_random_records_match Iteration 5: Largest change in params was 0.0177 in probability_two_random_records_match Iteration 6: Largest change in params was 0.0107 in probability_two_random_records_match Iteration 7: Largest change in params was 0.00702 in probability_two_random_records_match Iteration 8: Largest change in params was 0.00481 in probability_two_random_records_match Iteration 9: Largest change in params was 0.00341 in probability_two_random_records_match Iteration 10: Largest change in params was 0.00247 in probability_two_random_records_match Iteration 11: Largest change in params was 0.00182 in probability_two_random_records_match Iteration 12: Largest change in params was 0.00136 in probability_two_random_records_match Iteration 13: Largest change in params was 0.00102 in probability_two_random_records_match Iteration 14: Largest change in params was 0.000776 in probability_two_random_records_match Iteration 15: Largest change in params was 0.000592 in probability_two_random_records_match Iteration 16: Largest change in params was 0.000453 in probability_two_random_records_match Iteration 17: Largest change in params was 0.000348 in probability_two_random_records_match Iteration 18: Largest change in params was 0.000268 in probability_two_random_records_match Iteration 19: Largest change in params was 0.000206 in probability_two_random_records_match Iteration 20: Largest change in params was 0.000159 in probability_two_random_records_match Iteration 21: Largest change in params was 0.000123 in probability_two_random_records_match Iteration 22: Largest change in params was 9.54e-05 in probability_two_random_records_match EM converged after 22 iterations Your model is fully trained. All comparisons have at least one estimate for their m and u values linker . estimate_m_from_label_column ( \"cluster\" ) -------- Estimating m probabilities using from column cluster -------- Your model is fully trained. All comparisons have at least one estimate for their m and u values linker . parameter_estimate_comparisons_chart () The final match weights can be viewed in the match weights chart: linker . match_weights_chart () Step 7: Predicting match weights using the trained model \u00b6 df_predictions = linker . predict () df_predictions . as_pandas_dataframe ( limit = 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } match_weight match_probability unique_id_l unique_id_r first_name_l first_name_r gamma_first_name bf_first_name surname_l surname_r ... bf_tf_adj_city tf_city_l tf_city_r email_l email_r gamma_email bf_email cluster_l cluster_r match_key 0 5.756288 0.981836 4 5 Grace Grace 2 85.525038 NaN Kelly ... 1.0 0.00123 NaN grace.kelly52@jones.com grace.kelly52@jones.com 1 256.837973 1 1 0 1 -4.285461 0.048779 9 922 Evie Evie 2 85.525038 Dean Jones ... 1.0 0.00123 NaN evihd56@earris-bailey.net eviejones@brewer-sparks.org 0 0.437490 3 230 0 2 -4.285461 0.048779 14 998 Oliver Oliver 2 85.525038 Griffiths Bird ... 1.0 0.00123 NaN o.griffiths90@reyes-coleman.com oliver.b@smith.net 0 0.437490 5 250 0 3 -3.092784 0.104916 18 475 Caleb Caleb 2 85.525038 Rwoe Scott ... 1.0 0.04059 NaN NaN c.scott@brooks.com -1 1.000000 8 119 0 4 -5.412137 0.022946 21 917 Darcy Darcy 2 85.525038 Bernass Rhodes ... 1.0 0.00861 0.0492 darcy.b@silva.com drhodes16@johnson-robinson.com 0 0.437490 9 229 0 5 rows \u00d7 30 columns Step 8: Visualising results \u00b6 You can also view rows in this dataset as a waterfall chart as follows: records_to_plot = df_predictions . as_record_dict ( limit = 5 ) linker . waterfall_chart ( records_to_plot , filter_nulls = False ) We can visualise a sample of individual record comparisons using a waterfall chart: A histogram showing the distribution of match weights can be viewed as follows linker . match_weight_histogram ( df_predictions ) If you have a sample of labels, you can output a ROC chart. (A precision-recall chart is also available with linker.precision_recall_from_labels ) Your labels need to be formatted as follows: Step 9: Accuracy analysis \u00b6 Since we have labels in this dataset, we can compute the accuracy of our trained model df_labels = pd . read_csv ( \"./data/fake_1000_labels.csv\" ) df_labels . head ( 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } unique_id_l source_dataset_l unique_id_r source_dataset_r clerical_match_score 0 0 fake_1000 1 fake_1000 1.0 1 0 fake_1000 2 fake_1000 1.0 2 0 fake_1000 3 fake_1000 1.0 3 0 fake_1000 4 fake_1000 0.0 4 0 fake_1000 5 fake_1000 0.0 Then to produce the chart: linker . _con . register ( \"labels\" , df_labels ) linker . roc_chart_from_labels ( \"labels\" ) Step 10: Splink comparison viewer \u00b6 Create a splink_comparison_viewer interactive dashboard and display in an iframe linker . comparison_viewer_dashboard ( df_predictions , \"scv.html\" , True , 2 ) from IPython.display import IFrame IFrame ( src = \"./scv.html\" , width = \"100%\" , height = 1200 ) Clustering and visualising clusters \u00b6 df_clustered = linker . cluster_pairwise_predictions_at_threshold ( df_predictions , 0.2 ) df_clustered . as_pandas_dataframe ( limit = 5 ) Completed iteration 1, root rows count 32 Completed iteration 2, root rows count 21 Completed iteration 3, root rows count 11 Completed iteration 4, root rows count 8 Completed iteration 5, root rows count 3 Completed iteration 6, root rows count 1 Completed iteration 7, root rows count 0 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } cluster_id unique_id first_name surname dob city email cluster tf_city 0 0 0 Robert Alan 1971-06-24 NaN robert255@smith.net 0 NaN 1 1 1 Robert Allen 1971-05-24 NaN roberta25@smith.net 0 NaN 2 1 2 Rob Allen 1971-06-24 London roberta25@smith.net 0 0.212792 3 0 3 Robert Alen 1971-06-24 Lonon NaN 0 0.007380 4 4 4 Grace NaN 1997-04-26 Hull grace.kelly52@jones.com 1 0.001230 linker . cluster_studio_dashboard ( df_predictions , df_clustered , sampling_method = \"by_cluster_size\" , out_path = \"cluster_studio.html\" , overwrite = True ) from IPython.display import IFrame IFrame ( src = \"./cluster_studio.html\" , width = \"100%\" , height = 1200 )","title":"Quickstart demo deduplication"},{"location":"demos/quickstart_demo_deduplication.html#deduplication-tutorial","text":"This example shows how to de-duplicate a small dataset using simple settings. The aim is to demonstarate core Splink functionality succinctly, rather that comprehensively document all configuration options. We use the duckdb backend, which is the recommended option for smaller datasets of up to around 1 million records on a normal laptop. from splink.duckdb.duckdb_linker import DuckDBLinker","title":"Deduplication tutorial"},{"location":"demos/quickstart_demo_deduplication.html#step-1-read-in-data","text":"import pandas as pd pd . options . display . max_rows = 1000 df = pd . read_csv ( \"./data/fake_1000.csv\" ) df . head ( 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } unique_id first_name surname dob city email cluster 0 0 Robert Alan 1971-06-24 NaN robert255@smith.net 0 1 1 Robert Allen 1971-05-24 NaN roberta25@smith.net 0 2 2 Rob Allen 1971-06-24 London roberta25@smith.net 0 3 3 Robert Alen 1971-06-24 Lonon NaN 0 4 4 Grace NaN 1997-04-26 Hull grace.kelly52@jones.com 1 Note that the cluster column represents the 'ground truth' - a column which tells us with which rows refer to the same person. In most real linkage scenarios, we wouldn't have this column (this is what Splink is trying to estimate.)","title":"Step 1: Read in data"},{"location":"demos/quickstart_demo_deduplication.html#step-2-exploratory-analysis","text":"Splink contains exploratory analysis tools designed to highlight the aspects of your data most relevant to data linking - things like missingness, skew, and whether further data cleaning may be needed prior to linking. This is useful for understanding your data, whether it suffers from skew, and whether additional data cleaning may be necessary. # Initialise the linker, passing in the input dataset(s) linker = DuckDBLinker ( df ) import altair as alt alt . renderers . enable ( 'jupyterlab' ) linker . missingness_chart () The profile_columns method creates summary charts. You may input column names (e.g. first_name ), or arbitrary sql expressions like concat(first_name, surname) ). linker . profile_columns ([ \"first_name\" , \"city\" , \"substr(dob, 1,4)\" ], top_n = 10 , bottom_n = 5 ) linker . compute_number_of_comparisons_generated_by_blocking_rule ( \"l.first_name = r.first_name\" ) {'count_of_pairwise_comparisons_generated': 1998}","title":"Step 2: Exploratory analysis"},{"location":"demos/quickstart_demo_deduplication.html#step-3-configure-how-splink-compares-records-using-a-settings-dictionary","text":"splink needs to know how to compare pairs records from the input dataset, with the aim of computing an overall score that quantifies the similarity. For example, here is a pair of records from our input dataset. unique_id first_name surname dob city email 1 Robert Allen 1971-05-24 nan roberta25@smith.net 2 Rob Allen 1971-06-24 London roberta25@smith.net What functions should we use to assess the similarity of Rob vs. Robert in the the first_name field? This is configured using a settings dictionary. Rules are defined that map similarity to discrete number levels, known as \"comparison levels\". In this introductory example, we will make these comparisons simple. (In a real linkage model, they'd usually be more sophisticated). For the first_name column, there will be three comparison levels: an 'exact match' (e.g. John vs John ) similar but not the exactly the same (e.g. John vs Jon ). Specifically this will be defined as a levenshtein distance of either 1 or 2. all other comparisons For all other comparisons, we will model just two comparison levels: an 'exact match' (e.g. Smith vs Smith ), or 'anything else' (e.g. Smith vs Jones , or even Smith vs Smyth ). For city , we enable term frequency comparisons because we observed significant skew in the distribution of values import splink.duckdb.duckdb_comparison_library as cl settings = { \"probability_two_random_records_match\" : 4 / 1000 , \"link_type\" : \"dedupe_only\" , \"comparisons\" : [ cl . levenshtein_at_thresholds ( \"first_name\" , 2 ), cl . exact_match ( \"surname\" ), cl . exact_match ( \"dob\" ), cl . exact_match ( \"city\" , term_frequency_adjustments = True ), cl . exact_match ( \"email\" ), ], \"blocking_rules_to_generate_predictions\" : [ \"l.first_name = r.first_name\" , \"l.surname = r.surname\" , ], \"retain_matching_columns\" : True , \"retain_intermediate_calculation_columns\" : True , \"additional_columns_to_retain\" : [ \"cluster\" ], } In words, this setting dictionary says: We are performing a deduplication task (the other options are link_only , or link_and_dedupe , which may be used if there are multiple input datasets) When comparing records, we will use information from the first_name , surname , dob , city and email columns to compute a match score. The blocking_rules_to_generate_predictions states that we will only check for duplicates amongst records where either the first_name or surname is identical. We have enabled term frequency adjustments for the 'city' column, because some values (e.g. London ) appear much more frequently than others We will retain the cluster column in the results even though this is not used as part of comparisons. Later we'll be able to use this to compare Splink scores to the ground truth. We have set retain_intermediate_calculation_columns and additional_columns_to_retain to True for the purposes of the demo, because this will mean the output datasets contain additional information that, whilst not strictly needed by Splink, helps the user understand the calculations. If these were omitted from the settings dictionary, they would be set to False (their default value).","title":"Step 3: Configure how Splink compares records using a settings dictionary"},{"location":"demos/quickstart_demo_deduplication.html#step-4-estimate-the-parameters-of-the-model","text":"Use the train_u_using_random_sampling to compute the u values of the model. linker . initialise_settings ( settings ) linker . estimate_u_using_random_sampling ( target_rows = 1e6 ) ----- Estimating u probabilities using random sampling ----- Estimated u probabilities using random sampling Your model is not yet fully trained. Missing estimates for: - first_name (no m values are trained). - surname (no m values are trained). - dob (no m values are trained). - city (no m values are trained). - email (no m values are trained). We then use the expectation maximisation algorithm to train the m values. Note that in this first EM training session we block on first_name and surname , meaning that all comparisons will have first_name and surname exactly equal. This means that, in this training session, we cannot estimate parameter estimates for the first_name or surname columns, as seen in the log messages, and their absence from the match weights chart. training_blocking_rule = \"l.first_name = r.first_name and l.surname = r.surname\" training_session_names = linker . estimate_parameters_using_expectation_maximisation ( training_blocking_rule ) training_session_names . match_weights_interactive_history_chart () ----- Starting EM training session ----- Estimating the m probabilities of the model by blocking on: l.first_name = r.first_name and l.surname = r.surname Parameter estimates will be made for the following comparison(s): - dob - city - email Parameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: - first_name - surname Iteration 1: Largest change in params was 0.479 in the m_probability of dob, level `All other comparisons` Iteration 2: Largest change in params was -0.0334 in the m_probability of email, level `Exact match` Iteration 3: Largest change in params was 0.023 in probability_two_random_records_match Iteration 4: Largest change in params was 0.0157 in probability_two_random_records_match Iteration 5: Largest change in params was 0.0113 in probability_two_random_records_match Iteration 6: Largest change in params was 0.00851 in probability_two_random_records_match Iteration 7: Largest change in params was 0.00662 in probability_two_random_records_match Iteration 8: Largest change in params was 0.00528 in probability_two_random_records_match Iteration 9: Largest change in params was 0.00429 in probability_two_random_records_match Iteration 10: Largest change in params was 0.00354 in probability_two_random_records_match Iteration 11: Largest change in params was 0.00297 in probability_two_random_records_match Iteration 12: Largest change in params was 0.00251 in probability_two_random_records_match Iteration 13: Largest change in params was 0.00214 in probability_two_random_records_match Iteration 14: Largest change in params was 0.00184 in probability_two_random_records_match Iteration 15: Largest change in params was 0.0016 in probability_two_random_records_match Iteration 16: Largest change in params was 0.00139 in probability_two_random_records_match Iteration 17: Largest change in params was 0.00122 in probability_two_random_records_match Iteration 18: Largest change in params was 0.00107 in probability_two_random_records_match Iteration 19: Largest change in params was 0.000946 in probability_two_random_records_match Iteration 20: Largest change in params was 0.000838 in probability_two_random_records_match Iteration 21: Largest change in params was 0.000745 in probability_two_random_records_match Iteration 22: Largest change in params was 0.000663 in probability_two_random_records_match Iteration 23: Largest change in params was 0.000593 in probability_two_random_records_match Iteration 24: Largest change in params was 0.00053 in probability_two_random_records_match Iteration 25: Largest change in params was 0.000476 in probability_two_random_records_match EM converged after 25 iterations Your model is not yet fully trained. Missing estimates for: - first_name (no m values are trained). - surname (no m values are trained). In a second training session, we block on dob . This allows us to estimate parameters for the first_name and surname comparisons. Between the two training sessions, we now have parameter estimates for all comparisons. training_blocking_rule = \"l.dob = r.dob\" training_session_dob = linker . estimate_parameters_using_expectation_maximisation ( training_blocking_rule ) training_session_dob . match_weights_interactive_history_chart () ----- Starting EM training session ----- Estimating the m probabilities of the model by blocking on: l.dob = r.dob Parameter estimates will be made for the following comparison(s): - first_name - surname - city - email Parameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: - dob Iteration 1: Largest change in params was 0.456 in probability_two_random_records_match Iteration 2: Largest change in params was 0.207 in probability_two_random_records_match Iteration 3: Largest change in params was 0.0733 in probability_two_random_records_match Iteration 4: Largest change in params was 0.0326 in probability_two_random_records_match Iteration 5: Largest change in params was 0.0177 in probability_two_random_records_match Iteration 6: Largest change in params was 0.0107 in probability_two_random_records_match Iteration 7: Largest change in params was 0.00702 in probability_two_random_records_match Iteration 8: Largest change in params was 0.00481 in probability_two_random_records_match Iteration 9: Largest change in params was 0.00341 in probability_two_random_records_match Iteration 10: Largest change in params was 0.00247 in probability_two_random_records_match Iteration 11: Largest change in params was 0.00182 in probability_two_random_records_match Iteration 12: Largest change in params was 0.00136 in probability_two_random_records_match Iteration 13: Largest change in params was 0.00102 in probability_two_random_records_match Iteration 14: Largest change in params was 0.000776 in probability_two_random_records_match Iteration 15: Largest change in params was 0.000592 in probability_two_random_records_match Iteration 16: Largest change in params was 0.000453 in probability_two_random_records_match Iteration 17: Largest change in params was 0.000348 in probability_two_random_records_match Iteration 18: Largest change in params was 0.000268 in probability_two_random_records_match Iteration 19: Largest change in params was 0.000206 in probability_two_random_records_match Iteration 20: Largest change in params was 0.000159 in probability_two_random_records_match Iteration 21: Largest change in params was 0.000123 in probability_two_random_records_match Iteration 22: Largest change in params was 9.54e-05 in probability_two_random_records_match EM converged after 22 iterations Your model is fully trained. All comparisons have at least one estimate for their m and u values linker . estimate_m_from_label_column ( \"cluster\" ) -------- Estimating m probabilities using from column cluster -------- Your model is fully trained. All comparisons have at least one estimate for their m and u values linker . parameter_estimate_comparisons_chart () The final match weights can be viewed in the match weights chart: linker . match_weights_chart ()","title":"Step 4: Estimate the parameters of the model"},{"location":"demos/quickstart_demo_deduplication.html#step-7-predicting-match-weights-using-the-trained-model","text":"df_predictions = linker . predict () df_predictions . as_pandas_dataframe ( limit = 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } match_weight match_probability unique_id_l unique_id_r first_name_l first_name_r gamma_first_name bf_first_name surname_l surname_r ... bf_tf_adj_city tf_city_l tf_city_r email_l email_r gamma_email bf_email cluster_l cluster_r match_key 0 5.756288 0.981836 4 5 Grace Grace 2 85.525038 NaN Kelly ... 1.0 0.00123 NaN grace.kelly52@jones.com grace.kelly52@jones.com 1 256.837973 1 1 0 1 -4.285461 0.048779 9 922 Evie Evie 2 85.525038 Dean Jones ... 1.0 0.00123 NaN evihd56@earris-bailey.net eviejones@brewer-sparks.org 0 0.437490 3 230 0 2 -4.285461 0.048779 14 998 Oliver Oliver 2 85.525038 Griffiths Bird ... 1.0 0.00123 NaN o.griffiths90@reyes-coleman.com oliver.b@smith.net 0 0.437490 5 250 0 3 -3.092784 0.104916 18 475 Caleb Caleb 2 85.525038 Rwoe Scott ... 1.0 0.04059 NaN NaN c.scott@brooks.com -1 1.000000 8 119 0 4 -5.412137 0.022946 21 917 Darcy Darcy 2 85.525038 Bernass Rhodes ... 1.0 0.00861 0.0492 darcy.b@silva.com drhodes16@johnson-robinson.com 0 0.437490 9 229 0 5 rows \u00d7 30 columns","title":"Step 7: Predicting match weights using the trained model"},{"location":"demos/quickstart_demo_deduplication.html#step-8-visualising-results","text":"You can also view rows in this dataset as a waterfall chart as follows: records_to_plot = df_predictions . as_record_dict ( limit = 5 ) linker . waterfall_chart ( records_to_plot , filter_nulls = False ) We can visualise a sample of individual record comparisons using a waterfall chart: A histogram showing the distribution of match weights can be viewed as follows linker . match_weight_histogram ( df_predictions ) If you have a sample of labels, you can output a ROC chart. (A precision-recall chart is also available with linker.precision_recall_from_labels ) Your labels need to be formatted as follows:","title":"Step 8: Visualising results"},{"location":"demos/quickstart_demo_deduplication.html#step-9-accuracy-analysis","text":"Since we have labels in this dataset, we can compute the accuracy of our trained model df_labels = pd . read_csv ( \"./data/fake_1000_labels.csv\" ) df_labels . head ( 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } unique_id_l source_dataset_l unique_id_r source_dataset_r clerical_match_score 0 0 fake_1000 1 fake_1000 1.0 1 0 fake_1000 2 fake_1000 1.0 2 0 fake_1000 3 fake_1000 1.0 3 0 fake_1000 4 fake_1000 0.0 4 0 fake_1000 5 fake_1000 0.0 Then to produce the chart: linker . _con . register ( \"labels\" , df_labels ) linker . roc_chart_from_labels ( \"labels\" )","title":"Step 9: Accuracy analysis"},{"location":"demos/quickstart_demo_deduplication.html#step-10-splink-comparison-viewer","text":"Create a splink_comparison_viewer interactive dashboard and display in an iframe linker . comparison_viewer_dashboard ( df_predictions , \"scv.html\" , True , 2 ) from IPython.display import IFrame IFrame ( src = \"./scv.html\" , width = \"100%\" , height = 1200 )","title":"Step 10: Splink comparison viewer"},{"location":"demos/quickstart_demo_deduplication.html#clustering-and-visualising-clusters","text":"df_clustered = linker . cluster_pairwise_predictions_at_threshold ( df_predictions , 0.2 ) df_clustered . as_pandas_dataframe ( limit = 5 ) Completed iteration 1, root rows count 32 Completed iteration 2, root rows count 21 Completed iteration 3, root rows count 11 Completed iteration 4, root rows count 8 Completed iteration 5, root rows count 3 Completed iteration 6, root rows count 1 Completed iteration 7, root rows count 0 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } cluster_id unique_id first_name surname dob city email cluster tf_city 0 0 0 Robert Alan 1971-06-24 NaN robert255@smith.net 0 NaN 1 1 1 Robert Allen 1971-05-24 NaN roberta25@smith.net 0 NaN 2 1 2 Rob Allen 1971-06-24 London roberta25@smith.net 0 0.212792 3 0 3 Robert Alen 1971-06-24 Lonon NaN 0 0.007380 4 4 4 Grace NaN 1997-04-26 Hull grace.kelly52@jones.com 1 0.001230 linker . cluster_studio_dashboard ( df_predictions , df_clustered , sampling_method = \"by_cluster_size\" , out_path = \"cluster_studio.html\" , overwrite = True ) from IPython.display import IFrame IFrame ( src = \"./cluster_studio.html\" , width = \"100%\" , height = 1200 )","title":"Clustering and visualising clusters"},{"location":"demos/real_time_record_linkage.html","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Exploring linkage models using real time linkage \u00b6 In this notebook, we demonstrate splink's incremental and real time linkage capabilities - specifically: - the linker.compare_two_records function, that allows you to interactively explore the results of a linkage model; and - the linker.find_matches_to_new_records that allows you to incrementally find matches to a small number of new records Step 1: Load a pre-trained linkage model \u00b6 import sys sys . path . insert ( 0 , '/Users/robinlinacre/Documents/data_linking/splink/' ) import pandas as pd import json from splink.duckdb.duckdb_linker import DuckDBLinker with open ( \"demo_settings/real_time_settings.json\" ) as f : trained_settings = json . load ( f ) df = pd . read_csv ( \"./data/fake_1000.csv\" ) linker = DuckDBLinker ( df , trained_settings ) linker . _initialise_df_concat_with_tf () linker . compute_tf_table ( \"first_name\" ) linker . compute_tf_table ( \"surname\" ) linker . compute_tf_table ( \"dob\" ) linker . compute_tf_table ( \"city\" ) t = linker . compute_tf_table ( \"email\" ) linker . waterfall_chart ( linker . predict () . as_record_dict ( limit = 2 )) var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG; (function(spec, embedOpt){ let outputDiv = document.currentScript.previousElementSibling; if (outputDiv.id !== \"altair-viz-80d1108ba74643f4864bb0710ebb7335\") { outputDiv = document.getElementById(\"altair-viz-80d1108ba74643f4864bb0710ebb7335\"); } const paths = { \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\", \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\", \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\", \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\", }; function maybeLoadScript(lib, version) { var key = `${lib.replace(\"-\", \"\")}_version`; return (VEGA_DEBUG[key] == version) ? Promise.resolve(paths[lib]) : new Promise(function(resolve, reject) { var s = document.createElement('script'); document.getElementsByTagName(\"head\")[0].appendChild(s); s.async = true; s.onload = () => { VEGA_DEBUG[key] = version; return resolve(paths[lib]); }; s.onerror = () => reject(`Error loading script: ${paths[lib]}`); s.src = paths[lib]; }); } function showError(err) { outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`; throw err; } function displayChart(vegaEmbed) { vegaEmbed(outputDiv, spec, embedOpt) .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`)); } if(typeof define === \"function\" && define.amd) { requirejs.config({paths}); require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`)); } else { maybeLoadScript(\"vega\", \"5\") .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\")) .then(() => maybeLoadScript(\"vega-embed\", \"6\")) .catch(showError) .then(() => displayChart(vegaEmbed)); } })({\"$schema\": \"https://vega.github.io/schema/vega-lite/v5.2.0.json\", \"height\": 450, \"resolve\": {\"axis\": {\"y\": \"independent\"}}, \"width\": {\"step\": 75}, \"data\": {\"values\": [{\"column_name\": \"Prior\", \"label_for_charts\": \"Starting match weight (prior)\", \"sql_condition\": null, \"log2_bayes_factor\": -5.391889789559854, \"bayes_factor\": 0.023816582302252427, \"comparison_vector_value\": null, \"m_probability\": null, \"u_probability\": null, \"bayes_factor_description\": null, \"value_l\": \"\", \"value_r\": \"\", \"term_frequency_adjustment\": null, \"bar_sort_order\": 0, \"record_number\": 0}, {\"column_name\": \"first_name\", \"label_for_charts\": \"All other comparisons\", \"sql_condition\": \"ELSE\", \"log2_bayes_factor\": -2.1925340745596764, \"bayes_factor\": 0.21876683164040506, \"comparison_vector_value\": 0, \"m_probability\": 0.21528549148688833, \"u_probability\": 0.9840865266118626, \"bayes_factor_description\": \"If comparison level is `all other comparisons` then comparison is 4.57 times less likely to be a match\", \"value_l\": \"Oliver\", \"value_r\": \"Alfie\", \"term_frequency_adjustment\": false, \"bar_sort_order\": 1, \"record_number\": 0}, {\"column_name\": \"tf_first_name\", \"label_for_charts\": \"All other comparisons\", \"sql_condition\": \"ELSE\", \"log2_bayes_factor\": 0.0, \"bayes_factor\": 1.0, \"comparison_vector_value\": 0, \"m_probability\": 0.21528549148688833, \"u_probability\": 0.9840865266118626, \"bayes_factor_description\": \"If comparison level is `all other comparisons` then comparison is 4.57 times less likely to be a match\", \"value_l\": \"\", \"value_r\": \"\", \"term_frequency_adjustment\": true, \"bar_sort_order\": 2, \"record_number\": 0}, {\"column_name\": \"surname\", \"label_for_charts\": \"exact_match\", \"sql_condition\": \"surname_l = surname_r\", \"log2_bayes_factor\": 6.529599913880287, \"bayes_factor\": 92.38584465067325, \"comparison_vector_value\": 2, \"m_probability\": 0.4517645215191846, \"u_probability\": 0.004889975550122249, \"bayes_factor_description\": \"If comparison level is `exact_match` then comparison is 92.39 times more likely to be a match\", \"value_l\": \"Griffiths\", \"value_r\": \"Griffiths\", \"term_frequency_adjustment\": false, \"bar_sort_order\": 3, \"record_number\": 0}, {\"column_name\": \"tf_surname\", \"label_for_charts\": \"Term freq adjustment on surname with weight {cl.tf_adjustment_weight}\", \"sql_condition\": \"surname_l = surname_r\", \"log2_bayes_factor\": 0.4168001079781037, \"bayes_factor\": 1.3349633251833741, \"comparison_vector_value\": 2, \"m_probability\": null, \"u_probability\": null, \"bayes_factor_description\": \"Term frequency adjustment on surname makes comparison 1.33 times more likely to be a match\", \"value_l\": \"Griffiths\", \"value_r\": \"Griffiths\", \"term_frequency_adjustment\": true, \"bar_sort_order\": 4, \"record_number\": 0}, {\"column_name\": \"dob\", \"label_for_charts\": \"All other comparisons\", \"sql_condition\": \"ELSE\", \"log2_bayes_factor\": -2.114723717091652, \"bayes_factor\": 0.23088978993311773, \"comparison_vector_value\": 0, \"m_probability\": 0.22653362300553073, \"u_probability\": 0.9811331331331331, \"bayes_factor_description\": \"If comparison level is `all other comparisons` then comparison is 4.33 times less likely to be a match\", \"value_l\": \"1991-10-26\", \"value_r\": \"2008-05-07\", \"term_frequency_adjustment\": false, \"bar_sort_order\": 5, \"record_number\": 0}, {\"column_name\": \"tf_dob\", \"label_for_charts\": \"All other comparisons\", \"sql_condition\": \"ELSE\", \"log2_bayes_factor\": 0.0, \"bayes_factor\": 1.0, \"comparison_vector_value\": 0, \"m_probability\": 0.22653362300553073, \"u_probability\": 0.9811331331331331, \"bayes_factor_description\": \"If comparison level is `all other comparisons` then comparison is 4.33 times less likely to be a match\", \"value_l\": \"\", \"value_r\": \"\", \"term_frequency_adjustment\": true, \"bar_sort_order\": 6, \"record_number\": 0}, {\"column_name\": \"city\", \"label_for_charts\": \"All other comparisons\", \"sql_condition\": \"ELSE\", \"log2_bayes_factor\": -1.1635794871398053, \"bayes_factor\": 0.4464035832880252, \"comparison_vector_value\": 0, \"m_probability\": 0.4217855099035769, \"u_probability\": 0.9448524288198547, \"bayes_factor_description\": \"If comparison level is `all other comparisons` then comparison is 2.24 times less likely to be a match\", \"value_l\": \"Lunton\", \"value_r\": \"Plymouth\", \"term_frequency_adjustment\": false, \"bar_sort_order\": 7, \"record_number\": 0}, {\"column_name\": \"tf_city\", \"label_for_charts\": \"All other comparisons\", \"sql_condition\": \"ELSE\", \"log2_bayes_factor\": 0.0, \"bayes_factor\": 1.0, \"comparison_vector_value\": 0, \"m_probability\": 0.4217855099035769, \"u_probability\": 0.9448524288198547, \"bayes_factor_description\": \"If comparison level is `all other comparisons` then comparison is 2.24 times less likely to be a match\", \"value_l\": \"\", \"value_r\": \"\", \"term_frequency_adjustment\": true, \"bar_sort_order\": 8, \"record_number\": 0}, {\"column_name\": \"email\", \"label_for_charts\": \"All other comparisons\", \"sql_condition\": \"ELSE\", \"log2_bayes_factor\": -1.239777184635766, \"bayes_factor\": 0.4234380485302649, \"comparison_vector_value\": 0, \"m_probability\": 0.42250907994219916, \"u_probability\": 0.9978061286856716, \"bayes_factor_description\": \"If comparison level is `all other comparisons` then comparison is 2.36 times less likely to be a match\", \"value_l\": \"o.griffiths90@reyes-coleman.com\", \"value_r\": \"a.griffiths@garner-bridges.com\", \"term_frequency_adjustment\": false, \"bar_sort_order\": 9, \"record_number\": 0}, {\"column_name\": \"tf_email\", \"label_for_charts\": \"All other comparisons\", \"sql_condition\": \"ELSE\", \"log2_bayes_factor\": 0.0, \"bayes_factor\": 1.0, \"comparison_vector_value\": 0, \"m_probability\": 0.42250907994219916, \"u_probability\": 0.9978061286856716, \"bayes_factor_description\": \"If comparison level is `all other comparisons` then comparison is 2.36 times less likely to be a match\", \"value_l\": \"\", \"value_r\": \"\", \"term_frequency_adjustment\": true, \"bar_sort_order\": 10, \"record_number\": 0}, {\"column_name\": \"Final score\", \"label_for_charts\": \"Final score\", \"sql_condition\": null, \"log2_bayes_factor\": -5.156104231128363, \"bayes_factor\": 0.028045162816898225, \"comparison_vector_value\": null, \"m_probability\": null, \"u_probability\": null, \"bayes_factor_description\": null, \"value_l\": \"\", \"value_r\": \"\", \"term_frequency_adjustment\": null, \"bar_sort_order\": 11, \"record_number\": 0}, {\"column_name\": \"Prior\", \"label_for_charts\": \"Starting match weight (prior)\", \"sql_condition\": null, \"log2_bayes_factor\": -5.391889789559854, \"bayes_factor\": 0.023816582302252427, \"comparison_vector_value\": null, \"m_probability\": null, \"u_probability\": null, \"bayes_factor_description\": null, \"value_l\": \"\", \"value_r\": \"\", \"term_frequency_adjustment\": null, \"bar_sort_order\": 0, \"record_number\": 1}, {\"column_name\": \"first_name\", \"label_for_charts\": \"All other comparisons\", \"sql_condition\": \"ELSE\", \"log2_bayes_factor\": -2.1925340745596764, \"bayes_factor\": 0.21876683164040506, \"comparison_vector_value\": 0, \"m_probability\": 0.21528549148688833, \"u_probability\": 0.9840865266118626, \"bayes_factor_description\": \"If comparison level is `all other comparisons` then comparison is 4.57 times less likely to be a match\", \"value_l\": \"Rowe\", \"value_r\": \"Scott\", \"term_frequency_adjustment\": false, \"bar_sort_order\": 1, \"record_number\": 1}, {\"column_name\": \"tf_first_name\", \"label_for_charts\": \"All other comparisons\", \"sql_condition\": \"ELSE\", \"log2_bayes_factor\": 0.0, \"bayes_factor\": 1.0, \"comparison_vector_value\": 0, \"m_probability\": 0.21528549148688833, \"u_probability\": 0.9840865266118626, \"bayes_factor_description\": \"If comparison level is `all other comparisons` then comparison is 4.57 times less likely to be a match\", \"value_l\": \"\", \"value_r\": \"\", \"term_frequency_adjustment\": true, \"bar_sort_order\": 2, \"record_number\": 1}, {\"column_name\": \"surname\", \"label_for_charts\": \"exact_match\", \"sql_condition\": \"surname_l = surname_r\", \"log2_bayes_factor\": 6.529599913880287, \"bayes_factor\": 92.38584465067325, \"comparison_vector_value\": 2, \"m_probability\": 0.4517645215191846, \"u_probability\": 0.004889975550122249, \"bayes_factor_description\": \"If comparison level is `exact_match` then comparison is 92.39 times more likely to be a match\", \"value_l\": \"Caleb\", \"value_r\": \"Caleb\", \"term_frequency_adjustment\": false, \"bar_sort_order\": 3, \"record_number\": 1}, {\"column_name\": \"tf_surname\", \"label_for_charts\": \"Term freq adjustment on surname with weight {cl.tf_adjustment_weight}\", \"sql_condition\": \"surname_l = surname_r\", \"log2_bayes_factor\": 0.4168001079781037, \"bayes_factor\": 1.3349633251833741, \"comparison_vector_value\": 2, \"m_probability\": null, \"u_probability\": null, \"bayes_factor_description\": \"Term frequency adjustment on surname makes comparison 1.33 times more likely to be a match\", \"value_l\": \"Caleb\", \"value_r\": \"Caleb\", \"term_frequency_adjustment\": true, \"bar_sort_order\": 4, \"record_number\": 1}, {\"column_name\": \"dob\", \"label_for_charts\": \"All other comparisons\", \"sql_condition\": \"ELSE\", \"log2_bayes_factor\": -2.114723717091652, \"bayes_factor\": 0.23088978993311773, \"comparison_vector_value\": 0, \"m_probability\": 0.22653362300553073, \"u_probability\": 0.9811331331331331, \"bayes_factor_description\": \"If comparison level is `all other comparisons` then comparison is 4.33 times less likely to be a match\", \"value_l\": \"1992-12-20\", \"value_r\": \"1990-12-11\", \"term_frequency_adjustment\": false, \"bar_sort_order\": 5, \"record_number\": 1}, {\"column_name\": \"tf_dob\", \"label_for_charts\": \"All other comparisons\", \"sql_condition\": \"ELSE\", \"log2_bayes_factor\": 0.0, \"bayes_factor\": 1.0, \"comparison_vector_value\": 0, \"m_probability\": 0.22653362300553073, \"u_probability\": 0.9811331331331331, \"bayes_factor_description\": \"If comparison level is `all other comparisons` then comparison is 4.33 times less likely to be a match\", \"value_l\": \"\", \"value_r\": \"\", \"term_frequency_adjustment\": true, \"bar_sort_order\": 6, \"record_number\": 1}, {\"column_name\": \"city\", \"label_for_charts\": \"Null\", \"sql_condition\": \"city_l IS NULL OR city_r IS NULL\", \"log2_bayes_factor\": 0.0, \"bayes_factor\": 1.0, \"comparison_vector_value\": -1, \"m_probability\": null, \"u_probability\": null, \"bayes_factor_description\": \"If comparison level is `null` then comparison is 1.00 times more likely to be a match\", \"value_l\": \"Lvpreool\", \"value_r\": \"nan\", \"term_frequency_adjustment\": false, \"bar_sort_order\": 7, \"record_number\": 1}, {\"column_name\": \"tf_city\", \"label_for_charts\": \"Null\", \"sql_condition\": \"city_l IS NULL OR city_r IS NULL\", \"log2_bayes_factor\": 0.0, \"bayes_factor\": 1.0, \"comparison_vector_value\": -1, \"m_probability\": null, \"u_probability\": null, \"bayes_factor_description\": \"If comparison level is `null` then comparison is 1.00 times more likely to be a match\", \"value_l\": \"\", \"value_r\": \"\", \"term_frequency_adjustment\": true, \"bar_sort_order\": 8, \"record_number\": 1}, {\"column_name\": \"email\", \"label_for_charts\": \"All other comparisons\", \"sql_condition\": \"ELSE\", \"log2_bayes_factor\": -1.239777184635766, \"bayes_factor\": 0.4234380485302649, \"comparison_vector_value\": 0, \"m_probability\": 0.42250907994219916, \"u_probability\": 0.9978061286856716, \"bayes_factor_description\": \"If comparison level is `all other comparisons` then comparison is 2.36 times less likely to be a match\", \"value_l\": \"calebr@thompson.org\", \"value_r\": \"c.scott@brooks.com\", \"term_frequency_adjustment\": false, \"bar_sort_order\": 9, \"record_number\": 1}, {\"column_name\": \"tf_email\", \"label_for_charts\": \"All other comparisons\", \"sql_condition\": \"ELSE\", \"log2_bayes_factor\": 0.0, \"bayes_factor\": 1.0, \"comparison_vector_value\": 0, \"m_probability\": 0.42250907994219916, \"u_probability\": 0.9978061286856716, \"bayes_factor_description\": \"If comparison level is `all other comparisons` then comparison is 2.36 times less likely to be a match\", \"value_l\": \"\", \"value_r\": \"\", \"term_frequency_adjustment\": true, \"bar_sort_order\": 10, \"record_number\": 1}, {\"column_name\": \"Final score\", \"label_for_charts\": \"Final score\", \"sql_condition\": null, \"log2_bayes_factor\": -3.9925247439885574, \"bayes_factor\": 0.06282468122305176, \"comparison_vector_value\": null, \"m_probability\": null, \"u_probability\": null, \"bayes_factor_description\": null, \"value_l\": \"\", \"value_r\": \"\", \"term_frequency_adjustment\": null, \"bar_sort_order\": 11, \"record_number\": 1}]}, \"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"params\": [{\"name\": \"record_number\", \"value\": 0, \"bind\": {\"input\": \"range\", \"min\": 0, \"max\": 1, \"step\": 1}, \"description\": \"Filter by the interation number\"}], \"title\": {\"text\": \"Match weights waterfall chart\", \"subtitle\": \"How each comparison contributes to the final match score\"}, \"transform\": [{\"filter\": \"(datum.record_number == record_number)\"}, {\"filter\": \"(datum.bayes_factor !== 1.0)\"}, {\"window\": [{\"op\": \"sum\", \"field\": \"log2_bayes_factor\", \"as\": \"sum\"}, {\"op\": \"lead\", \"field\": \"column_name\", \"as\": \"lead\"}], \"frame\": [null, 0]}, {\"calculate\": \"datum.column_name === \\\"Final score\\\" ? datum.sum - datum.log2_bayes_factor : datum.sum\", \"as\": \"sum\"}, {\"calculate\": \"datum.lead === null ? datum.column_name : datum.lead\", \"as\": \"lead\"}, {\"calculate\": \"datum.column_name === \\\"Final score\\\" || datum.column_name === \\\"Prior match weight\\\" ? 0 : datum.sum - datum.log2_bayes_factor\", \"as\": \"previous_sum\"}, {\"calculate\": \"datum.sum > datum.previous_sum ? datum.column_name : \\\"\\\"\", \"as\": \"top_label\"}, {\"calculate\": \"datum.sum < datum.previous_sum ? datum.column_name : \\\"\\\"\", \"as\": \"bottom_label\"}, {\"calculate\": \"datum.sum > datum.previous_sum ? datum.sum : datum.previous_sum\", \"as\": \"sum_top\"}, {\"calculate\": \"datum.sum < datum.previous_sum ? datum.sum : datum.previous_sum\", \"as\": \"sum_bottom\"}, {\"calculate\": \"(datum.sum + datum.previous_sum) / 2\", \"as\": \"center\"}, {\"calculate\": \"(datum.log2_bayes_factor > 0 ? \\\"+\\\" : \\\"\\\") + datum.log2_bayes_factor\", \"as\": \"text_log2_bayes_factor\"}, {\"calculate\": \"datum.sum < datum.previous_sum ? 4 : -4\", \"as\": \"dy\"}, {\"calculate\": \"datum.sum < datum.previous_sum ? \\\"top\\\" : \\\"bottom\\\"\", \"as\": \"baseline\"}, {\"calculate\": \"1. / (1 + pow(2, -1.*datum.sum))\", \"as\": \"prob\"}, {\"calculate\": \"0*datum.sum\", \"as\": \"zero\"}], \"layer\": [{\"layer\": [{\"mark\": \"rule\", \"encoding\": {\"y\": {\"field\": \"zero\", \"type\": \"quantitative\"}, \"size\": {\"value\": 0.5}, \"color\": {\"value\": \"black\"}}}, {\"mark\": {\"type\": \"bar\", \"width\": 60}, \"encoding\": {\"color\": {\"condition\": {\"value\": \"red\", \"test\": \"(datum.log2_bayes_factor < 0)\"}, \"value\": \"green\"}, \"opacity\": {\"condition\": {\"value\": 1, \"test\": \"datum.column_name == 'Prior match weight' || datum.column_name == 'Final score'\"}, \"value\": 0.5}, \"tooltip\": [{\"type\": \"nominal\", \"field\": \"column_name\", \"title\": \"Comparison column\"}, {\"type\": \"nominal\", \"field\": \"value_l\", \"title\": \"Value (L)\"}, {\"type\": \"nominal\", \"field\": \"value_r\", \"title\": \"Value (R)\"}, {\"type\": \"ordinal\", \"field\": \"label_for_charts\", \"title\": \"Label\"}, {\"type\": \"nominal\", \"field\": \"sql_condition\", \"title\": \"SQL condition\"}, {\"type\": \"nominal\", \"field\": \"comparison_vector_value\", \"title\": \"Comparison vector value\"}, {\"type\": \"quantitative\", \"field\": \"bayes_factor\", \"title\": \"Bayes factor = m/u\", \"format\": \",.4f\"}, {\"type\": \"quantitative\", \"field\": \"log2_bayes_factor\", \"title\": \"Match weight = log2(m/u)\", \"format\": \",.4f\"}, {\"type\": \"quantitative\", \"field\": \"prob\", \"format\": \".4f\", \"title\": \"Adjusted match score\"}, {\"type\": \"nominal\", \"field\": \"bayes_factor_description\", \"title\": \"Match weight description\"}], \"x\": {\"type\": \"nominal\", \"axis\": {\"labelExpr\": \"datum.value == 'Prior' || datum.value == 'Final score' ? '' : datum.value\", \"labelAngle\": -20, \"labelAlign\": \"center\", \"labelPadding\": 10, \"title\": \"Column\", \"grid\": true, \"tickBand\": \"extent\"}, \"field\": \"column_name\", \"sort\": {\"field\": \"bar_sort_order\", \"order\": \"ascending\"}}, \"y\": {\"type\": \"quantitative\", \"axis\": {\"grid\": false, \"orient\": \"left\", \"title\": \"log2(Bayes factor)\"}, \"field\": \"previous_sum\"}, \"y2\": {\"field\": \"sum\"}}}, {\"mark\": {\"type\": \"text\", \"fontWeight\": \"bold\"}, \"encoding\": {\"color\": {\"value\": \"white\"}, \"text\": {\"condition\": {\"type\": \"nominal\", \"field\": \"log2_bayes_factor\", \"format\": \".2f\", \"test\": \"abs(datum.log2_bayes_factor) > 1\"}, \"value\": \"\"}, \"x\": {\"type\": \"nominal\", \"axis\": {\"labelAngle\": 0, \"title\": \"Column\"}, \"field\": \"column_name\", \"sort\": {\"field\": \"bar_sort_order\", \"order\": \"ascending\"}}, \"y\": {\"type\": \"quantitative\", \"axis\": {\"orient\": \"left\"}, \"field\": \"center\"}}}, {\"mark\": {\"type\": \"text\", \"baseline\": \"bottom\", \"dy\": -25, \"fontWeight\": \"bold\"}, \"encoding\": {\"color\": {\"value\": \"black\"}, \"text\": {\"type\": \"nominal\", \"field\": \"column_name\"}, \"x\": {\"type\": \"nominal\", \"axis\": {\"labelAngle\": 0, \"title\": \"Column\"}, \"field\": \"column_name\", \"sort\": {\"field\": \"bar_sort_order\", \"order\": \"ascending\"}}, \"y\": {\"type\": \"quantitative\", \"field\": \"sum_top\"}}}, {\"mark\": {\"type\": \"text\", \"baseline\": \"bottom\", \"fontSize\": 8, \"dy\": -13}, \"encoding\": {\"color\": {\"value\": \"grey\"}, \"text\": {\"type\": \"nominal\", \"field\": \"value_l\"}, \"x\": {\"type\": \"nominal\", \"axis\": {\"labelAngle\": 0, \"title\": \"Column\"}, \"field\": \"column_name\", \"sort\": {\"field\": \"bar_sort_order\", \"order\": \"ascending\"}}, \"y\": {\"type\": \"quantitative\", \"field\": \"sum_top\"}}}, {\"mark\": {\"type\": \"text\", \"baseline\": \"bottom\", \"fontSize\": 8, \"dy\": -5}, \"encoding\": {\"color\": {\"value\": \"grey\"}, \"text\": {\"type\": \"nominal\", \"field\": \"value_r\"}, \"x\": {\"type\": \"nominal\", \"axis\": {\"labelAngle\": 0, \"title\": \"Column\"}, \"field\": \"column_name\", \"sort\": {\"field\": \"bar_sort_order\", \"order\": \"ascending\"}}, \"y\": {\"type\": \"quantitative\", \"field\": \"sum_top\"}}}]}, {\"mark\": {\"type\": \"rule\", \"color\": \"black\", \"strokeWidth\": 2, \"x2Offset\": 30, \"xOffset\": -30}, \"encoding\": {\"x\": {\"type\": \"nominal\", \"axis\": {\"labelAngle\": 0, \"title\": \"Column\"}, \"field\": \"column_name\", \"sort\": {\"field\": \"bar_sort_order\", \"order\": \"ascending\"}}, \"x2\": {\"field\": \"lead\"}, \"y\": {\"type\": \"quantitative\", \"axis\": {\"labelExpr\": \"format(1 / (1 + pow(2, -1*datum.value)), '.2r')\", \"orient\": \"right\", \"title\": \"Probability\"}, \"field\": \"sum\", \"scale\": {\"zero\": false}}}}]}, {\"mode\": \"vega-lite\"}); Step Comparing two records \u00b6 It's now possible to compute a match weight for any two records using linker.compare_two_records() record_1 = { 'unique_id' : 1 , 'first_name' : \"Lucas\" , 'surname' : \"Smith\" , 'dob' : \"1984-01-02\" , 'city' : \"London\" , 'email' : \"lucas.smith@hotmail.com\" } record_2 = { 'unique_id' : 2 , 'first_name' : \"Lucas\" , 'surname' : \"Smith\" , 'dob' : \"1983-02-12\" , 'city' : \"Machester\" , 'email' : \"lucas.smith@hotmail.com\" } linker . _settings_obj_ . _retain_intermediate_calculation_columns = True linker . _settings_obj_ . _retain_matching_columns = True df_two = linker . compare_two_records ( record_1 , record_2 ) df_two . as_pandas_dataframe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } match_weight match_probability unique_id_l unique_id_r first_name_l first_name_r gamma_first_name bf_first_name bf_tf_adj_first_name tf_first_name_l ... bf_tf_adj_city tf_city_l tf_city_r email_l email_r gamma_email bf_email bf_tf_adj_email tf_email_l tf_email_r 0 13.161672 0.999891 1 2 Lucas Lucas 2 87.571229 4.814458 0.001203 ... 1.0 0.212792 NaN lucas.smith@hotmail.com lucas.smith@hotmail.com 1 263.229168 1.0 NaN NaN 1 rows \u00d7 39 columns Step 3: Interactive comparisons \u00b6 One interesting applicatin of compare_two_records is to create a simple interface that allows the user to input two records interactively, and get real time feedback. In the following cell we use ipywidets for this purpose. \u2728\u2728 Change the values in the text boxes to see the waterfall chart update in real time. \u2728\u2728 import ipywidgets as widgets fields = [ \"unique_id\" , \"first_name\" , \"surname\" , \"dob\" , \"email\" , \"city\" ] left_text_boxes = [] right_text_boxes = [] inputs_to_interactive_output = {} for f in fields : wl = widgets . Text ( description = f , value = str ( record_1 [ f ])) left_text_boxes . append ( wl ) inputs_to_interactive_output [ f \" { f } _l\" ] = wl wr = widgets . Text ( description = f , value = str ( record_2 [ f ])) right_text_boxes . append ( wr ) inputs_to_interactive_output [ f \" { f } _r\" ] = wr b1 = widgets . VBox ( left_text_boxes ) b2 = widgets . VBox ( right_text_boxes ) ui = widgets . HBox ([ b1 , b2 ]) def myfn ( ** kwargs ): my_args = dict ( kwargs ) record_left = {} record_right = {} for key , value in my_args . items (): if value == '' : value = None if key . endswith ( \"_l\" ): record_left [ key [: - 2 ]] = value if key . endswith ( \"_r\" ): record_right [ key [: - 2 ]] = value linker . _settings_obj_ . _retain_intermediate_calculation_columns = True linker . _settings_obj_ . _retain_matching_columns = True df_two = linker . compare_two_records ( record_left , record_right ) recs = df_two . as_pandas_dataframe () . to_dict ( orient = \"records\" ) from splink.charts import waterfall_chart display ( linker . waterfall_chart ( recs , filter_nulls = False )) out = widgets . interactive_output ( myfn , inputs_to_interactive_output ) display ( ui , out ) HBox(children=(VBox(children=(Text(value='1', description='unique_id'), Text(value='Lucas', description='first\u2026 Output() Finding matching records interactively \u00b6 It is also possible to search the records in the input dataset rapidly using the linker.find_matches_to_new_records() function record = { 'unique_id' : 123987 , 'first_name' : \"Robert\" , 'surname' : \"Alan\" , 'dob' : \"1971-05-24\" , 'city' : \"London\" , 'email' : \"robert255@smith.net\" } df_inc = linker . find_matches_to_new_records ([ record ], blocking_rules = []) . as_pandas_dataframe () df_inc . sort_values ( \"match_weight\" , ascending = False ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } match_weight match_probability unique_id_l unique_id_r first_name_l first_name_r gamma_first_name bf_first_name bf_tf_adj_first_name tf_first_name_l ... bf_tf_adj_city tf_city_l tf_city_r email_l email_r gamma_email bf_email bf_tf_adj_email tf_email_l tf_email_r 1 23.531793 1.000000 0 123987 Robert Robert 2 87.571229 1.604819 0.003610 ... 1.000000 NaN 0.212792 robert255@smith.net robert255@smith.net 1 263.229168 1.730964 0.001267 0.001267 2 14.550320 0.999958 1 123987 Robert Robert 2 87.571229 1.604819 0.003610 ... 1.000000 NaN 0.212792 roberta25@smith.net robert255@smith.net 0 0.423438 1.000000 0.002535 0.001267 4 10.388623 0.999255 3 123987 Robert Robert 2 87.571229 1.604819 0.003610 ... 1.000000 0.007380 0.212792 NaN robert255@smith.net -1 1.000000 1.000000 NaN 0.001267 0 2.427256 0.843228 2 123987 Rob Robert 0 0.218767 1.000000 0.001203 ... 0.259162 0.212792 0.212792 roberta25@smith.net robert255@smith.net 0 0.423438 1.000000 0.002535 0.001267 6 -2.123090 0.186697 8 123987 NaN Robert -1 1.000000 1.000000 NaN ... 1.000000 NaN 0.212792 NaN robert255@smith.net -1 1.000000 1.000000 NaN 0.001267 5 -2.205894 0.178139 754 123987 NaN Robert -1 1.000000 1.000000 NaN ... 1.000000 NaN 0.212792 j.c@whige.wort robert255@smith.net 0 0.423438 1.000000 0.001267 0.001267 3 -2.802309 0.125383 750 123987 NaN Robert -1 1.000000 1.000000 NaN ... 0.259162 0.212792 0.212792 j.c@white.org robert255@smith.net 0 0.423438 1.000000 0.002535 0.001267 7 rows \u00d7 39 columns Interactive interface for finding records \u00b6 Again, we can use ipywidgets to build an interactive interface for the linker.find_matches_to_new_records function from splink.charts import waterfall_chart @widgets . interact ( first_name = 'Robert' , surname = \"Alan\" , dob = \"1971-05-24\" , city = \"London\" , email = \"robert255@smith.net\" ) def interactive_link ( first_name , surname , dob , city , email ): record = { 'unique_id' : 123987 , 'first_name' : first_name , 'surname' : surname , 'dob' : dob , 'city' : city , 'email' : email , 'group' : 0 } for key in record . keys (): if type ( record [ key ]) == str : if record [ key ] . strip () == \"\" : record [ key ] = None df_inc = linker . find_matches_to_new_records ([ record ], blocking_rules = [ f \"(true)\" ]) . as_pandas_dataframe () df_inc = df_inc . sort_values ( \"match_weight\" , ascending = False ) recs = df_inc . to_dict ( orient = \"records\" ) display ( linker . waterfall_chart ( recs , filter_nulls = False )) interactive(children=(Text(value='Robert', description='first_name'), Text(value='Alan', description='surname'\u2026 linker . match_weights_chart () var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG; (function(spec, embedOpt){ let outputDiv = document.currentScript.previousElementSibling; if (outputDiv.id !== \"altair-viz-4663e99319364be8854b4bc5d4c58cbf\") { outputDiv = document.getElementById(\"altair-viz-4663e99319364be8854b4bc5d4c58cbf\"); } const paths = { \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\", \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\", \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\", \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\", }; function maybeLoadScript(lib, version) { var key = `${lib.replace(\"-\", \"\")}_version`; return (VEGA_DEBUG[key] == version) ? Promise.resolve(paths[lib]) : new Promise(function(resolve, reject) { var s = document.createElement('script'); document.getElementsByTagName(\"head\")[0].appendChild(s); s.async = true; s.onload = () => { VEGA_DEBUG[key] = version; return resolve(paths[lib]); }; s.onerror = () => reject(`Error loading script: ${paths[lib]}`); s.src = paths[lib]; }); } function showError(err) { outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`; throw err; } function displayChart(vegaEmbed) { vegaEmbed(outputDiv, spec, embedOpt) .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`)); } if(typeof define === \"function\" && define.amd) { requirejs.config({paths}); require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`)); } else { maybeLoadScript(\"vega\", \"5\") .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\")) .then(() => maybeLoadScript(\"vega-embed\", \"6\")) .catch(showError) .then(() => displayChart(vegaEmbed)); } })({\"config\": {\"view\": {\"width\": 400, \"height\": 60}, \"mark\": {\"tooltip\": null}, \"title\": {\"anchor\": \"middle\"}, \"header\": {\"title\": null}}, \"data\": {\"values\": [{\"comparison_name\": \"probability_two_random_records_match\", \"sql_condition\": null, \"label_for_charts\": \"\", \"m_probability\": null, \"u_probability\": null, \"m_probability_description\": null, \"u_probability_description\": null, \"has_tf_adjustments\": false, \"tf_adjustment_column\": null, \"tf_adjustment_weight\": null, \"is_null_level\": false, \"bayes_factor\": 0.023816582302252427, \"log2_bayes_factor\": -5.391889789559854, \"comparison_vector_value\": 0, \"max_comparison_vector_value\": 0, \"bayes_factor_description\": \"The probability that two random records drawn at random match is 0.023 or one in 43.0 records.This is equivalent to a starting match weight of -5.392.\", \"probability_two_random_records_match\": 0.02326254791526835, \"comparison_sort_order\": -1}, {\"comparison_name\": \"first_name\", \"sql_condition\": \"first_name_l = first_name_r\", \"label_for_charts\": \"exact_match\", \"m_probability\": 0.5073501669215337, \"u_probability\": 0.0057935713975033705, \"m_probability_description\": \"Amongst matching record comparisons, 50.74% of records are in the exact_match comparison level\", \"u_probability_description\": \"Amongst non-matching record comparisons, 0.58% of records are in the exact_match comparison level\", \"has_tf_adjustments\": true, \"tf_adjustment_column\": \"first_name\", \"tf_adjustment_weight\": 1.0, \"is_null_level\": false, \"bayes_factor\": 87.57122888658395, \"log2_bayes_factor\": 6.452385051922501, \"comparison_vector_value\": 2, \"max_comparison_vector_value\": 2, \"bayes_factor_description\": \"If comparison level is `exact_match` then comparison is 87.57 times more likely to be a match\", \"probability_two_random_records_match\": 0.02326254791526835, \"comparison_sort_order\": 0}, {\"comparison_name\": \"first_name\", \"sql_condition\": \"levenshtein(first_name_l, first_name_r) <= 2\", \"label_for_charts\": \"Levenstein <= 2\", \"m_probability\": 0.27736434159157797, \"u_probability\": 0.010119901990634016, \"m_probability_description\": \"Amongst matching record comparisons, 27.74% of records are in the levenstein <= 2 comparison level\", \"u_probability_description\": \"Amongst non-matching record comparisons, 1.01% of records are in the levenstein <= 2 comparison level\", \"has_tf_adjustments\": false, \"tf_adjustment_column\": null, \"tf_adjustment_weight\": 1.0, \"is_null_level\": false, \"bayes_factor\": 27.407809072486973, \"log2_bayes_factor\": 4.776515101395072, \"comparison_vector_value\": 1, \"max_comparison_vector_value\": 2, \"bayes_factor_description\": \"If comparison level is `levenstein <= 2` then comparison is 27.41 times more likely to be a match\", \"probability_two_random_records_match\": 0.02326254791526835, \"comparison_sort_order\": 0}, {\"comparison_name\": \"first_name\", \"sql_condition\": \"ELSE\", \"label_for_charts\": \"All other comparisons\", \"m_probability\": 0.21528549148688833, \"u_probability\": 0.9840865266118626, \"m_probability_description\": \"Amongst matching record comparisons, 21.53% of records are in the all other comparisons comparison level\", \"u_probability_description\": \"Amongst non-matching record comparisons, 98.41% of records are in the all other comparisons comparison level\", \"has_tf_adjustments\": false, \"tf_adjustment_column\": null, \"tf_adjustment_weight\": 1.0, \"is_null_level\": false, \"bayes_factor\": 0.21876683164040506, \"log2_bayes_factor\": -2.1925340745596764, \"comparison_vector_value\": 0, \"max_comparison_vector_value\": 2, \"bayes_factor_description\": \"If comparison level is `all other comparisons` then comparison is 4.57 times less likely to be a match\", \"probability_two_random_records_match\": 0.02326254791526835, \"comparison_sort_order\": 0}, {\"comparison_name\": \"surname\", \"sql_condition\": \"surname_l = surname_r\", \"label_for_charts\": \"exact_match\", \"m_probability\": 0.4517645215191846, \"u_probability\": 0.004889975550122249, \"m_probability_description\": \"Amongst matching record comparisons, 45.18% of records are in the exact_match comparison level\", \"u_probability_description\": \"Amongst non-matching record comparisons, 0.49% of records are in the exact_match comparison level\", \"has_tf_adjustments\": true, \"tf_adjustment_column\": \"surname\", \"tf_adjustment_weight\": 1.0, \"is_null_level\": false, \"bayes_factor\": 92.38584465067325, \"log2_bayes_factor\": 6.529599913880287, \"comparison_vector_value\": 2, \"max_comparison_vector_value\": 2, \"bayes_factor_description\": \"If comparison level is `exact_match` then comparison is 92.39 times more likely to be a match\", \"probability_two_random_records_match\": 0.02326254791526835, \"comparison_sort_order\": 1}, {\"comparison_name\": \"surname\", \"sql_condition\": \"levenshtein(surname_l, surname_r) <= 2\", \"label_for_charts\": \"Levenstein <= 2\", \"m_probability\": 0.3078165102205689, \"u_probability\": 0.007373772654946249, \"m_probability_description\": \"Amongst matching record comparisons, 30.78% of records are in the levenstein <= 2 comparison level\", \"u_probability_description\": \"Amongst non-matching record comparisons, 0.74% of records are in the levenstein <= 2 comparison level\", \"has_tf_adjustments\": false, \"tf_adjustment_column\": null, \"tf_adjustment_weight\": 1.0, \"is_null_level\": false, \"bayes_factor\": 41.74477904659683, \"log2_bayes_factor\": 5.383523868172574, \"comparison_vector_value\": 1, \"max_comparison_vector_value\": 2, \"bayes_factor_description\": \"If comparison level is `levenstein <= 2` then comparison is 41.74 times more likely to be a match\", \"probability_two_random_records_match\": 0.02326254791526835, \"comparison_sort_order\": 1}, {\"comparison_name\": \"surname\", \"sql_condition\": \"ELSE\", \"label_for_charts\": \"All other comparisons\", \"m_probability\": 0.24041896826024636, \"u_probability\": 0.9877362517949315, \"m_probability_description\": \"Amongst matching record comparisons, 24.04% of records are in the all other comparisons comparison level\", \"u_probability_description\": \"Amongst non-matching record comparisons, 98.77% of records are in the all other comparisons comparison level\", \"has_tf_adjustments\": false, \"tf_adjustment_column\": null, \"tf_adjustment_weight\": 1.0, \"is_null_level\": false, \"bayes_factor\": 0.24340401379756268, \"log2_bayes_factor\": -2.03857513621085, \"comparison_vector_value\": 0, \"max_comparison_vector_value\": 2, \"bayes_factor_description\": \"If comparison level is `all other comparisons` then comparison is 4.11 times less likely to be a match\", \"probability_two_random_records_match\": 0.02326254791526835, \"comparison_sort_order\": 1}, {\"comparison_name\": \"dob\", \"sql_condition\": \"dob_l = dob_r\", \"label_for_charts\": \"exact_match\", \"m_probability\": 0.405530771330678, \"u_probability\": 0.0017477477477477479, \"m_probability_description\": \"Amongst matching record comparisons, 40.55% of records are in the exact_match comparison level\", \"u_probability_description\": \"Amongst non-matching record comparisons, 0.17% of records are in the exact_match comparison level\", \"has_tf_adjustments\": true, \"tf_adjustment_column\": \"dob\", \"tf_adjustment_weight\": 1.0, \"is_null_level\": false, \"bayes_factor\": 232.03049287476935, \"log2_bayes_factor\": 7.858170603008739, \"comparison_vector_value\": 2, \"max_comparison_vector_value\": 2, \"bayes_factor_description\": \"If comparison level is `exact_match` then comparison is 232.03 times more likely to be a match\", \"probability_two_random_records_match\": 0.02326254791526835, \"comparison_sort_order\": 2}, {\"comparison_name\": \"dob\", \"sql_condition\": \"levenshtein(dob_l, dob_r) <= 2\", \"label_for_charts\": \"Levenstein <= 2\", \"m_probability\": 0.3679356056637918, \"u_probability\": 0.01711911911911912, \"m_probability_description\": \"Amongst matching record comparisons, 36.79% of records are in the levenstein <= 2 comparison level\", \"u_probability_description\": \"Amongst non-matching record comparisons, 1.71% of records are in the levenstein <= 2 comparison level\", \"has_tf_adjustments\": false, \"tf_adjustment_column\": null, \"tf_adjustment_weight\": 1.0, \"is_null_level\": false, \"bayes_factor\": 21.492671620753597, \"log2_bayes_factor\": 4.425772921275592, \"comparison_vector_value\": 1, \"max_comparison_vector_value\": 2, \"bayes_factor_description\": \"If comparison level is `levenstein <= 2` then comparison is 21.49 times more likely to be a match\", \"probability_two_random_records_match\": 0.02326254791526835, \"comparison_sort_order\": 2}, {\"comparison_name\": \"dob\", \"sql_condition\": \"ELSE\", \"label_for_charts\": \"All other comparisons\", \"m_probability\": 0.22653362300553073, \"u_probability\": 0.9811331331331331, \"m_probability_description\": \"Amongst matching record comparisons, 22.65% of records are in the all other comparisons comparison level\", \"u_probability_description\": \"Amongst non-matching record comparisons, 98.11% of records are in the all other comparisons comparison level\", \"has_tf_adjustments\": false, \"tf_adjustment_column\": null, \"tf_adjustment_weight\": 1.0, \"is_null_level\": false, \"bayes_factor\": 0.23088978993311773, \"log2_bayes_factor\": -2.114723717091652, \"comparison_vector_value\": 0, \"max_comparison_vector_value\": 2, \"bayes_factor_description\": \"If comparison level is `all other comparisons` then comparison is 4.33 times less likely to be a match\", \"probability_two_random_records_match\": 0.02326254791526835, \"comparison_sort_order\": 2}, {\"comparison_name\": \"city\", \"sql_condition\": \"city_l = city_r\", \"label_for_charts\": \"exact_match\", \"m_probability\": 0.5782144900964232, \"u_probability\": 0.0551475711801453, \"m_probability_description\": \"Amongst matching record comparisons, 57.82% of records are in the exact_match comparison level\", \"u_probability_description\": \"Amongst non-matching record comparisons, 5.51% of records are in the exact_match comparison level\", \"has_tf_adjustments\": true, \"tf_adjustment_column\": \"city\", \"tf_adjustment_weight\": 1.0, \"is_null_level\": false, \"bayes_factor\": 10.484858675056154, \"log2_bayes_factor\": 3.3902355104306197, \"comparison_vector_value\": 1, \"max_comparison_vector_value\": 1, \"bayes_factor_description\": \"If comparison level is `exact_match` then comparison is 10.48 times more likely to be a match\", \"probability_two_random_records_match\": 0.02326254791526835, \"comparison_sort_order\": 3}, {\"comparison_name\": \"city\", \"sql_condition\": \"ELSE\", \"label_for_charts\": \"All other comparisons\", \"m_probability\": 0.4217855099035769, \"u_probability\": 0.9448524288198547, \"m_probability_description\": \"Amongst matching record comparisons, 42.18% of records are in the all other comparisons comparison level\", \"u_probability_description\": \"Amongst non-matching record comparisons, 94.49% of records are in the all other comparisons comparison level\", \"has_tf_adjustments\": false, \"tf_adjustment_column\": null, \"tf_adjustment_weight\": 1.0, \"is_null_level\": false, \"bayes_factor\": 0.4464035832880252, \"log2_bayes_factor\": -1.1635794871398053, \"comparison_vector_value\": 0, \"max_comparison_vector_value\": 1, \"bayes_factor_description\": \"If comparison level is `all other comparisons` then comparison is 2.24 times less likely to be a match\", \"probability_two_random_records_match\": 0.02326254791526835, \"comparison_sort_order\": 3}, {\"comparison_name\": \"email\", \"sql_condition\": \"email_l = email_r\", \"label_for_charts\": \"exact_match\", \"m_probability\": 0.5774909200578013, \"u_probability\": 0.0021938713143283602, \"m_probability_description\": \"Amongst matching record comparisons, 57.75% of records are in the exact_match comparison level\", \"u_probability_description\": \"Amongst non-matching record comparisons, 0.22% of records are in the exact_match comparison level\", \"has_tf_adjustments\": true, \"tf_adjustment_column\": \"email\", \"tf_adjustment_weight\": 1.0, \"is_null_level\": false, \"bayes_factor\": 263.2291676754963, \"log2_bayes_factor\": 8.04017554864013, \"comparison_vector_value\": 1, \"max_comparison_vector_value\": 1, \"bayes_factor_description\": \"If comparison level is `exact_match` then comparison is 263.23 times more likely to be a match\", \"probability_two_random_records_match\": 0.02326254791526835, \"comparison_sort_order\": 4}, {\"comparison_name\": \"email\", \"sql_condition\": \"ELSE\", \"label_for_charts\": \"All other comparisons\", \"m_probability\": 0.42250907994219916, \"u_probability\": 0.9978061286856716, \"m_probability_description\": \"Amongst matching record comparisons, 42.25% of records are in the all other comparisons comparison level\", \"u_probability_description\": \"Amongst non-matching record comparisons, 99.78% of records are in the all other comparisons comparison level\", \"has_tf_adjustments\": false, \"tf_adjustment_column\": null, \"tf_adjustment_weight\": 1.0, \"is_null_level\": false, \"bayes_factor\": 0.4234380485302649, \"log2_bayes_factor\": -1.239777184635766, \"comparison_vector_value\": 0, \"max_comparison_vector_value\": 1, \"bayes_factor_description\": \"If comparison level is `all other comparisons` then comparison is 2.36 times less likely to be a match\", \"probability_two_random_records_match\": 0.02326254791526835, \"comparison_sort_order\": 4}]}, \"vconcat\": [{\"height\": 30, \"mark\": {\"type\": \"bar\", \"clip\": true, \"height\": 20}, \"selection\": {\"zoom_selector\": {\"type\": \"interval\", \"bind\": \"scales\", \"encodings\": [\"x\"]}}, \"transform\": [{\"filter\": \"(datum.comparison_name == 'probability_two_random_records_match')\"}], \"encoding\": {\"color\": {\"type\": \"quantitative\", \"field\": \"log2_bayes_factor\", \"title\": \"Match weight\", \"scale\": {\"range\": [\"red\", \"orange\", \"green\"], \"domain\": [-10, 0, 10]}}, \"tooltip\": [{\"type\": \"nominal\", \"field\": \"comparison_name\", \"title\": \"Comparison name\"}, {\"type\": \"nominal\", \"field\": \"probability_two_random_records_match\", \"format\": \".4f\", \"title\": \"Probability two random records match\"}, {\"type\": \"quantitative\", \"field\": \"log2_bayes_factor\", \"title\": \"Equivalent match weight\", \"format\": \",.4f\"}, {\"type\": \"nominal\", \"field\": \"bayes_factor_description\", \"title\": \"Match weight description\"}], \"x\": {\"type\": \"quantitative\", \"axis\": {\"labels\": false, \"domain\": false, \"title\": \"\", \"ticks\": false}, \"field\": \"log2_bayes_factor\", \"scale\": {\"domain\": [-10, 10]}}, \"y\": {\"type\": \"nominal\", \"field\": \"label_for_charts\", \"axis\": {\"title\": \"Prior (starting) match weight\", \"titleAngle\": 0, \"titleAlign\": \"right\", \"titleFontWeight\": \"normal\"}, \"sort\": {\"field\": \"comparison_vector_value\", \"order\": \"descending\"}}}}, {\"mark\": {\"type\": \"bar\", \"clip\": true}, \"selection\": {\"zoom_selector\": {\"type\": \"interval\", \"bind\": \"scales\", \"encodings\": [\"x\"]}}, \"transform\": [{\"filter\": \"(datum.comparison_name != 'probability_two_random_records_match')\"}], \"encoding\": {\"color\": {\"type\": \"quantitative\", \"field\": \"log2_bayes_factor\", \"title\": \"Match weight\", \"scale\": {\"range\": [\"red\", \"orange\", \"green\"], \"domain\": [-10, 0, 10]}}, \"row\": {\"type\": \"nominal\", \"field\": \"comparison_name\", \"sort\": {\"field\": \"comparison_sort_order\"}, \"header\": {\"labelAngle\": 0, \"labelAnchor\": \"middle\", \"labelAlign\": \"left\"}}, \"tooltip\": [{\"type\": \"nominal\", \"field\": \"comparison_name\", \"title\": \"Comparison name\"}, {\"type\": \"ordinal\", \"field\": \"label_for_charts\", \"title\": \"Label\"}, {\"type\": \"nominal\", \"field\": \"sql_condition\", \"title\": \"SQL condition\"}, {\"type\": \"quantitative\", \"field\": \"m_probability\", \"format\": \".4f\", \"title\": \"M probability\"}, {\"type\": \"quantitative\", \"field\": \"u_probability\", \"format\": \".4f\", \"title\": \"U probability\"}, {\"type\": \"quantitative\", \"field\": \"bayes_factor\", \"title\": \"Bayes factor = m/u\", \"format\": \",.4f\"}, {\"type\": \"quantitative\", \"field\": \"log2_bayes_factor\", \"title\": \"Match weight = log2(m/u)\", \"format\": \",.4f\"}, {\"type\": \"nominal\", \"field\": \"bayes_factor_description\", \"title\": \"Match weight description\"}], \"x\": {\"type\": \"quantitative\", \"axis\": {\"title\": \"Comparison level match weight = log2(m/u)\"}, \"field\": \"log2_bayes_factor\", \"scale\": {\"domain\": [-10, 10]}}, \"y\": {\"type\": \"nominal\", \"field\": \"label_for_charts\", \"axis\": {\"title\": null}, \"sort\": {\"field\": \"comparison_vector_value\", \"order\": \"descending\"}}}, \"resolve\": {\"axis\": {\"y\": \"independent\"}, \"scale\": {\"y\": \"independent\"}}}], \"selection\": {\"zoom_selector\": {\"type\": \"interval\", \"bind\": \"scales\", \"encodings\": [\"x\"]}}, \"resolve\": {\"axis\": {\"y\": \"independent\"}, \"scale\": {\"y\": \"independent\"}}, \"title\": {\"text\": \"Model parameters (components of final match weight)\", \"subtitle\": \"Use mousewheel to zoom\"}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.2.json\"}, {\"mode\": \"vega-lite\"});","title":"Real time record linkage"},{"location":"demos/real_time_record_linkage.html#exploring-linkage-models-using-real-time-linkage","text":"In this notebook, we demonstrate splink's incremental and real time linkage capabilities - specifically: - the linker.compare_two_records function, that allows you to interactively explore the results of a linkage model; and - the linker.find_matches_to_new_records that allows you to incrementally find matches to a small number of new records","title":"Exploring linkage models using real time linkage"},{"location":"demos/real_time_record_linkage.html#step-1-load-a-pre-trained-linkage-model","text":"import sys sys . path . insert ( 0 , '/Users/robinlinacre/Documents/data_linking/splink/' ) import pandas as pd import json from splink.duckdb.duckdb_linker import DuckDBLinker with open ( \"demo_settings/real_time_settings.json\" ) as f : trained_settings = json . load ( f ) df = pd . read_csv ( \"./data/fake_1000.csv\" ) linker = DuckDBLinker ( df , trained_settings ) linker . _initialise_df_concat_with_tf () linker . compute_tf_table ( \"first_name\" ) linker . compute_tf_table ( \"surname\" ) linker . compute_tf_table ( \"dob\" ) linker . compute_tf_table ( \"city\" ) t = linker . compute_tf_table ( \"email\" ) linker . waterfall_chart ( linker . predict () . as_record_dict ( limit = 2 )) var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG; (function(spec, embedOpt){ let outputDiv = document.currentScript.previousElementSibling; if (outputDiv.id !== \"altair-viz-80d1108ba74643f4864bb0710ebb7335\") { outputDiv = document.getElementById(\"altair-viz-80d1108ba74643f4864bb0710ebb7335\"); } const paths = { \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\", \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\", \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\", \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\", }; function maybeLoadScript(lib, version) { var key = `${lib.replace(\"-\", \"\")}_version`; return (VEGA_DEBUG[key] == version) ? Promise.resolve(paths[lib]) : new Promise(function(resolve, reject) { var s = document.createElement('script'); document.getElementsByTagName(\"head\")[0].appendChild(s); s.async = true; s.onload = () => { VEGA_DEBUG[key] = version; return resolve(paths[lib]); }; s.onerror = () => reject(`Error loading script: ${paths[lib]}`); s.src = paths[lib]; }); } function showError(err) { outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`; throw err; } function displayChart(vegaEmbed) { vegaEmbed(outputDiv, spec, embedOpt) .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`)); } if(typeof define === \"function\" && define.amd) { requirejs.config({paths}); require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`)); } else { maybeLoadScript(\"vega\", \"5\") .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\")) .then(() => maybeLoadScript(\"vega-embed\", \"6\")) .catch(showError) .then(() => displayChart(vegaEmbed)); } })({\"$schema\": \"https://vega.github.io/schema/vega-lite/v5.2.0.json\", \"height\": 450, \"resolve\": {\"axis\": {\"y\": \"independent\"}}, \"width\": {\"step\": 75}, \"data\": {\"values\": [{\"column_name\": \"Prior\", \"label_for_charts\": \"Starting match weight (prior)\", \"sql_condition\": null, \"log2_bayes_factor\": -5.391889789559854, \"bayes_factor\": 0.023816582302252427, \"comparison_vector_value\": null, \"m_probability\": null, \"u_probability\": null, \"bayes_factor_description\": null, \"value_l\": \"\", \"value_r\": \"\", \"term_frequency_adjustment\": null, \"bar_sort_order\": 0, \"record_number\": 0}, {\"column_name\": \"first_name\", \"label_for_charts\": \"All other comparisons\", \"sql_condition\": \"ELSE\", \"log2_bayes_factor\": -2.1925340745596764, \"bayes_factor\": 0.21876683164040506, \"comparison_vector_value\": 0, \"m_probability\": 0.21528549148688833, \"u_probability\": 0.9840865266118626, \"bayes_factor_description\": \"If comparison level is `all other comparisons` then comparison is 4.57 times less likely to be a match\", \"value_l\": \"Oliver\", \"value_r\": \"Alfie\", \"term_frequency_adjustment\": false, \"bar_sort_order\": 1, \"record_number\": 0}, {\"column_name\": \"tf_first_name\", \"label_for_charts\": \"All other comparisons\", \"sql_condition\": \"ELSE\", \"log2_bayes_factor\": 0.0, \"bayes_factor\": 1.0, \"comparison_vector_value\": 0, \"m_probability\": 0.21528549148688833, \"u_probability\": 0.9840865266118626, \"bayes_factor_description\": \"If comparison level is `all other comparisons` then comparison is 4.57 times less likely to be a match\", \"value_l\": \"\", \"value_r\": \"\", \"term_frequency_adjustment\": true, \"bar_sort_order\": 2, \"record_number\": 0}, {\"column_name\": \"surname\", \"label_for_charts\": \"exact_match\", \"sql_condition\": \"surname_l = surname_r\", \"log2_bayes_factor\": 6.529599913880287, \"bayes_factor\": 92.38584465067325, \"comparison_vector_value\": 2, \"m_probability\": 0.4517645215191846, \"u_probability\": 0.004889975550122249, \"bayes_factor_description\": \"If comparison level is `exact_match` then comparison is 92.39 times more likely to be a match\", \"value_l\": \"Griffiths\", \"value_r\": \"Griffiths\", \"term_frequency_adjustment\": false, \"bar_sort_order\": 3, \"record_number\": 0}, {\"column_name\": \"tf_surname\", \"label_for_charts\": \"Term freq adjustment on surname with weight {cl.tf_adjustment_weight}\", \"sql_condition\": \"surname_l = surname_r\", \"log2_bayes_factor\": 0.4168001079781037, \"bayes_factor\": 1.3349633251833741, \"comparison_vector_value\": 2, \"m_probability\": null, \"u_probability\": null, \"bayes_factor_description\": \"Term frequency adjustment on surname makes comparison 1.33 times more likely to be a match\", \"value_l\": \"Griffiths\", \"value_r\": \"Griffiths\", \"term_frequency_adjustment\": true, \"bar_sort_order\": 4, \"record_number\": 0}, {\"column_name\": \"dob\", \"label_for_charts\": \"All other comparisons\", \"sql_condition\": \"ELSE\", \"log2_bayes_factor\": -2.114723717091652, \"bayes_factor\": 0.23088978993311773, \"comparison_vector_value\": 0, \"m_probability\": 0.22653362300553073, \"u_probability\": 0.9811331331331331, \"bayes_factor_description\": \"If comparison level is `all other comparisons` then comparison is 4.33 times less likely to be a match\", \"value_l\": \"1991-10-26\", \"value_r\": \"2008-05-07\", \"term_frequency_adjustment\": false, \"bar_sort_order\": 5, \"record_number\": 0}, {\"column_name\": \"tf_dob\", \"label_for_charts\": \"All other comparisons\", \"sql_condition\": \"ELSE\", \"log2_bayes_factor\": 0.0, \"bayes_factor\": 1.0, \"comparison_vector_value\": 0, \"m_probability\": 0.22653362300553073, \"u_probability\": 0.9811331331331331, \"bayes_factor_description\": \"If comparison level is `all other comparisons` then comparison is 4.33 times less likely to be a match\", \"value_l\": \"\", \"value_r\": \"\", \"term_frequency_adjustment\": true, \"bar_sort_order\": 6, \"record_number\": 0}, {\"column_name\": \"city\", \"label_for_charts\": \"All other comparisons\", \"sql_condition\": \"ELSE\", \"log2_bayes_factor\": -1.1635794871398053, \"bayes_factor\": 0.4464035832880252, \"comparison_vector_value\": 0, \"m_probability\": 0.4217855099035769, \"u_probability\": 0.9448524288198547, \"bayes_factor_description\": \"If comparison level is `all other comparisons` then comparison is 2.24 times less likely to be a match\", \"value_l\": \"Lunton\", \"value_r\": \"Plymouth\", \"term_frequency_adjustment\": false, \"bar_sort_order\": 7, \"record_number\": 0}, {\"column_name\": \"tf_city\", \"label_for_charts\": \"All other comparisons\", \"sql_condition\": \"ELSE\", \"log2_bayes_factor\": 0.0, \"bayes_factor\": 1.0, \"comparison_vector_value\": 0, \"m_probability\": 0.4217855099035769, \"u_probability\": 0.9448524288198547, \"bayes_factor_description\": \"If comparison level is `all other comparisons` then comparison is 2.24 times less likely to be a match\", \"value_l\": \"\", \"value_r\": \"\", \"term_frequency_adjustment\": true, \"bar_sort_order\": 8, \"record_number\": 0}, {\"column_name\": \"email\", \"label_for_charts\": \"All other comparisons\", \"sql_condition\": \"ELSE\", \"log2_bayes_factor\": -1.239777184635766, \"bayes_factor\": 0.4234380485302649, \"comparison_vector_value\": 0, \"m_probability\": 0.42250907994219916, \"u_probability\": 0.9978061286856716, \"bayes_factor_description\": \"If comparison level is `all other comparisons` then comparison is 2.36 times less likely to be a match\", \"value_l\": \"o.griffiths90@reyes-coleman.com\", \"value_r\": \"a.griffiths@garner-bridges.com\", \"term_frequency_adjustment\": false, \"bar_sort_order\": 9, \"record_number\": 0}, {\"column_name\": \"tf_email\", \"label_for_charts\": \"All other comparisons\", \"sql_condition\": \"ELSE\", \"log2_bayes_factor\": 0.0, \"bayes_factor\": 1.0, \"comparison_vector_value\": 0, \"m_probability\": 0.42250907994219916, \"u_probability\": 0.9978061286856716, \"bayes_factor_description\": \"If comparison level is `all other comparisons` then comparison is 2.36 times less likely to be a match\", \"value_l\": \"\", \"value_r\": \"\", \"term_frequency_adjustment\": true, \"bar_sort_order\": 10, \"record_number\": 0}, {\"column_name\": \"Final score\", \"label_for_charts\": \"Final score\", \"sql_condition\": null, \"log2_bayes_factor\": -5.156104231128363, \"bayes_factor\": 0.028045162816898225, \"comparison_vector_value\": null, \"m_probability\": null, \"u_probability\": null, \"bayes_factor_description\": null, \"value_l\": \"\", \"value_r\": \"\", \"term_frequency_adjustment\": null, \"bar_sort_order\": 11, \"record_number\": 0}, {\"column_name\": \"Prior\", \"label_for_charts\": \"Starting match weight (prior)\", \"sql_condition\": null, \"log2_bayes_factor\": -5.391889789559854, \"bayes_factor\": 0.023816582302252427, \"comparison_vector_value\": null, \"m_probability\": null, \"u_probability\": null, \"bayes_factor_description\": null, \"value_l\": \"\", \"value_r\": \"\", \"term_frequency_adjustment\": null, \"bar_sort_order\": 0, \"record_number\": 1}, {\"column_name\": \"first_name\", \"label_for_charts\": \"All other comparisons\", \"sql_condition\": \"ELSE\", \"log2_bayes_factor\": -2.1925340745596764, \"bayes_factor\": 0.21876683164040506, \"comparison_vector_value\": 0, \"m_probability\": 0.21528549148688833, \"u_probability\": 0.9840865266118626, \"bayes_factor_description\": \"If comparison level is `all other comparisons` then comparison is 4.57 times less likely to be a match\", \"value_l\": \"Rowe\", \"value_r\": \"Scott\", \"term_frequency_adjustment\": false, \"bar_sort_order\": 1, \"record_number\": 1}, {\"column_name\": \"tf_first_name\", \"label_for_charts\": \"All other comparisons\", \"sql_condition\": \"ELSE\", \"log2_bayes_factor\": 0.0, \"bayes_factor\": 1.0, \"comparison_vector_value\": 0, \"m_probability\": 0.21528549148688833, \"u_probability\": 0.9840865266118626, \"bayes_factor_description\": \"If comparison level is `all other comparisons` then comparison is 4.57 times less likely to be a match\", \"value_l\": \"\", \"value_r\": \"\", \"term_frequency_adjustment\": true, \"bar_sort_order\": 2, \"record_number\": 1}, {\"column_name\": \"surname\", \"label_for_charts\": \"exact_match\", \"sql_condition\": \"surname_l = surname_r\", \"log2_bayes_factor\": 6.529599913880287, \"bayes_factor\": 92.38584465067325, \"comparison_vector_value\": 2, \"m_probability\": 0.4517645215191846, \"u_probability\": 0.004889975550122249, \"bayes_factor_description\": \"If comparison level is `exact_match` then comparison is 92.39 times more likely to be a match\", \"value_l\": \"Caleb\", \"value_r\": \"Caleb\", \"term_frequency_adjustment\": false, \"bar_sort_order\": 3, \"record_number\": 1}, {\"column_name\": \"tf_surname\", \"label_for_charts\": \"Term freq adjustment on surname with weight {cl.tf_adjustment_weight}\", \"sql_condition\": \"surname_l = surname_r\", \"log2_bayes_factor\": 0.4168001079781037, \"bayes_factor\": 1.3349633251833741, \"comparison_vector_value\": 2, \"m_probability\": null, \"u_probability\": null, \"bayes_factor_description\": \"Term frequency adjustment on surname makes comparison 1.33 times more likely to be a match\", \"value_l\": \"Caleb\", \"value_r\": \"Caleb\", \"term_frequency_adjustment\": true, \"bar_sort_order\": 4, \"record_number\": 1}, {\"column_name\": \"dob\", \"label_for_charts\": \"All other comparisons\", \"sql_condition\": \"ELSE\", \"log2_bayes_factor\": -2.114723717091652, \"bayes_factor\": 0.23088978993311773, \"comparison_vector_value\": 0, \"m_probability\": 0.22653362300553073, \"u_probability\": 0.9811331331331331, \"bayes_factor_description\": \"If comparison level is `all other comparisons` then comparison is 4.33 times less likely to be a match\", \"value_l\": \"1992-12-20\", \"value_r\": \"1990-12-11\", \"term_frequency_adjustment\": false, \"bar_sort_order\": 5, \"record_number\": 1}, {\"column_name\": \"tf_dob\", \"label_for_charts\": \"All other comparisons\", \"sql_condition\": \"ELSE\", \"log2_bayes_factor\": 0.0, \"bayes_factor\": 1.0, \"comparison_vector_value\": 0, \"m_probability\": 0.22653362300553073, \"u_probability\": 0.9811331331331331, \"bayes_factor_description\": \"If comparison level is `all other comparisons` then comparison is 4.33 times less likely to be a match\", \"value_l\": \"\", \"value_r\": \"\", \"term_frequency_adjustment\": true, \"bar_sort_order\": 6, \"record_number\": 1}, {\"column_name\": \"city\", \"label_for_charts\": \"Null\", \"sql_condition\": \"city_l IS NULL OR city_r IS NULL\", \"log2_bayes_factor\": 0.0, \"bayes_factor\": 1.0, \"comparison_vector_value\": -1, \"m_probability\": null, \"u_probability\": null, \"bayes_factor_description\": \"If comparison level is `null` then comparison is 1.00 times more likely to be a match\", \"value_l\": \"Lvpreool\", \"value_r\": \"nan\", \"term_frequency_adjustment\": false, \"bar_sort_order\": 7, \"record_number\": 1}, {\"column_name\": \"tf_city\", \"label_for_charts\": \"Null\", \"sql_condition\": \"city_l IS NULL OR city_r IS NULL\", \"log2_bayes_factor\": 0.0, \"bayes_factor\": 1.0, \"comparison_vector_value\": -1, \"m_probability\": null, \"u_probability\": null, \"bayes_factor_description\": \"If comparison level is `null` then comparison is 1.00 times more likely to be a match\", \"value_l\": \"\", \"value_r\": \"\", \"term_frequency_adjustment\": true, \"bar_sort_order\": 8, \"record_number\": 1}, {\"column_name\": \"email\", \"label_for_charts\": \"All other comparisons\", \"sql_condition\": \"ELSE\", \"log2_bayes_factor\": -1.239777184635766, \"bayes_factor\": 0.4234380485302649, \"comparison_vector_value\": 0, \"m_probability\": 0.42250907994219916, \"u_probability\": 0.9978061286856716, \"bayes_factor_description\": \"If comparison level is `all other comparisons` then comparison is 2.36 times less likely to be a match\", \"value_l\": \"calebr@thompson.org\", \"value_r\": \"c.scott@brooks.com\", \"term_frequency_adjustment\": false, \"bar_sort_order\": 9, \"record_number\": 1}, {\"column_name\": \"tf_email\", \"label_for_charts\": \"All other comparisons\", \"sql_condition\": \"ELSE\", \"log2_bayes_factor\": 0.0, \"bayes_factor\": 1.0, \"comparison_vector_value\": 0, \"m_probability\": 0.42250907994219916, \"u_probability\": 0.9978061286856716, \"bayes_factor_description\": \"If comparison level is `all other comparisons` then comparison is 2.36 times less likely to be a match\", \"value_l\": \"\", \"value_r\": \"\", \"term_frequency_adjustment\": true, \"bar_sort_order\": 10, \"record_number\": 1}, {\"column_name\": \"Final score\", \"label_for_charts\": \"Final score\", \"sql_condition\": null, \"log2_bayes_factor\": -3.9925247439885574, \"bayes_factor\": 0.06282468122305176, \"comparison_vector_value\": null, \"m_probability\": null, \"u_probability\": null, \"bayes_factor_description\": null, \"value_l\": \"\", \"value_r\": \"\", \"term_frequency_adjustment\": null, \"bar_sort_order\": 11, \"record_number\": 1}]}, \"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"params\": [{\"name\": \"record_number\", \"value\": 0, \"bind\": {\"input\": \"range\", \"min\": 0, \"max\": 1, \"step\": 1}, \"description\": \"Filter by the interation number\"}], \"title\": {\"text\": \"Match weights waterfall chart\", \"subtitle\": \"How each comparison contributes to the final match score\"}, \"transform\": [{\"filter\": \"(datum.record_number == record_number)\"}, {\"filter\": \"(datum.bayes_factor !== 1.0)\"}, {\"window\": [{\"op\": \"sum\", \"field\": \"log2_bayes_factor\", \"as\": \"sum\"}, {\"op\": \"lead\", \"field\": \"column_name\", \"as\": \"lead\"}], \"frame\": [null, 0]}, {\"calculate\": \"datum.column_name === \\\"Final score\\\" ? datum.sum - datum.log2_bayes_factor : datum.sum\", \"as\": \"sum\"}, {\"calculate\": \"datum.lead === null ? datum.column_name : datum.lead\", \"as\": \"lead\"}, {\"calculate\": \"datum.column_name === \\\"Final score\\\" || datum.column_name === \\\"Prior match weight\\\" ? 0 : datum.sum - datum.log2_bayes_factor\", \"as\": \"previous_sum\"}, {\"calculate\": \"datum.sum > datum.previous_sum ? datum.column_name : \\\"\\\"\", \"as\": \"top_label\"}, {\"calculate\": \"datum.sum < datum.previous_sum ? datum.column_name : \\\"\\\"\", \"as\": \"bottom_label\"}, {\"calculate\": \"datum.sum > datum.previous_sum ? datum.sum : datum.previous_sum\", \"as\": \"sum_top\"}, {\"calculate\": \"datum.sum < datum.previous_sum ? datum.sum : datum.previous_sum\", \"as\": \"sum_bottom\"}, {\"calculate\": \"(datum.sum + datum.previous_sum) / 2\", \"as\": \"center\"}, {\"calculate\": \"(datum.log2_bayes_factor > 0 ? \\\"+\\\" : \\\"\\\") + datum.log2_bayes_factor\", \"as\": \"text_log2_bayes_factor\"}, {\"calculate\": \"datum.sum < datum.previous_sum ? 4 : -4\", \"as\": \"dy\"}, {\"calculate\": \"datum.sum < datum.previous_sum ? \\\"top\\\" : \\\"bottom\\\"\", \"as\": \"baseline\"}, {\"calculate\": \"1. / (1 + pow(2, -1.*datum.sum))\", \"as\": \"prob\"}, {\"calculate\": \"0*datum.sum\", \"as\": \"zero\"}], \"layer\": [{\"layer\": [{\"mark\": \"rule\", \"encoding\": {\"y\": {\"field\": \"zero\", \"type\": \"quantitative\"}, \"size\": {\"value\": 0.5}, \"color\": {\"value\": \"black\"}}}, {\"mark\": {\"type\": \"bar\", \"width\": 60}, \"encoding\": {\"color\": {\"condition\": {\"value\": \"red\", \"test\": \"(datum.log2_bayes_factor < 0)\"}, \"value\": \"green\"}, \"opacity\": {\"condition\": {\"value\": 1, \"test\": \"datum.column_name == 'Prior match weight' || datum.column_name == 'Final score'\"}, \"value\": 0.5}, \"tooltip\": [{\"type\": \"nominal\", \"field\": \"column_name\", \"title\": \"Comparison column\"}, {\"type\": \"nominal\", \"field\": \"value_l\", \"title\": \"Value (L)\"}, {\"type\": \"nominal\", \"field\": \"value_r\", \"title\": \"Value (R)\"}, {\"type\": \"ordinal\", \"field\": \"label_for_charts\", \"title\": \"Label\"}, {\"type\": \"nominal\", \"field\": \"sql_condition\", \"title\": \"SQL condition\"}, {\"type\": \"nominal\", \"field\": \"comparison_vector_value\", \"title\": \"Comparison vector value\"}, {\"type\": \"quantitative\", \"field\": \"bayes_factor\", \"title\": \"Bayes factor = m/u\", \"format\": \",.4f\"}, {\"type\": \"quantitative\", \"field\": \"log2_bayes_factor\", \"title\": \"Match weight = log2(m/u)\", \"format\": \",.4f\"}, {\"type\": \"quantitative\", \"field\": \"prob\", \"format\": \".4f\", \"title\": \"Adjusted match score\"}, {\"type\": \"nominal\", \"field\": \"bayes_factor_description\", \"title\": \"Match weight description\"}], \"x\": {\"type\": \"nominal\", \"axis\": {\"labelExpr\": \"datum.value == 'Prior' || datum.value == 'Final score' ? '' : datum.value\", \"labelAngle\": -20, \"labelAlign\": \"center\", \"labelPadding\": 10, \"title\": \"Column\", \"grid\": true, \"tickBand\": \"extent\"}, \"field\": \"column_name\", \"sort\": {\"field\": \"bar_sort_order\", \"order\": \"ascending\"}}, \"y\": {\"type\": \"quantitative\", \"axis\": {\"grid\": false, \"orient\": \"left\", \"title\": \"log2(Bayes factor)\"}, \"field\": \"previous_sum\"}, \"y2\": {\"field\": \"sum\"}}}, {\"mark\": {\"type\": \"text\", \"fontWeight\": \"bold\"}, \"encoding\": {\"color\": {\"value\": \"white\"}, \"text\": {\"condition\": {\"type\": \"nominal\", \"field\": \"log2_bayes_factor\", \"format\": \".2f\", \"test\": \"abs(datum.log2_bayes_factor) > 1\"}, \"value\": \"\"}, \"x\": {\"type\": \"nominal\", \"axis\": {\"labelAngle\": 0, \"title\": \"Column\"}, \"field\": \"column_name\", \"sort\": {\"field\": \"bar_sort_order\", \"order\": \"ascending\"}}, \"y\": {\"type\": \"quantitative\", \"axis\": {\"orient\": \"left\"}, \"field\": \"center\"}}}, {\"mark\": {\"type\": \"text\", \"baseline\": \"bottom\", \"dy\": -25, \"fontWeight\": \"bold\"}, \"encoding\": {\"color\": {\"value\": \"black\"}, \"text\": {\"type\": \"nominal\", \"field\": \"column_name\"}, \"x\": {\"type\": \"nominal\", \"axis\": {\"labelAngle\": 0, \"title\": \"Column\"}, \"field\": \"column_name\", \"sort\": {\"field\": \"bar_sort_order\", \"order\": \"ascending\"}}, \"y\": {\"type\": \"quantitative\", \"field\": \"sum_top\"}}}, {\"mark\": {\"type\": \"text\", \"baseline\": \"bottom\", \"fontSize\": 8, \"dy\": -13}, \"encoding\": {\"color\": {\"value\": \"grey\"}, \"text\": {\"type\": \"nominal\", \"field\": \"value_l\"}, \"x\": {\"type\": \"nominal\", \"axis\": {\"labelAngle\": 0, \"title\": \"Column\"}, \"field\": \"column_name\", \"sort\": {\"field\": \"bar_sort_order\", \"order\": \"ascending\"}}, \"y\": {\"type\": \"quantitative\", \"field\": \"sum_top\"}}}, {\"mark\": {\"type\": \"text\", \"baseline\": \"bottom\", \"fontSize\": 8, \"dy\": -5}, \"encoding\": {\"color\": {\"value\": \"grey\"}, \"text\": {\"type\": \"nominal\", \"field\": \"value_r\"}, \"x\": {\"type\": \"nominal\", \"axis\": {\"labelAngle\": 0, \"title\": \"Column\"}, \"field\": \"column_name\", \"sort\": {\"field\": \"bar_sort_order\", \"order\": \"ascending\"}}, \"y\": {\"type\": \"quantitative\", \"field\": \"sum_top\"}}}]}, {\"mark\": {\"type\": \"rule\", \"color\": \"black\", \"strokeWidth\": 2, \"x2Offset\": 30, \"xOffset\": -30}, \"encoding\": {\"x\": {\"type\": \"nominal\", \"axis\": {\"labelAngle\": 0, \"title\": \"Column\"}, \"field\": \"column_name\", \"sort\": {\"field\": \"bar_sort_order\", \"order\": \"ascending\"}}, \"x2\": {\"field\": \"lead\"}, \"y\": {\"type\": \"quantitative\", \"axis\": {\"labelExpr\": \"format(1 / (1 + pow(2, -1*datum.value)), '.2r')\", \"orient\": \"right\", \"title\": \"Probability\"}, \"field\": \"sum\", \"scale\": {\"zero\": false}}}}]}, {\"mode\": \"vega-lite\"});","title":"Step 1: Load a pre-trained linkage model"},{"location":"demos/real_time_record_linkage.html#step-comparing-two-records","text":"It's now possible to compute a match weight for any two records using linker.compare_two_records() record_1 = { 'unique_id' : 1 , 'first_name' : \"Lucas\" , 'surname' : \"Smith\" , 'dob' : \"1984-01-02\" , 'city' : \"London\" , 'email' : \"lucas.smith@hotmail.com\" } record_2 = { 'unique_id' : 2 , 'first_name' : \"Lucas\" , 'surname' : \"Smith\" , 'dob' : \"1983-02-12\" , 'city' : \"Machester\" , 'email' : \"lucas.smith@hotmail.com\" } linker . _settings_obj_ . _retain_intermediate_calculation_columns = True linker . _settings_obj_ . _retain_matching_columns = True df_two = linker . compare_two_records ( record_1 , record_2 ) df_two . as_pandas_dataframe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } match_weight match_probability unique_id_l unique_id_r first_name_l first_name_r gamma_first_name bf_first_name bf_tf_adj_first_name tf_first_name_l ... bf_tf_adj_city tf_city_l tf_city_r email_l email_r gamma_email bf_email bf_tf_adj_email tf_email_l tf_email_r 0 13.161672 0.999891 1 2 Lucas Lucas 2 87.571229 4.814458 0.001203 ... 1.0 0.212792 NaN lucas.smith@hotmail.com lucas.smith@hotmail.com 1 263.229168 1.0 NaN NaN 1 rows \u00d7 39 columns","title":"Step  Comparing two records"},{"location":"demos/real_time_record_linkage.html#step-3-interactive-comparisons","text":"One interesting applicatin of compare_two_records is to create a simple interface that allows the user to input two records interactively, and get real time feedback. In the following cell we use ipywidets for this purpose. \u2728\u2728 Change the values in the text boxes to see the waterfall chart update in real time. \u2728\u2728 import ipywidgets as widgets fields = [ \"unique_id\" , \"first_name\" , \"surname\" , \"dob\" , \"email\" , \"city\" ] left_text_boxes = [] right_text_boxes = [] inputs_to_interactive_output = {} for f in fields : wl = widgets . Text ( description = f , value = str ( record_1 [ f ])) left_text_boxes . append ( wl ) inputs_to_interactive_output [ f \" { f } _l\" ] = wl wr = widgets . Text ( description = f , value = str ( record_2 [ f ])) right_text_boxes . append ( wr ) inputs_to_interactive_output [ f \" { f } _r\" ] = wr b1 = widgets . VBox ( left_text_boxes ) b2 = widgets . VBox ( right_text_boxes ) ui = widgets . HBox ([ b1 , b2 ]) def myfn ( ** kwargs ): my_args = dict ( kwargs ) record_left = {} record_right = {} for key , value in my_args . items (): if value == '' : value = None if key . endswith ( \"_l\" ): record_left [ key [: - 2 ]] = value if key . endswith ( \"_r\" ): record_right [ key [: - 2 ]] = value linker . _settings_obj_ . _retain_intermediate_calculation_columns = True linker . _settings_obj_ . _retain_matching_columns = True df_two = linker . compare_two_records ( record_left , record_right ) recs = df_two . as_pandas_dataframe () . to_dict ( orient = \"records\" ) from splink.charts import waterfall_chart display ( linker . waterfall_chart ( recs , filter_nulls = False )) out = widgets . interactive_output ( myfn , inputs_to_interactive_output ) display ( ui , out ) HBox(children=(VBox(children=(Text(value='1', description='unique_id'), Text(value='Lucas', description='first\u2026 Output()","title":"Step 3: Interactive comparisons"},{"location":"demos/real_time_record_linkage.html#finding-matching-records-interactively","text":"It is also possible to search the records in the input dataset rapidly using the linker.find_matches_to_new_records() function record = { 'unique_id' : 123987 , 'first_name' : \"Robert\" , 'surname' : \"Alan\" , 'dob' : \"1971-05-24\" , 'city' : \"London\" , 'email' : \"robert255@smith.net\" } df_inc = linker . find_matches_to_new_records ([ record ], blocking_rules = []) . as_pandas_dataframe () df_inc . sort_values ( \"match_weight\" , ascending = False ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } match_weight match_probability unique_id_l unique_id_r first_name_l first_name_r gamma_first_name bf_first_name bf_tf_adj_first_name tf_first_name_l ... bf_tf_adj_city tf_city_l tf_city_r email_l email_r gamma_email bf_email bf_tf_adj_email tf_email_l tf_email_r 1 23.531793 1.000000 0 123987 Robert Robert 2 87.571229 1.604819 0.003610 ... 1.000000 NaN 0.212792 robert255@smith.net robert255@smith.net 1 263.229168 1.730964 0.001267 0.001267 2 14.550320 0.999958 1 123987 Robert Robert 2 87.571229 1.604819 0.003610 ... 1.000000 NaN 0.212792 roberta25@smith.net robert255@smith.net 0 0.423438 1.000000 0.002535 0.001267 4 10.388623 0.999255 3 123987 Robert Robert 2 87.571229 1.604819 0.003610 ... 1.000000 0.007380 0.212792 NaN robert255@smith.net -1 1.000000 1.000000 NaN 0.001267 0 2.427256 0.843228 2 123987 Rob Robert 0 0.218767 1.000000 0.001203 ... 0.259162 0.212792 0.212792 roberta25@smith.net robert255@smith.net 0 0.423438 1.000000 0.002535 0.001267 6 -2.123090 0.186697 8 123987 NaN Robert -1 1.000000 1.000000 NaN ... 1.000000 NaN 0.212792 NaN robert255@smith.net -1 1.000000 1.000000 NaN 0.001267 5 -2.205894 0.178139 754 123987 NaN Robert -1 1.000000 1.000000 NaN ... 1.000000 NaN 0.212792 j.c@whige.wort robert255@smith.net 0 0.423438 1.000000 0.001267 0.001267 3 -2.802309 0.125383 750 123987 NaN Robert -1 1.000000 1.000000 NaN ... 0.259162 0.212792 0.212792 j.c@white.org robert255@smith.net 0 0.423438 1.000000 0.002535 0.001267 7 rows \u00d7 39 columns","title":"Finding matching records interactively"},{"location":"demos/real_time_record_linkage.html#interactive-interface-for-finding-records","text":"Again, we can use ipywidgets to build an interactive interface for the linker.find_matches_to_new_records function from splink.charts import waterfall_chart @widgets . interact ( first_name = 'Robert' , surname = \"Alan\" , dob = \"1971-05-24\" , city = \"London\" , email = \"robert255@smith.net\" ) def interactive_link ( first_name , surname , dob , city , email ): record = { 'unique_id' : 123987 , 'first_name' : first_name , 'surname' : surname , 'dob' : dob , 'city' : city , 'email' : email , 'group' : 0 } for key in record . keys (): if type ( record [ key ]) == str : if record [ key ] . strip () == \"\" : record [ key ] = None df_inc = linker . find_matches_to_new_records ([ record ], blocking_rules = [ f \"(true)\" ]) . as_pandas_dataframe () df_inc = df_inc . sort_values ( \"match_weight\" , ascending = False ) recs = df_inc . to_dict ( orient = \"records\" ) display ( linker . waterfall_chart ( recs , filter_nulls = False )) interactive(children=(Text(value='Robert', description='first_name'), Text(value='Alan', description='surname'\u2026 linker . match_weights_chart () var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG; (function(spec, embedOpt){ let outputDiv = document.currentScript.previousElementSibling; if (outputDiv.id !== \"altair-viz-4663e99319364be8854b4bc5d4c58cbf\") { outputDiv = document.getElementById(\"altair-viz-4663e99319364be8854b4bc5d4c58cbf\"); } const paths = { \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\", \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\", \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\", \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\", }; function maybeLoadScript(lib, version) { var key = `${lib.replace(\"-\", \"\")}_version`; return (VEGA_DEBUG[key] == version) ? Promise.resolve(paths[lib]) : new Promise(function(resolve, reject) { var s = document.createElement('script'); document.getElementsByTagName(\"head\")[0].appendChild(s); s.async = true; s.onload = () => { VEGA_DEBUG[key] = version; return resolve(paths[lib]); }; s.onerror = () => reject(`Error loading script: ${paths[lib]}`); s.src = paths[lib]; }); } function showError(err) { outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`; throw err; } function displayChart(vegaEmbed) { vegaEmbed(outputDiv, spec, embedOpt) .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`)); } if(typeof define === \"function\" && define.amd) { requirejs.config({paths}); require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`)); } else { maybeLoadScript(\"vega\", \"5\") .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\")) .then(() => maybeLoadScript(\"vega-embed\", \"6\")) .catch(showError) .then(() => displayChart(vegaEmbed)); } })({\"config\": {\"view\": {\"width\": 400, \"height\": 60}, \"mark\": {\"tooltip\": null}, \"title\": {\"anchor\": \"middle\"}, \"header\": {\"title\": null}}, \"data\": {\"values\": [{\"comparison_name\": \"probability_two_random_records_match\", \"sql_condition\": null, \"label_for_charts\": \"\", \"m_probability\": null, \"u_probability\": null, \"m_probability_description\": null, \"u_probability_description\": null, \"has_tf_adjustments\": false, \"tf_adjustment_column\": null, \"tf_adjustment_weight\": null, \"is_null_level\": false, \"bayes_factor\": 0.023816582302252427, \"log2_bayes_factor\": -5.391889789559854, \"comparison_vector_value\": 0, \"max_comparison_vector_value\": 0, \"bayes_factor_description\": \"The probability that two random records drawn at random match is 0.023 or one in 43.0 records.This is equivalent to a starting match weight of -5.392.\", \"probability_two_random_records_match\": 0.02326254791526835, \"comparison_sort_order\": -1}, {\"comparison_name\": \"first_name\", \"sql_condition\": \"first_name_l = first_name_r\", \"label_for_charts\": \"exact_match\", \"m_probability\": 0.5073501669215337, \"u_probability\": 0.0057935713975033705, \"m_probability_description\": \"Amongst matching record comparisons, 50.74% of records are in the exact_match comparison level\", \"u_probability_description\": \"Amongst non-matching record comparisons, 0.58% of records are in the exact_match comparison level\", \"has_tf_adjustments\": true, \"tf_adjustment_column\": \"first_name\", \"tf_adjustment_weight\": 1.0, \"is_null_level\": false, \"bayes_factor\": 87.57122888658395, \"log2_bayes_factor\": 6.452385051922501, \"comparison_vector_value\": 2, \"max_comparison_vector_value\": 2, \"bayes_factor_description\": \"If comparison level is `exact_match` then comparison is 87.57 times more likely to be a match\", \"probability_two_random_records_match\": 0.02326254791526835, \"comparison_sort_order\": 0}, {\"comparison_name\": \"first_name\", \"sql_condition\": \"levenshtein(first_name_l, first_name_r) <= 2\", \"label_for_charts\": \"Levenstein <= 2\", \"m_probability\": 0.27736434159157797, \"u_probability\": 0.010119901990634016, \"m_probability_description\": \"Amongst matching record comparisons, 27.74% of records are in the levenstein <= 2 comparison level\", \"u_probability_description\": \"Amongst non-matching record comparisons, 1.01% of records are in the levenstein <= 2 comparison level\", \"has_tf_adjustments\": false, \"tf_adjustment_column\": null, \"tf_adjustment_weight\": 1.0, \"is_null_level\": false, \"bayes_factor\": 27.407809072486973, \"log2_bayes_factor\": 4.776515101395072, \"comparison_vector_value\": 1, \"max_comparison_vector_value\": 2, \"bayes_factor_description\": \"If comparison level is `levenstein <= 2` then comparison is 27.41 times more likely to be a match\", \"probability_two_random_records_match\": 0.02326254791526835, \"comparison_sort_order\": 0}, {\"comparison_name\": \"first_name\", \"sql_condition\": \"ELSE\", \"label_for_charts\": \"All other comparisons\", \"m_probability\": 0.21528549148688833, \"u_probability\": 0.9840865266118626, \"m_probability_description\": \"Amongst matching record comparisons, 21.53% of records are in the all other comparisons comparison level\", \"u_probability_description\": \"Amongst non-matching record comparisons, 98.41% of records are in the all other comparisons comparison level\", \"has_tf_adjustments\": false, \"tf_adjustment_column\": null, \"tf_adjustment_weight\": 1.0, \"is_null_level\": false, \"bayes_factor\": 0.21876683164040506, \"log2_bayes_factor\": -2.1925340745596764, \"comparison_vector_value\": 0, \"max_comparison_vector_value\": 2, \"bayes_factor_description\": \"If comparison level is `all other comparisons` then comparison is 4.57 times less likely to be a match\", \"probability_two_random_records_match\": 0.02326254791526835, \"comparison_sort_order\": 0}, {\"comparison_name\": \"surname\", \"sql_condition\": \"surname_l = surname_r\", \"label_for_charts\": \"exact_match\", \"m_probability\": 0.4517645215191846, \"u_probability\": 0.004889975550122249, \"m_probability_description\": \"Amongst matching record comparisons, 45.18% of records are in the exact_match comparison level\", \"u_probability_description\": \"Amongst non-matching record comparisons, 0.49% of records are in the exact_match comparison level\", \"has_tf_adjustments\": true, \"tf_adjustment_column\": \"surname\", \"tf_adjustment_weight\": 1.0, \"is_null_level\": false, \"bayes_factor\": 92.38584465067325, \"log2_bayes_factor\": 6.529599913880287, \"comparison_vector_value\": 2, \"max_comparison_vector_value\": 2, \"bayes_factor_description\": \"If comparison level is `exact_match` then comparison is 92.39 times more likely to be a match\", \"probability_two_random_records_match\": 0.02326254791526835, \"comparison_sort_order\": 1}, {\"comparison_name\": \"surname\", \"sql_condition\": \"levenshtein(surname_l, surname_r) <= 2\", \"label_for_charts\": \"Levenstein <= 2\", \"m_probability\": 0.3078165102205689, \"u_probability\": 0.007373772654946249, \"m_probability_description\": \"Amongst matching record comparisons, 30.78% of records are in the levenstein <= 2 comparison level\", \"u_probability_description\": \"Amongst non-matching record comparisons, 0.74% of records are in the levenstein <= 2 comparison level\", \"has_tf_adjustments\": false, \"tf_adjustment_column\": null, \"tf_adjustment_weight\": 1.0, \"is_null_level\": false, \"bayes_factor\": 41.74477904659683, \"log2_bayes_factor\": 5.383523868172574, \"comparison_vector_value\": 1, \"max_comparison_vector_value\": 2, \"bayes_factor_description\": \"If comparison level is `levenstein <= 2` then comparison is 41.74 times more likely to be a match\", \"probability_two_random_records_match\": 0.02326254791526835, \"comparison_sort_order\": 1}, {\"comparison_name\": \"surname\", \"sql_condition\": \"ELSE\", \"label_for_charts\": \"All other comparisons\", \"m_probability\": 0.24041896826024636, \"u_probability\": 0.9877362517949315, \"m_probability_description\": \"Amongst matching record comparisons, 24.04% of records are in the all other comparisons comparison level\", \"u_probability_description\": \"Amongst non-matching record comparisons, 98.77% of records are in the all other comparisons comparison level\", \"has_tf_adjustments\": false, \"tf_adjustment_column\": null, \"tf_adjustment_weight\": 1.0, \"is_null_level\": false, \"bayes_factor\": 0.24340401379756268, \"log2_bayes_factor\": -2.03857513621085, \"comparison_vector_value\": 0, \"max_comparison_vector_value\": 2, \"bayes_factor_description\": \"If comparison level is `all other comparisons` then comparison is 4.11 times less likely to be a match\", \"probability_two_random_records_match\": 0.02326254791526835, \"comparison_sort_order\": 1}, {\"comparison_name\": \"dob\", \"sql_condition\": \"dob_l = dob_r\", \"label_for_charts\": \"exact_match\", \"m_probability\": 0.405530771330678, \"u_probability\": 0.0017477477477477479, \"m_probability_description\": \"Amongst matching record comparisons, 40.55% of records are in the exact_match comparison level\", \"u_probability_description\": \"Amongst non-matching record comparisons, 0.17% of records are in the exact_match comparison level\", \"has_tf_adjustments\": true, \"tf_adjustment_column\": \"dob\", \"tf_adjustment_weight\": 1.0, \"is_null_level\": false, \"bayes_factor\": 232.03049287476935, \"log2_bayes_factor\": 7.858170603008739, \"comparison_vector_value\": 2, \"max_comparison_vector_value\": 2, \"bayes_factor_description\": \"If comparison level is `exact_match` then comparison is 232.03 times more likely to be a match\", \"probability_two_random_records_match\": 0.02326254791526835, \"comparison_sort_order\": 2}, {\"comparison_name\": \"dob\", \"sql_condition\": \"levenshtein(dob_l, dob_r) <= 2\", \"label_for_charts\": \"Levenstein <= 2\", \"m_probability\": 0.3679356056637918, \"u_probability\": 0.01711911911911912, \"m_probability_description\": \"Amongst matching record comparisons, 36.79% of records are in the levenstein <= 2 comparison level\", \"u_probability_description\": \"Amongst non-matching record comparisons, 1.71% of records are in the levenstein <= 2 comparison level\", \"has_tf_adjustments\": false, \"tf_adjustment_column\": null, \"tf_adjustment_weight\": 1.0, \"is_null_level\": false, \"bayes_factor\": 21.492671620753597, \"log2_bayes_factor\": 4.425772921275592, \"comparison_vector_value\": 1, \"max_comparison_vector_value\": 2, \"bayes_factor_description\": \"If comparison level is `levenstein <= 2` then comparison is 21.49 times more likely to be a match\", \"probability_two_random_records_match\": 0.02326254791526835, \"comparison_sort_order\": 2}, {\"comparison_name\": \"dob\", \"sql_condition\": \"ELSE\", \"label_for_charts\": \"All other comparisons\", \"m_probability\": 0.22653362300553073, \"u_probability\": 0.9811331331331331, \"m_probability_description\": \"Amongst matching record comparisons, 22.65% of records are in the all other comparisons comparison level\", \"u_probability_description\": \"Amongst non-matching record comparisons, 98.11% of records are in the all other comparisons comparison level\", \"has_tf_adjustments\": false, \"tf_adjustment_column\": null, \"tf_adjustment_weight\": 1.0, \"is_null_level\": false, \"bayes_factor\": 0.23088978993311773, \"log2_bayes_factor\": -2.114723717091652, \"comparison_vector_value\": 0, \"max_comparison_vector_value\": 2, \"bayes_factor_description\": \"If comparison level is `all other comparisons` then comparison is 4.33 times less likely to be a match\", \"probability_two_random_records_match\": 0.02326254791526835, \"comparison_sort_order\": 2}, {\"comparison_name\": \"city\", \"sql_condition\": \"city_l = city_r\", \"label_for_charts\": \"exact_match\", \"m_probability\": 0.5782144900964232, \"u_probability\": 0.0551475711801453, \"m_probability_description\": \"Amongst matching record comparisons, 57.82% of records are in the exact_match comparison level\", \"u_probability_description\": \"Amongst non-matching record comparisons, 5.51% of records are in the exact_match comparison level\", \"has_tf_adjustments\": true, \"tf_adjustment_column\": \"city\", \"tf_adjustment_weight\": 1.0, \"is_null_level\": false, \"bayes_factor\": 10.484858675056154, \"log2_bayes_factor\": 3.3902355104306197, \"comparison_vector_value\": 1, \"max_comparison_vector_value\": 1, \"bayes_factor_description\": \"If comparison level is `exact_match` then comparison is 10.48 times more likely to be a match\", \"probability_two_random_records_match\": 0.02326254791526835, \"comparison_sort_order\": 3}, {\"comparison_name\": \"city\", \"sql_condition\": \"ELSE\", \"label_for_charts\": \"All other comparisons\", \"m_probability\": 0.4217855099035769, \"u_probability\": 0.9448524288198547, \"m_probability_description\": \"Amongst matching record comparisons, 42.18% of records are in the all other comparisons comparison level\", \"u_probability_description\": \"Amongst non-matching record comparisons, 94.49% of records are in the all other comparisons comparison level\", \"has_tf_adjustments\": false, \"tf_adjustment_column\": null, \"tf_adjustment_weight\": 1.0, \"is_null_level\": false, \"bayes_factor\": 0.4464035832880252, \"log2_bayes_factor\": -1.1635794871398053, \"comparison_vector_value\": 0, \"max_comparison_vector_value\": 1, \"bayes_factor_description\": \"If comparison level is `all other comparisons` then comparison is 2.24 times less likely to be a match\", \"probability_two_random_records_match\": 0.02326254791526835, \"comparison_sort_order\": 3}, {\"comparison_name\": \"email\", \"sql_condition\": \"email_l = email_r\", \"label_for_charts\": \"exact_match\", \"m_probability\": 0.5774909200578013, \"u_probability\": 0.0021938713143283602, \"m_probability_description\": \"Amongst matching record comparisons, 57.75% of records are in the exact_match comparison level\", \"u_probability_description\": \"Amongst non-matching record comparisons, 0.22% of records are in the exact_match comparison level\", \"has_tf_adjustments\": true, \"tf_adjustment_column\": \"email\", \"tf_adjustment_weight\": 1.0, \"is_null_level\": false, \"bayes_factor\": 263.2291676754963, \"log2_bayes_factor\": 8.04017554864013, \"comparison_vector_value\": 1, \"max_comparison_vector_value\": 1, \"bayes_factor_description\": \"If comparison level is `exact_match` then comparison is 263.23 times more likely to be a match\", \"probability_two_random_records_match\": 0.02326254791526835, \"comparison_sort_order\": 4}, {\"comparison_name\": \"email\", \"sql_condition\": \"ELSE\", \"label_for_charts\": \"All other comparisons\", \"m_probability\": 0.42250907994219916, \"u_probability\": 0.9978061286856716, \"m_probability_description\": \"Amongst matching record comparisons, 42.25% of records are in the all other comparisons comparison level\", \"u_probability_description\": \"Amongst non-matching record comparisons, 99.78% of records are in the all other comparisons comparison level\", \"has_tf_adjustments\": false, \"tf_adjustment_column\": null, \"tf_adjustment_weight\": 1.0, \"is_null_level\": false, \"bayes_factor\": 0.4234380485302649, \"log2_bayes_factor\": -1.239777184635766, \"comparison_vector_value\": 0, \"max_comparison_vector_value\": 1, \"bayes_factor_description\": \"If comparison level is `all other comparisons` then comparison is 2.36 times less likely to be a match\", \"probability_two_random_records_match\": 0.02326254791526835, \"comparison_sort_order\": 4}]}, \"vconcat\": [{\"height\": 30, \"mark\": {\"type\": \"bar\", \"clip\": true, \"height\": 20}, \"selection\": {\"zoom_selector\": {\"type\": \"interval\", \"bind\": \"scales\", \"encodings\": [\"x\"]}}, \"transform\": [{\"filter\": \"(datum.comparison_name == 'probability_two_random_records_match')\"}], \"encoding\": {\"color\": {\"type\": \"quantitative\", \"field\": \"log2_bayes_factor\", \"title\": \"Match weight\", \"scale\": {\"range\": [\"red\", \"orange\", \"green\"], \"domain\": [-10, 0, 10]}}, \"tooltip\": [{\"type\": \"nominal\", \"field\": \"comparison_name\", \"title\": \"Comparison name\"}, {\"type\": \"nominal\", \"field\": \"probability_two_random_records_match\", \"format\": \".4f\", \"title\": \"Probability two random records match\"}, {\"type\": \"quantitative\", \"field\": \"log2_bayes_factor\", \"title\": \"Equivalent match weight\", \"format\": \",.4f\"}, {\"type\": \"nominal\", \"field\": \"bayes_factor_description\", \"title\": \"Match weight description\"}], \"x\": {\"type\": \"quantitative\", \"axis\": {\"labels\": false, \"domain\": false, \"title\": \"\", \"ticks\": false}, \"field\": \"log2_bayes_factor\", \"scale\": {\"domain\": [-10, 10]}}, \"y\": {\"type\": \"nominal\", \"field\": \"label_for_charts\", \"axis\": {\"title\": \"Prior (starting) match weight\", \"titleAngle\": 0, \"titleAlign\": \"right\", \"titleFontWeight\": \"normal\"}, \"sort\": {\"field\": \"comparison_vector_value\", \"order\": \"descending\"}}}}, {\"mark\": {\"type\": \"bar\", \"clip\": true}, \"selection\": {\"zoom_selector\": {\"type\": \"interval\", \"bind\": \"scales\", \"encodings\": [\"x\"]}}, \"transform\": [{\"filter\": \"(datum.comparison_name != 'probability_two_random_records_match')\"}], \"encoding\": {\"color\": {\"type\": \"quantitative\", \"field\": \"log2_bayes_factor\", \"title\": \"Match weight\", \"scale\": {\"range\": [\"red\", \"orange\", \"green\"], \"domain\": [-10, 0, 10]}}, \"row\": {\"type\": \"nominal\", \"field\": \"comparison_name\", \"sort\": {\"field\": \"comparison_sort_order\"}, \"header\": {\"labelAngle\": 0, \"labelAnchor\": \"middle\", \"labelAlign\": \"left\"}}, \"tooltip\": [{\"type\": \"nominal\", \"field\": \"comparison_name\", \"title\": \"Comparison name\"}, {\"type\": \"ordinal\", \"field\": \"label_for_charts\", \"title\": \"Label\"}, {\"type\": \"nominal\", \"field\": \"sql_condition\", \"title\": \"SQL condition\"}, {\"type\": \"quantitative\", \"field\": \"m_probability\", \"format\": \".4f\", \"title\": \"M probability\"}, {\"type\": \"quantitative\", \"field\": \"u_probability\", \"format\": \".4f\", \"title\": \"U probability\"}, {\"type\": \"quantitative\", \"field\": \"bayes_factor\", \"title\": \"Bayes factor = m/u\", \"format\": \",.4f\"}, {\"type\": \"quantitative\", \"field\": \"log2_bayes_factor\", \"title\": \"Match weight = log2(m/u)\", \"format\": \",.4f\"}, {\"type\": \"nominal\", \"field\": \"bayes_factor_description\", \"title\": \"Match weight description\"}], \"x\": {\"type\": \"quantitative\", \"axis\": {\"title\": \"Comparison level match weight = log2(m/u)\"}, \"field\": \"log2_bayes_factor\", \"scale\": {\"domain\": [-10, 10]}}, \"y\": {\"type\": \"nominal\", \"field\": \"label_for_charts\", \"axis\": {\"title\": null}, \"sort\": {\"field\": \"comparison_vector_value\", \"order\": \"descending\"}}}, \"resolve\": {\"axis\": {\"y\": \"independent\"}, \"scale\": {\"y\": \"independent\"}}}], \"selection\": {\"zoom_selector\": {\"type\": \"interval\", \"bind\": \"scales\", \"encodings\": [\"x\"]}}, \"resolve\": {\"axis\": {\"y\": \"independent\"}, \"scale\": {\"y\": \"independent\"}}, \"title\": {\"text\": \"Model parameters (components of final match weight)\", \"subtitle\": \"Use mousewheel to zoom\"}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.2.json\"}, {\"mode\": \"vega-lite\"});","title":"Interactive interface for finding records"},{"location":"includes/abbreviations.html","text":"","title":"Abbreviations"},{"location":"includes/tags.html","text":"Tags \u00b6 Following is a list of relevant tags: [TAGS]","title":"Tags"},{"location":"includes/tags.html#tags","text":"Following is a list of relevant tags: [TAGS]","title":"Tags"},{"location":"settingseditor/editor.html","text":"","title":"Settings Editor"},{"location":"splink/index.html","text":"splink: Probabilistic record linkage and deduplication at scale \u00b6 splink implements Fellegi-Sunter's canonical model of record linkage in Apache Spark, including the EM algorithm to estimate parameters of the model. It: Works at much greater scale than current open source implementations (100 million records+). Runs quickly - with runtimes of less than an hour. Has a highly transparent methodology; match scores can be easily explained both graphically and in words Is highly accurate It is assumed that users of Splink are familiar with the probabilistic record linkage theory, and the Fellegi-Sunter model in particular. A series of interactive articles explores the theory behind Splink. The statistical model behind splink is the same as that used in the R fastLink package . Accompanying the fastLink package is an academic paper that describes this model. This is the best place to start for users wanting to understand the theory about how splink works. Data Matching , a book by Peter Christen, is another excellent resource. Installation \u00b6 splink is a Python package. It uses the Spark Python API to execute data linking jobs in a Spark cluster. It has been tested in Apache Spark 2.3, 2.4 and 3.1. Install splink using: pip install splink Note that Splink requires pyspark and a working Spark installation. These are not specified as explicit dependencies becuase it is assumed users have an existing pyspark setup they wish to use. Interactive demo \u00b6 You can run demos of splink in an interactive Jupyter notebook by clicking the button below: Documentation \u00b6 The best documentation is currently a series of demonstrations notebooks in the splink_demos repo. Other tools in the Splink family \u00b6 Splink Graph \u00b6 splink_graph is a graph utility library for use in Apache Spark. It computes graph metrics on the outputs of data linking. The repo is here Quality assurance of linkage results and identifying false positive links Computing quality metrics associated with groups (clusters) of linked records Automatically identifying possible false positive links in clusters Splink Comparison Viewer \u00b6 splink_comparison_viewer produces interactive dashboards that help you rapidly understand and quality assure the outputs of record linkage. A tutorial video is available here . Splink Cluster Studio \u00b6 splink_cluster_studio creates an interactive html dashboard from Splink output that allows you to visualise and analyse a sample of clusters from your record linkage. The repo is here . Splink Synthetic Data \u00b6 This code is able to generate realistic test datasets for linkage using the WikiData Query Service. It has been used to performance test the accuracy of various Splink models . Interactive settings editor with autocomplete \u00b6 We also provide an interactive splink settings editor and example settings here . Starting parameter generation tools \u00b6 A tool to generate custom m and u probabilities can be found here . Blog \u00b6 You can read a short blog post about splink here . Videos \u00b6 You can find a short video introducing splink and running though an introductory demo here . A 'best practices and performance tuning' tutorial can be found here . How to make changes to Splink \u00b6 (Steps 5 onwards for repo admins only) Raise new issue or target existing issue Create new branch (usually off master). Or fork for external contributors. Make changes, commit and push to GitHub Make pull request, referencing the issue Wait for tests to pass Review pull request Bump Splink version in pyproject.toml and update CHANGELOG.md as part of pull request Merge Release on PyPI and create new tag Acknowledgements \u00b6 We are very grateful to ADR UK (Administrative Data Research UK) for providing funding for this work as part of the Data First project. We are also very grateful to colleagues at the UK's Office for National Statistics for their expert advice and peer review of this work.","title":"Home"},{"location":"splink/index.html#splink-probabilistic-record-linkage-and-deduplication-at-scale","text":"splink implements Fellegi-Sunter's canonical model of record linkage in Apache Spark, including the EM algorithm to estimate parameters of the model. It: Works at much greater scale than current open source implementations (100 million records+). Runs quickly - with runtimes of less than an hour. Has a highly transparent methodology; match scores can be easily explained both graphically and in words Is highly accurate It is assumed that users of Splink are familiar with the probabilistic record linkage theory, and the Fellegi-Sunter model in particular. A series of interactive articles explores the theory behind Splink. The statistical model behind splink is the same as that used in the R fastLink package . Accompanying the fastLink package is an academic paper that describes this model. This is the best place to start for users wanting to understand the theory about how splink works. Data Matching , a book by Peter Christen, is another excellent resource.","title":"splink: Probabilistic record linkage and deduplication at scale"},{"location":"splink/index.html#installation","text":"splink is a Python package. It uses the Spark Python API to execute data linking jobs in a Spark cluster. It has been tested in Apache Spark 2.3, 2.4 and 3.1. Install splink using: pip install splink Note that Splink requires pyspark and a working Spark installation. These are not specified as explicit dependencies becuase it is assumed users have an existing pyspark setup they wish to use.","title":"Installation"},{"location":"splink/index.html#interactive-demo","text":"You can run demos of splink in an interactive Jupyter notebook by clicking the button below:","title":"Interactive demo"},{"location":"splink/index.html#documentation","text":"The best documentation is currently a series of demonstrations notebooks in the splink_demos repo.","title":"Documentation"},{"location":"splink/index.html#other-tools-in-the-splink-family","text":"","title":"Other tools in the Splink family"},{"location":"splink/index.html#splink-graph","text":"splink_graph is a graph utility library for use in Apache Spark. It computes graph metrics on the outputs of data linking. The repo is here Quality assurance of linkage results and identifying false positive links Computing quality metrics associated with groups (clusters) of linked records Automatically identifying possible false positive links in clusters","title":"Splink Graph"},{"location":"splink/index.html#splink-comparison-viewer","text":"splink_comparison_viewer produces interactive dashboards that help you rapidly understand and quality assure the outputs of record linkage. A tutorial video is available here .","title":"Splink Comparison Viewer"},{"location":"splink/index.html#splink-cluster-studio","text":"splink_cluster_studio creates an interactive html dashboard from Splink output that allows you to visualise and analyse a sample of clusters from your record linkage. The repo is here .","title":"Splink Cluster Studio"},{"location":"splink/index.html#splink-synthetic-data","text":"This code is able to generate realistic test datasets for linkage using the WikiData Query Service. It has been used to performance test the accuracy of various Splink models .","title":"Splink Synthetic Data"},{"location":"splink/index.html#interactive-settings-editor-with-autocomplete","text":"We also provide an interactive splink settings editor and example settings here .","title":"Interactive settings editor with autocomplete"},{"location":"splink/index.html#starting-parameter-generation-tools","text":"A tool to generate custom m and u probabilities can be found here .","title":"Starting parameter generation tools"},{"location":"splink/index.html#blog","text":"You can read a short blog post about splink here .","title":"Blog"},{"location":"splink/index.html#videos","text":"You can find a short video introducing splink and running though an introductory demo here . A 'best practices and performance tuning' tutorial can be found here .","title":"Videos"},{"location":"splink/index.html#how-to-make-changes-to-splink","text":"(Steps 5 onwards for repo admins only) Raise new issue or target existing issue Create new branch (usually off master). Or fork for external contributors. Make changes, commit and push to GitHub Make pull request, referencing the issue Wait for tests to pass Review pull request Bump Splink version in pyproject.toml and update CHANGELOG.md as part of pull request Merge Release on PyPI and create new tag","title":"How to make changes to Splink"},{"location":"splink/index.html#acknowledgements","text":"We are very grateful to ADR UK (Administrative Data Research UK) for providing funding for this work as part of the Data First project. We are also very grateful to colleagues at the UK's Office for National Statistics for their expert advice and peer review of this work.","title":"Acknowledgements"}]}